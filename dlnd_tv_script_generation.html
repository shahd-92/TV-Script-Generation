<!DOCTYPE html>
<html>
<head><meta charset="utf-8" />

<title>dlnd_tv_script_generation</title>

<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.1.10/require.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script>
(function() {
  function addWidgetsRenderer() {
    var mimeElement = document.querySelector('script[type="application/vnd.jupyter.widget-view+json"]');
    var scriptElement = document.createElement('script');
    var widgetRendererSrc = '@jupyter-widgets/html-manager@*/dist/embed-amd.js';
    var widgetState;

    // Fallback for older version:
    try {
      widgetState = mimeElement && JSON.parse(mimeElement.innerHTML);

      if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) {
        widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js';
      }
    } catch(e) {}

    scriptElement.src = widgetRendererSrc;
    document.body.appendChild(scriptElement);
  }

  document.addEventListener('DOMContentLoaded', addWidgetsRenderer);
}());
</script>

<style type="text/css">
    /*!
*
* Twitter Bootstrap
*
*/
/*!
 * Bootstrap v3.3.7 (http://getbootstrap.com)
 * Copyright 2011-2016 Twitter, Inc.
 * Licensed under MIT (https://github.com/twbs/bootstrap/blob/master/LICENSE)
 */
/*! normalize.css v3.0.3 | MIT License | github.com/necolas/normalize.css */
html {
  font-family: sans-serif;
  -ms-text-size-adjust: 100%;
  -webkit-text-size-adjust: 100%;
}
body {
  margin: 0;
}
article,
aside,
details,
figcaption,
figure,
footer,
header,
hgroup,
main,
menu,
nav,
section,
summary {
  display: block;
}
audio,
canvas,
progress,
video {
  display: inline-block;
  vertical-align: baseline;
}
audio:not([controls]) {
  display: none;
  height: 0;
}
[hidden],
template {
  display: none;
}
a {
  background-color: transparent;
}
a:active,
a:hover {
  outline: 0;
}
abbr[title] {
  border-bottom: 1px dotted;
}
b,
strong {
  font-weight: bold;
}
dfn {
  font-style: italic;
}
h1 {
  font-size: 2em;
  margin: 0.67em 0;
}
mark {
  background: #ff0;
  color: #000;
}
small {
  font-size: 80%;
}
sub,
sup {
  font-size: 75%;
  line-height: 0;
  position: relative;
  vertical-align: baseline;
}
sup {
  top: -0.5em;
}
sub {
  bottom: -0.25em;
}
img {
  border: 0;
}
svg:not(:root) {
  overflow: hidden;
}
figure {
  margin: 1em 40px;
}
hr {
  box-sizing: content-box;
  height: 0;
}
pre {
  overflow: auto;
}
code,
kbd,
pre,
samp {
  font-family: monospace, monospace;
  font-size: 1em;
}
button,
input,
optgroup,
select,
textarea {
  color: inherit;
  font: inherit;
  margin: 0;
}
button {
  overflow: visible;
}
button,
select {
  text-transform: none;
}
button,
html input[type="button"],
input[type="reset"],
input[type="submit"] {
  -webkit-appearance: button;
  cursor: pointer;
}
button[disabled],
html input[disabled] {
  cursor: default;
}
button::-moz-focus-inner,
input::-moz-focus-inner {
  border: 0;
  padding: 0;
}
input {
  line-height: normal;
}
input[type="checkbox"],
input[type="radio"] {
  box-sizing: border-box;
  padding: 0;
}
input[type="number"]::-webkit-inner-spin-button,
input[type="number"]::-webkit-outer-spin-button {
  height: auto;
}
input[type="search"] {
  -webkit-appearance: textfield;
  box-sizing: content-box;
}
input[type="search"]::-webkit-search-cancel-button,
input[type="search"]::-webkit-search-decoration {
  -webkit-appearance: none;
}
fieldset {
  border: 1px solid #c0c0c0;
  margin: 0 2px;
  padding: 0.35em 0.625em 0.75em;
}
legend {
  border: 0;
  padding: 0;
}
textarea {
  overflow: auto;
}
optgroup {
  font-weight: bold;
}
table {
  border-collapse: collapse;
  border-spacing: 0;
}
td,
th {
  padding: 0;
}
/*! Source: https://github.com/h5bp/html5-boilerplate/blob/master/src/css/main.css */
@media print {
  *,
  *:before,
  *:after {
    background: transparent !important;
    box-shadow: none !important;
    text-shadow: none !important;
  }
  a,
  a:visited {
    text-decoration: underline;
  }
  a[href]:after {
    content: " (" attr(href) ")";
  }
  abbr[title]:after {
    content: " (" attr(title) ")";
  }
  a[href^="#"]:after,
  a[href^="javascript:"]:after {
    content: "";
  }
  pre,
  blockquote {
    border: 1px solid #999;
    page-break-inside: avoid;
  }
  thead {
    display: table-header-group;
  }
  tr,
  img {
    page-break-inside: avoid;
  }
  img {
    max-width: 100% !important;
  }
  p,
  h2,
  h3 {
    orphans: 3;
    widows: 3;
  }
  h2,
  h3 {
    page-break-after: avoid;
  }
  .navbar {
    display: none;
  }
  .btn > .caret,
  .dropup > .btn > .caret {
    border-top-color: #000 !important;
  }
  .label {
    border: 1px solid #000;
  }
  .table {
    border-collapse: collapse !important;
  }
  .table td,
  .table th {
    background-color: #fff !important;
  }
  .table-bordered th,
  .table-bordered td {
    border: 1px solid #ddd !important;
  }
}
@font-face {
  font-family: 'Glyphicons Halflings';
  src: url('../components/bootstrap/fonts/glyphicons-halflings-regular.eot');
  src: url('../components/bootstrap/fonts/glyphicons-halflings-regular.eot?#iefix') format('embedded-opentype'), url('../components/bootstrap/fonts/glyphicons-halflings-regular.woff2') format('woff2'), url('../components/bootstrap/fonts/glyphicons-halflings-regular.woff') format('woff'), url('../components/bootstrap/fonts/glyphicons-halflings-regular.ttf') format('truetype'), url('../components/bootstrap/fonts/glyphicons-halflings-regular.svg#glyphicons_halflingsregular') format('svg');
}
.glyphicon {
  position: relative;
  top: 1px;
  display: inline-block;
  font-family: 'Glyphicons Halflings';
  font-style: normal;
  font-weight: normal;
  line-height: 1;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
}
.glyphicon-asterisk:before {
  content: "\002a";
}
.glyphicon-plus:before {
  content: "\002b";
}
.glyphicon-euro:before,
.glyphicon-eur:before {
  content: "\20ac";
}
.glyphicon-minus:before {
  content: "\2212";
}
.glyphicon-cloud:before {
  content: "\2601";
}
.glyphicon-envelope:before {
  content: "\2709";
}
.glyphicon-pencil:before {
  content: "\270f";
}
.glyphicon-glass:before {
  content: "\e001";
}
.glyphicon-music:before {
  content: "\e002";
}
.glyphicon-search:before {
  content: "\e003";
}
.glyphicon-heart:before {
  content: "\e005";
}
.glyphicon-star:before {
  content: "\e006";
}
.glyphicon-star-empty:before {
  content: "\e007";
}
.glyphicon-user:before {
  content: "\e008";
}
.glyphicon-film:before {
  content: "\e009";
}
.glyphicon-th-large:before {
  content: "\e010";
}
.glyphicon-th:before {
  content: "\e011";
}
.glyphicon-th-list:before {
  content: "\e012";
}
.glyphicon-ok:before {
  content: "\e013";
}
.glyphicon-remove:before {
  content: "\e014";
}
.glyphicon-zoom-in:before {
  content: "\e015";
}
.glyphicon-zoom-out:before {
  content: "\e016";
}
.glyphicon-off:before {
  content: "\e017";
}
.glyphicon-signal:before {
  content: "\e018";
}
.glyphicon-cog:before {
  content: "\e019";
}
.glyphicon-trash:before {
  content: "\e020";
}
.glyphicon-home:before {
  content: "\e021";
}
.glyphicon-file:before {
  content: "\e022";
}
.glyphicon-time:before {
  content: "\e023";
}
.glyphicon-road:before {
  content: "\e024";
}
.glyphicon-download-alt:before {
  content: "\e025";
}
.glyphicon-download:before {
  content: "\e026";
}
.glyphicon-upload:before {
  content: "\e027";
}
.glyphicon-inbox:before {
  content: "\e028";
}
.glyphicon-play-circle:before {
  content: "\e029";
}
.glyphicon-repeat:before {
  content: "\e030";
}
.glyphicon-refresh:before {
  content: "\e031";
}
.glyphicon-list-alt:before {
  content: "\e032";
}
.glyphicon-lock:before {
  content: "\e033";
}
.glyphicon-flag:before {
  content: "\e034";
}
.glyphicon-headphones:before {
  content: "\e035";
}
.glyphicon-volume-off:before {
  content: "\e036";
}
.glyphicon-volume-down:before {
  content: "\e037";
}
.glyphicon-volume-up:before {
  content: "\e038";
}
.glyphicon-qrcode:before {
  content: "\e039";
}
.glyphicon-barcode:before {
  content: "\e040";
}
.glyphicon-tag:before {
  content: "\e041";
}
.glyphicon-tags:before {
  content: "\e042";
}
.glyphicon-book:before {
  content: "\e043";
}
.glyphicon-bookmark:before {
  content: "\e044";
}
.glyphicon-print:before {
  content: "\e045";
}
.glyphicon-camera:before {
  content: "\e046";
}
.glyphicon-font:before {
  content: "\e047";
}
.glyphicon-bold:before {
  content: "\e048";
}
.glyphicon-italic:before {
  content: "\e049";
}
.glyphicon-text-height:before {
  content: "\e050";
}
.glyphicon-text-width:before {
  content: "\e051";
}
.glyphicon-align-left:before {
  content: "\e052";
}
.glyphicon-align-center:before {
  content: "\e053";
}
.glyphicon-align-right:before {
  content: "\e054";
}
.glyphicon-align-justify:before {
  content: "\e055";
}
.glyphicon-list:before {
  content: "\e056";
}
.glyphicon-indent-left:before {
  content: "\e057";
}
.glyphicon-indent-right:before {
  content: "\e058";
}
.glyphicon-facetime-video:before {
  content: "\e059";
}
.glyphicon-picture:before {
  content: "\e060";
}
.glyphicon-map-marker:before {
  content: "\e062";
}
.glyphicon-adjust:before {
  content: "\e063";
}
.glyphicon-tint:before {
  content: "\e064";
}
.glyphicon-edit:before {
  content: "\e065";
}
.glyphicon-share:before {
  content: "\e066";
}
.glyphicon-check:before {
  content: "\e067";
}
.glyphicon-move:before {
  content: "\e068";
}
.glyphicon-step-backward:before {
  content: "\e069";
}
.glyphicon-fast-backward:before {
  content: "\e070";
}
.glyphicon-backward:before {
  content: "\e071";
}
.glyphicon-play:before {
  content: "\e072";
}
.glyphicon-pause:before {
  content: "\e073";
}
.glyphicon-stop:before {
  content: "\e074";
}
.glyphicon-forward:before {
  content: "\e075";
}
.glyphicon-fast-forward:before {
  content: "\e076";
}
.glyphicon-step-forward:before {
  content: "\e077";
}
.glyphicon-eject:before {
  content: "\e078";
}
.glyphicon-chevron-left:before {
  content: "\e079";
}
.glyphicon-chevron-right:before {
  content: "\e080";
}
.glyphicon-plus-sign:before {
  content: "\e081";
}
.glyphicon-minus-sign:before {
  content: "\e082";
}
.glyphicon-remove-sign:before {
  content: "\e083";
}
.glyphicon-ok-sign:before {
  content: "\e084";
}
.glyphicon-question-sign:before {
  content: "\e085";
}
.glyphicon-info-sign:before {
  content: "\e086";
}
.glyphicon-screenshot:before {
  content: "\e087";
}
.glyphicon-remove-circle:before {
  content: "\e088";
}
.glyphicon-ok-circle:before {
  content: "\e089";
}
.glyphicon-ban-circle:before {
  content: "\e090";
}
.glyphicon-arrow-left:before {
  content: "\e091";
}
.glyphicon-arrow-right:before {
  content: "\e092";
}
.glyphicon-arrow-up:before {
  content: "\e093";
}
.glyphicon-arrow-down:before {
  content: "\e094";
}
.glyphicon-share-alt:before {
  content: "\e095";
}
.glyphicon-resize-full:before {
  content: "\e096";
}
.glyphicon-resize-small:before {
  content: "\e097";
}
.glyphicon-exclamation-sign:before {
  content: "\e101";
}
.glyphicon-gift:before {
  content: "\e102";
}
.glyphicon-leaf:before {
  content: "\e103";
}
.glyphicon-fire:before {
  content: "\e104";
}
.glyphicon-eye-open:before {
  content: "\e105";
}
.glyphicon-eye-close:before {
  content: "\e106";
}
.glyphicon-warning-sign:before {
  content: "\e107";
}
.glyphicon-plane:before {
  content: "\e108";
}
.glyphicon-calendar:before {
  content: "\e109";
}
.glyphicon-random:before {
  content: "\e110";
}
.glyphicon-comment:before {
  content: "\e111";
}
.glyphicon-magnet:before {
  content: "\e112";
}
.glyphicon-chevron-up:before {
  content: "\e113";
}
.glyphicon-chevron-down:before {
  content: "\e114";
}
.glyphicon-retweet:before {
  content: "\e115";
}
.glyphicon-shopping-cart:before {
  content: "\e116";
}
.glyphicon-folder-close:before {
  content: "\e117";
}
.glyphicon-folder-open:before {
  content: "\e118";
}
.glyphicon-resize-vertical:before {
  content: "\e119";
}
.glyphicon-resize-horizontal:before {
  content: "\e120";
}
.glyphicon-hdd:before {
  content: "\e121";
}
.glyphicon-bullhorn:before {
  content: "\e122";
}
.glyphicon-bell:before {
  content: "\e123";
}
.glyphicon-certificate:before {
  content: "\e124";
}
.glyphicon-thumbs-up:before {
  content: "\e125";
}
.glyphicon-thumbs-down:before {
  content: "\e126";
}
.glyphicon-hand-right:before {
  content: "\e127";
}
.glyphicon-hand-left:before {
  content: "\e128";
}
.glyphicon-hand-up:before {
  content: "\e129";
}
.glyphicon-hand-down:before {
  content: "\e130";
}
.glyphicon-circle-arrow-right:before {
  content: "\e131";
}
.glyphicon-circle-arrow-left:before {
  content: "\e132";
}
.glyphicon-circle-arrow-up:before {
  content: "\e133";
}
.glyphicon-circle-arrow-down:before {
  content: "\e134";
}
.glyphicon-globe:before {
  content: "\e135";
}
.glyphicon-wrench:before {
  content: "\e136";
}
.glyphicon-tasks:before {
  content: "\e137";
}
.glyphicon-filter:before {
  content: "\e138";
}
.glyphicon-briefcase:before {
  content: "\e139";
}
.glyphicon-fullscreen:before {
  content: "\e140";
}
.glyphicon-dashboard:before {
  content: "\e141";
}
.glyphicon-paperclip:before {
  content: "\e142";
}
.glyphicon-heart-empty:before {
  content: "\e143";
}
.glyphicon-link:before {
  content: "\e144";
}
.glyphicon-phone:before {
  content: "\e145";
}
.glyphicon-pushpin:before {
  content: "\e146";
}
.glyphicon-usd:before {
  content: "\e148";
}
.glyphicon-gbp:before {
  content: "\e149";
}
.glyphicon-sort:before {
  content: "\e150";
}
.glyphicon-sort-by-alphabet:before {
  content: "\e151";
}
.glyphicon-sort-by-alphabet-alt:before {
  content: "\e152";
}
.glyphicon-sort-by-order:before {
  content: "\e153";
}
.glyphicon-sort-by-order-alt:before {
  content: "\e154";
}
.glyphicon-sort-by-attributes:before {
  content: "\e155";
}
.glyphicon-sort-by-attributes-alt:before {
  content: "\e156";
}
.glyphicon-unchecked:before {
  content: "\e157";
}
.glyphicon-expand:before {
  content: "\e158";
}
.glyphicon-collapse-down:before {
  content: "\e159";
}
.glyphicon-collapse-up:before {
  content: "\e160";
}
.glyphicon-log-in:before {
  content: "\e161";
}
.glyphicon-flash:before {
  content: "\e162";
}
.glyphicon-log-out:before {
  content: "\e163";
}
.glyphicon-new-window:before {
  content: "\e164";
}
.glyphicon-record:before {
  content: "\e165";
}
.glyphicon-save:before {
  content: "\e166";
}
.glyphicon-open:before {
  content: "\e167";
}
.glyphicon-saved:before {
  content: "\e168";
}
.glyphicon-import:before {
  content: "\e169";
}
.glyphicon-export:before {
  content: "\e170";
}
.glyphicon-send:before {
  content: "\e171";
}
.glyphicon-floppy-disk:before {
  content: "\e172";
}
.glyphicon-floppy-saved:before {
  content: "\e173";
}
.glyphicon-floppy-remove:before {
  content: "\e174";
}
.glyphicon-floppy-save:before {
  content: "\e175";
}
.glyphicon-floppy-open:before {
  content: "\e176";
}
.glyphicon-credit-card:before {
  content: "\e177";
}
.glyphicon-transfer:before {
  content: "\e178";
}
.glyphicon-cutlery:before {
  content: "\e179";
}
.glyphicon-header:before {
  content: "\e180";
}
.glyphicon-compressed:before {
  content: "\e181";
}
.glyphicon-earphone:before {
  content: "\e182";
}
.glyphicon-phone-alt:before {
  content: "\e183";
}
.glyphicon-tower:before {
  content: "\e184";
}
.glyphicon-stats:before {
  content: "\e185";
}
.glyphicon-sd-video:before {
  content: "\e186";
}
.glyphicon-hd-video:before {
  content: "\e187";
}
.glyphicon-subtitles:before {
  content: "\e188";
}
.glyphicon-sound-stereo:before {
  content: "\e189";
}
.glyphicon-sound-dolby:before {
  content: "\e190";
}
.glyphicon-sound-5-1:before {
  content: "\e191";
}
.glyphicon-sound-6-1:before {
  content: "\e192";
}
.glyphicon-sound-7-1:before {
  content: "\e193";
}
.glyphicon-copyright-mark:before {
  content: "\e194";
}
.glyphicon-registration-mark:before {
  content: "\e195";
}
.glyphicon-cloud-download:before {
  content: "\e197";
}
.glyphicon-cloud-upload:before {
  content: "\e198";
}
.glyphicon-tree-conifer:before {
  content: "\e199";
}
.glyphicon-tree-deciduous:before {
  content: "\e200";
}
.glyphicon-cd:before {
  content: "\e201";
}
.glyphicon-save-file:before {
  content: "\e202";
}
.glyphicon-open-file:before {
  content: "\e203";
}
.glyphicon-level-up:before {
  content: "\e204";
}
.glyphicon-copy:before {
  content: "\e205";
}
.glyphicon-paste:before {
  content: "\e206";
}
.glyphicon-alert:before {
  content: "\e209";
}
.glyphicon-equalizer:before {
  content: "\e210";
}
.glyphicon-king:before {
  content: "\e211";
}
.glyphicon-queen:before {
  content: "\e212";
}
.glyphicon-pawn:before {
  content: "\e213";
}
.glyphicon-bishop:before {
  content: "\e214";
}
.glyphicon-knight:before {
  content: "\e215";
}
.glyphicon-baby-formula:before {
  content: "\e216";
}
.glyphicon-tent:before {
  content: "\26fa";
}
.glyphicon-blackboard:before {
  content: "\e218";
}
.glyphicon-bed:before {
  content: "\e219";
}
.glyphicon-apple:before {
  content: "\f8ff";
}
.glyphicon-erase:before {
  content: "\e221";
}
.glyphicon-hourglass:before {
  content: "\231b";
}
.glyphicon-lamp:before {
  content: "\e223";
}
.glyphicon-duplicate:before {
  content: "\e224";
}
.glyphicon-piggy-bank:before {
  content: "\e225";
}
.glyphicon-scissors:before {
  content: "\e226";
}
.glyphicon-bitcoin:before {
  content: "\e227";
}
.glyphicon-btc:before {
  content: "\e227";
}
.glyphicon-xbt:before {
  content: "\e227";
}
.glyphicon-yen:before {
  content: "\00a5";
}
.glyphicon-jpy:before {
  content: "\00a5";
}
.glyphicon-ruble:before {
  content: "\20bd";
}
.glyphicon-rub:before {
  content: "\20bd";
}
.glyphicon-scale:before {
  content: "\e230";
}
.glyphicon-ice-lolly:before {
  content: "\e231";
}
.glyphicon-ice-lolly-tasted:before {
  content: "\e232";
}
.glyphicon-education:before {
  content: "\e233";
}
.glyphicon-option-horizontal:before {
  content: "\e234";
}
.glyphicon-option-vertical:before {
  content: "\e235";
}
.glyphicon-menu-hamburger:before {
  content: "\e236";
}
.glyphicon-modal-window:before {
  content: "\e237";
}
.glyphicon-oil:before {
  content: "\e238";
}
.glyphicon-grain:before {
  content: "\e239";
}
.glyphicon-sunglasses:before {
  content: "\e240";
}
.glyphicon-text-size:before {
  content: "\e241";
}
.glyphicon-text-color:before {
  content: "\e242";
}
.glyphicon-text-background:before {
  content: "\e243";
}
.glyphicon-object-align-top:before {
  content: "\e244";
}
.glyphicon-object-align-bottom:before {
  content: "\e245";
}
.glyphicon-object-align-horizontal:before {
  content: "\e246";
}
.glyphicon-object-align-left:before {
  content: "\e247";
}
.glyphicon-object-align-vertical:before {
  content: "\e248";
}
.glyphicon-object-align-right:before {
  content: "\e249";
}
.glyphicon-triangle-right:before {
  content: "\e250";
}
.glyphicon-triangle-left:before {
  content: "\e251";
}
.glyphicon-triangle-bottom:before {
  content: "\e252";
}
.glyphicon-triangle-top:before {
  content: "\e253";
}
.glyphicon-console:before {
  content: "\e254";
}
.glyphicon-superscript:before {
  content: "\e255";
}
.glyphicon-subscript:before {
  content: "\e256";
}
.glyphicon-menu-left:before {
  content: "\e257";
}
.glyphicon-menu-right:before {
  content: "\e258";
}
.glyphicon-menu-down:before {
  content: "\e259";
}
.glyphicon-menu-up:before {
  content: "\e260";
}
* {
  -webkit-box-sizing: border-box;
  -moz-box-sizing: border-box;
  box-sizing: border-box;
}
*:before,
*:after {
  -webkit-box-sizing: border-box;
  -moz-box-sizing: border-box;
  box-sizing: border-box;
}
html {
  font-size: 10px;
  -webkit-tap-highlight-color: rgba(0, 0, 0, 0);
}
body {
  font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;
  font-size: 13px;
  line-height: 1.42857143;
  color: #000;
  background-color: #fff;
}
input,
button,
select,
textarea {
  font-family: inherit;
  font-size: inherit;
  line-height: inherit;
}
a {
  color: #337ab7;
  text-decoration: none;
}
a:hover,
a:focus {
  color: #23527c;
  text-decoration: underline;
}
a:focus {
  outline: 5px auto -webkit-focus-ring-color;
  outline-offset: -2px;
}
figure {
  margin: 0;
}
img {
  vertical-align: middle;
}
.img-responsive,
.thumbnail > img,
.thumbnail a > img,
.carousel-inner > .item > img,
.carousel-inner > .item > a > img {
  display: block;
  max-width: 100%;
  height: auto;
}
.img-rounded {
  border-radius: 3px;
}
.img-thumbnail {
  padding: 4px;
  line-height: 1.42857143;
  background-color: #fff;
  border: 1px solid #ddd;
  border-radius: 2px;
  -webkit-transition: all 0.2s ease-in-out;
  -o-transition: all 0.2s ease-in-out;
  transition: all 0.2s ease-in-out;
  display: inline-block;
  max-width: 100%;
  height: auto;
}
.img-circle {
  border-radius: 50%;
}
hr {
  margin-top: 18px;
  margin-bottom: 18px;
  border: 0;
  border-top: 1px solid #eeeeee;
}
.sr-only {
  position: absolute;
  width: 1px;
  height: 1px;
  margin: -1px;
  padding: 0;
  overflow: hidden;
  clip: rect(0, 0, 0, 0);
  border: 0;
}
.sr-only-focusable:active,
.sr-only-focusable:focus {
  position: static;
  width: auto;
  height: auto;
  margin: 0;
  overflow: visible;
  clip: auto;
}
[role="button"] {
  cursor: pointer;
}
h1,
h2,
h3,
h4,
h5,
h6,
.h1,
.h2,
.h3,
.h4,
.h5,
.h6 {
  font-family: inherit;
  font-weight: 500;
  line-height: 1.1;
  color: inherit;
}
h1 small,
h2 small,
h3 small,
h4 small,
h5 small,
h6 small,
.h1 small,
.h2 small,
.h3 small,
.h4 small,
.h5 small,
.h6 small,
h1 .small,
h2 .small,
h3 .small,
h4 .small,
h5 .small,
h6 .small,
.h1 .small,
.h2 .small,
.h3 .small,
.h4 .small,
.h5 .small,
.h6 .small {
  font-weight: normal;
  line-height: 1;
  color: #777777;
}
h1,
.h1,
h2,
.h2,
h3,
.h3 {
  margin-top: 18px;
  margin-bottom: 9px;
}
h1 small,
.h1 small,
h2 small,
.h2 small,
h3 small,
.h3 small,
h1 .small,
.h1 .small,
h2 .small,
.h2 .small,
h3 .small,
.h3 .small {
  font-size: 65%;
}
h4,
.h4,
h5,
.h5,
h6,
.h6 {
  margin-top: 9px;
  margin-bottom: 9px;
}
h4 small,
.h4 small,
h5 small,
.h5 small,
h6 small,
.h6 small,
h4 .small,
.h4 .small,
h5 .small,
.h5 .small,
h6 .small,
.h6 .small {
  font-size: 75%;
}
h1,
.h1 {
  font-size: 33px;
}
h2,
.h2 {
  font-size: 27px;
}
h3,
.h3 {
  font-size: 23px;
}
h4,
.h4 {
  font-size: 17px;
}
h5,
.h5 {
  font-size: 13px;
}
h6,
.h6 {
  font-size: 12px;
}
p {
  margin: 0 0 9px;
}
.lead {
  margin-bottom: 18px;
  font-size: 14px;
  font-weight: 300;
  line-height: 1.4;
}
@media (min-width: 768px) {
  .lead {
    font-size: 19.5px;
  }
}
small,
.small {
  font-size: 92%;
}
mark,
.mark {
  background-color: #fcf8e3;
  padding: .2em;
}
.text-left {
  text-align: left;
}
.text-right {
  text-align: right;
}
.text-center {
  text-align: center;
}
.text-justify {
  text-align: justify;
}
.text-nowrap {
  white-space: nowrap;
}
.text-lowercase {
  text-transform: lowercase;
}
.text-uppercase {
  text-transform: uppercase;
}
.text-capitalize {
  text-transform: capitalize;
}
.text-muted {
  color: #777777;
}
.text-primary {
  color: #337ab7;
}
a.text-primary:hover,
a.text-primary:focus {
  color: #286090;
}
.text-success {
  color: #3c763d;
}
a.text-success:hover,
a.text-success:focus {
  color: #2b542c;
}
.text-info {
  color: #31708f;
}
a.text-info:hover,
a.text-info:focus {
  color: #245269;
}
.text-warning {
  color: #8a6d3b;
}
a.text-warning:hover,
a.text-warning:focus {
  color: #66512c;
}
.text-danger {
  color: #a94442;
}
a.text-danger:hover,
a.text-danger:focus {
  color: #843534;
}
.bg-primary {
  color: #fff;
  background-color: #337ab7;
}
a.bg-primary:hover,
a.bg-primary:focus {
  background-color: #286090;
}
.bg-success {
  background-color: #dff0d8;
}
a.bg-success:hover,
a.bg-success:focus {
  background-color: #c1e2b3;
}
.bg-info {
  background-color: #d9edf7;
}
a.bg-info:hover,
a.bg-info:focus {
  background-color: #afd9ee;
}
.bg-warning {
  background-color: #fcf8e3;
}
a.bg-warning:hover,
a.bg-warning:focus {
  background-color: #f7ecb5;
}
.bg-danger {
  background-color: #f2dede;
}
a.bg-danger:hover,
a.bg-danger:focus {
  background-color: #e4b9b9;
}
.page-header {
  padding-bottom: 8px;
  margin: 36px 0 18px;
  border-bottom: 1px solid #eeeeee;
}
ul,
ol {
  margin-top: 0;
  margin-bottom: 9px;
}
ul ul,
ol ul,
ul ol,
ol ol {
  margin-bottom: 0;
}
.list-unstyled {
  padding-left: 0;
  list-style: none;
}
.list-inline {
  padding-left: 0;
  list-style: none;
  margin-left: -5px;
}
.list-inline > li {
  display: inline-block;
  padding-left: 5px;
  padding-right: 5px;
}
dl {
  margin-top: 0;
  margin-bottom: 18px;
}
dt,
dd {
  line-height: 1.42857143;
}
dt {
  font-weight: bold;
}
dd {
  margin-left: 0;
}
@media (min-width: 541px) {
  .dl-horizontal dt {
    float: left;
    width: 160px;
    clear: left;
    text-align: right;
    overflow: hidden;
    text-overflow: ellipsis;
    white-space: nowrap;
  }
  .dl-horizontal dd {
    margin-left: 180px;
  }
}
abbr[title],
abbr[data-original-title] {
  cursor: help;
  border-bottom: 1px dotted #777777;
}
.initialism {
  font-size: 90%;
  text-transform: uppercase;
}
blockquote {
  padding: 9px 18px;
  margin: 0 0 18px;
  font-size: inherit;
  border-left: 5px solid #eeeeee;
}
blockquote p:last-child,
blockquote ul:last-child,
blockquote ol:last-child {
  margin-bottom: 0;
}
blockquote footer,
blockquote small,
blockquote .small {
  display: block;
  font-size: 80%;
  line-height: 1.42857143;
  color: #777777;
}
blockquote footer:before,
blockquote small:before,
blockquote .small:before {
  content: '\2014 \00A0';
}
.blockquote-reverse,
blockquote.pull-right {
  padding-right: 15px;
  padding-left: 0;
  border-right: 5px solid #eeeeee;
  border-left: 0;
  text-align: right;
}
.blockquote-reverse footer:before,
blockquote.pull-right footer:before,
.blockquote-reverse small:before,
blockquote.pull-right small:before,
.blockquote-reverse .small:before,
blockquote.pull-right .small:before {
  content: '';
}
.blockquote-reverse footer:after,
blockquote.pull-right footer:after,
.blockquote-reverse small:after,
blockquote.pull-right small:after,
.blockquote-reverse .small:after,
blockquote.pull-right .small:after {
  content: '\00A0 \2014';
}
address {
  margin-bottom: 18px;
  font-style: normal;
  line-height: 1.42857143;
}
code,
kbd,
pre,
samp {
  font-family: monospace;
}
code {
  padding: 2px 4px;
  font-size: 90%;
  color: #c7254e;
  background-color: #f9f2f4;
  border-radius: 2px;
}
kbd {
  padding: 2px 4px;
  font-size: 90%;
  color: #888;
  background-color: transparent;
  border-radius: 1px;
  box-shadow: inset 0 -1px 0 rgba(0, 0, 0, 0.25);
}
kbd kbd {
  padding: 0;
  font-size: 100%;
  font-weight: bold;
  box-shadow: none;
}
pre {
  display: block;
  padding: 8.5px;
  margin: 0 0 9px;
  font-size: 12px;
  line-height: 1.42857143;
  word-break: break-all;
  word-wrap: break-word;
  color: #333333;
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 2px;
}
pre code {
  padding: 0;
  font-size: inherit;
  color: inherit;
  white-space: pre-wrap;
  background-color: transparent;
  border-radius: 0;
}
.pre-scrollable {
  max-height: 340px;
  overflow-y: scroll;
}
.container {
  margin-right: auto;
  margin-left: auto;
  padding-left: 0px;
  padding-right: 0px;
}
@media (min-width: 768px) {
  .container {
    width: 768px;
  }
}
@media (min-width: 992px) {
  .container {
    width: 940px;
  }
}
@media (min-width: 1200px) {
  .container {
    width: 1140px;
  }
}
.container-fluid {
  margin-right: auto;
  margin-left: auto;
  padding-left: 0px;
  padding-right: 0px;
}
.row {
  margin-left: 0px;
  margin-right: 0px;
}
.col-xs-1, .col-sm-1, .col-md-1, .col-lg-1, .col-xs-2, .col-sm-2, .col-md-2, .col-lg-2, .col-xs-3, .col-sm-3, .col-md-3, .col-lg-3, .col-xs-4, .col-sm-4, .col-md-4, .col-lg-4, .col-xs-5, .col-sm-5, .col-md-5, .col-lg-5, .col-xs-6, .col-sm-6, .col-md-6, .col-lg-6, .col-xs-7, .col-sm-7, .col-md-7, .col-lg-7, .col-xs-8, .col-sm-8, .col-md-8, .col-lg-8, .col-xs-9, .col-sm-9, .col-md-9, .col-lg-9, .col-xs-10, .col-sm-10, .col-md-10, .col-lg-10, .col-xs-11, .col-sm-11, .col-md-11, .col-lg-11, .col-xs-12, .col-sm-12, .col-md-12, .col-lg-12 {
  position: relative;
  min-height: 1px;
  padding-left: 0px;
  padding-right: 0px;
}
.col-xs-1, .col-xs-2, .col-xs-3, .col-xs-4, .col-xs-5, .col-xs-6, .col-xs-7, .col-xs-8, .col-xs-9, .col-xs-10, .col-xs-11, .col-xs-12 {
  float: left;
}
.col-xs-12 {
  width: 100%;
}
.col-xs-11 {
  width: 91.66666667%;
}
.col-xs-10 {
  width: 83.33333333%;
}
.col-xs-9 {
  width: 75%;
}
.col-xs-8 {
  width: 66.66666667%;
}
.col-xs-7 {
  width: 58.33333333%;
}
.col-xs-6 {
  width: 50%;
}
.col-xs-5 {
  width: 41.66666667%;
}
.col-xs-4 {
  width: 33.33333333%;
}
.col-xs-3 {
  width: 25%;
}
.col-xs-2 {
  width: 16.66666667%;
}
.col-xs-1 {
  width: 8.33333333%;
}
.col-xs-pull-12 {
  right: 100%;
}
.col-xs-pull-11 {
  right: 91.66666667%;
}
.col-xs-pull-10 {
  right: 83.33333333%;
}
.col-xs-pull-9 {
  right: 75%;
}
.col-xs-pull-8 {
  right: 66.66666667%;
}
.col-xs-pull-7 {
  right: 58.33333333%;
}
.col-xs-pull-6 {
  right: 50%;
}
.col-xs-pull-5 {
  right: 41.66666667%;
}
.col-xs-pull-4 {
  right: 33.33333333%;
}
.col-xs-pull-3 {
  right: 25%;
}
.col-xs-pull-2 {
  right: 16.66666667%;
}
.col-xs-pull-1 {
  right: 8.33333333%;
}
.col-xs-pull-0 {
  right: auto;
}
.col-xs-push-12 {
  left: 100%;
}
.col-xs-push-11 {
  left: 91.66666667%;
}
.col-xs-push-10 {
  left: 83.33333333%;
}
.col-xs-push-9 {
  left: 75%;
}
.col-xs-push-8 {
  left: 66.66666667%;
}
.col-xs-push-7 {
  left: 58.33333333%;
}
.col-xs-push-6 {
  left: 50%;
}
.col-xs-push-5 {
  left: 41.66666667%;
}
.col-xs-push-4 {
  left: 33.33333333%;
}
.col-xs-push-3 {
  left: 25%;
}
.col-xs-push-2 {
  left: 16.66666667%;
}
.col-xs-push-1 {
  left: 8.33333333%;
}
.col-xs-push-0 {
  left: auto;
}
.col-xs-offset-12 {
  margin-left: 100%;
}
.col-xs-offset-11 {
  margin-left: 91.66666667%;
}
.col-xs-offset-10 {
  margin-left: 83.33333333%;
}
.col-xs-offset-9 {
  margin-left: 75%;
}
.col-xs-offset-8 {
  margin-left: 66.66666667%;
}
.col-xs-offset-7 {
  margin-left: 58.33333333%;
}
.col-xs-offset-6 {
  margin-left: 50%;
}
.col-xs-offset-5 {
  margin-left: 41.66666667%;
}
.col-xs-offset-4 {
  margin-left: 33.33333333%;
}
.col-xs-offset-3 {
  margin-left: 25%;
}
.col-xs-offset-2 {
  margin-left: 16.66666667%;
}
.col-xs-offset-1 {
  margin-left: 8.33333333%;
}
.col-xs-offset-0 {
  margin-left: 0%;
}
@media (min-width: 768px) {
  .col-sm-1, .col-sm-2, .col-sm-3, .col-sm-4, .col-sm-5, .col-sm-6, .col-sm-7, .col-sm-8, .col-sm-9, .col-sm-10, .col-sm-11, .col-sm-12 {
    float: left;
  }
  .col-sm-12 {
    width: 100%;
  }
  .col-sm-11 {
    width: 91.66666667%;
  }
  .col-sm-10 {
    width: 83.33333333%;
  }
  .col-sm-9 {
    width: 75%;
  }
  .col-sm-8 {
    width: 66.66666667%;
  }
  .col-sm-7 {
    width: 58.33333333%;
  }
  .col-sm-6 {
    width: 50%;
  }
  .col-sm-5 {
    width: 41.66666667%;
  }
  .col-sm-4 {
    width: 33.33333333%;
  }
  .col-sm-3 {
    width: 25%;
  }
  .col-sm-2 {
    width: 16.66666667%;
  }
  .col-sm-1 {
    width: 8.33333333%;
  }
  .col-sm-pull-12 {
    right: 100%;
  }
  .col-sm-pull-11 {
    right: 91.66666667%;
  }
  .col-sm-pull-10 {
    right: 83.33333333%;
  }
  .col-sm-pull-9 {
    right: 75%;
  }
  .col-sm-pull-8 {
    right: 66.66666667%;
  }
  .col-sm-pull-7 {
    right: 58.33333333%;
  }
  .col-sm-pull-6 {
    right: 50%;
  }
  .col-sm-pull-5 {
    right: 41.66666667%;
  }
  .col-sm-pull-4 {
    right: 33.33333333%;
  }
  .col-sm-pull-3 {
    right: 25%;
  }
  .col-sm-pull-2 {
    right: 16.66666667%;
  }
  .col-sm-pull-1 {
    right: 8.33333333%;
  }
  .col-sm-pull-0 {
    right: auto;
  }
  .col-sm-push-12 {
    left: 100%;
  }
  .col-sm-push-11 {
    left: 91.66666667%;
  }
  .col-sm-push-10 {
    left: 83.33333333%;
  }
  .col-sm-push-9 {
    left: 75%;
  }
  .col-sm-push-8 {
    left: 66.66666667%;
  }
  .col-sm-push-7 {
    left: 58.33333333%;
  }
  .col-sm-push-6 {
    left: 50%;
  }
  .col-sm-push-5 {
    left: 41.66666667%;
  }
  .col-sm-push-4 {
    left: 33.33333333%;
  }
  .col-sm-push-3 {
    left: 25%;
  }
  .col-sm-push-2 {
    left: 16.66666667%;
  }
  .col-sm-push-1 {
    left: 8.33333333%;
  }
  .col-sm-push-0 {
    left: auto;
  }
  .col-sm-offset-12 {
    margin-left: 100%;
  }
  .col-sm-offset-11 {
    margin-left: 91.66666667%;
  }
  .col-sm-offset-10 {
    margin-left: 83.33333333%;
  }
  .col-sm-offset-9 {
    margin-left: 75%;
  }
  .col-sm-offset-8 {
    margin-left: 66.66666667%;
  }
  .col-sm-offset-7 {
    margin-left: 58.33333333%;
  }
  .col-sm-offset-6 {
    margin-left: 50%;
  }
  .col-sm-offset-5 {
    margin-left: 41.66666667%;
  }
  .col-sm-offset-4 {
    margin-left: 33.33333333%;
  }
  .col-sm-offset-3 {
    margin-left: 25%;
  }
  .col-sm-offset-2 {
    margin-left: 16.66666667%;
  }
  .col-sm-offset-1 {
    margin-left: 8.33333333%;
  }
  .col-sm-offset-0 {
    margin-left: 0%;
  }
}
@media (min-width: 992px) {
  .col-md-1, .col-md-2, .col-md-3, .col-md-4, .col-md-5, .col-md-6, .col-md-7, .col-md-8, .col-md-9, .col-md-10, .col-md-11, .col-md-12 {
    float: left;
  }
  .col-md-12 {
    width: 100%;
  }
  .col-md-11 {
    width: 91.66666667%;
  }
  .col-md-10 {
    width: 83.33333333%;
  }
  .col-md-9 {
    width: 75%;
  }
  .col-md-8 {
    width: 66.66666667%;
  }
  .col-md-7 {
    width: 58.33333333%;
  }
  .col-md-6 {
    width: 50%;
  }
  .col-md-5 {
    width: 41.66666667%;
  }
  .col-md-4 {
    width: 33.33333333%;
  }
  .col-md-3 {
    width: 25%;
  }
  .col-md-2 {
    width: 16.66666667%;
  }
  .col-md-1 {
    width: 8.33333333%;
  }
  .col-md-pull-12 {
    right: 100%;
  }
  .col-md-pull-11 {
    right: 91.66666667%;
  }
  .col-md-pull-10 {
    right: 83.33333333%;
  }
  .col-md-pull-9 {
    right: 75%;
  }
  .col-md-pull-8 {
    right: 66.66666667%;
  }
  .col-md-pull-7 {
    right: 58.33333333%;
  }
  .col-md-pull-6 {
    right: 50%;
  }
  .col-md-pull-5 {
    right: 41.66666667%;
  }
  .col-md-pull-4 {
    right: 33.33333333%;
  }
  .col-md-pull-3 {
    right: 25%;
  }
  .col-md-pull-2 {
    right: 16.66666667%;
  }
  .col-md-pull-1 {
    right: 8.33333333%;
  }
  .col-md-pull-0 {
    right: auto;
  }
  .col-md-push-12 {
    left: 100%;
  }
  .col-md-push-11 {
    left: 91.66666667%;
  }
  .col-md-push-10 {
    left: 83.33333333%;
  }
  .col-md-push-9 {
    left: 75%;
  }
  .col-md-push-8 {
    left: 66.66666667%;
  }
  .col-md-push-7 {
    left: 58.33333333%;
  }
  .col-md-push-6 {
    left: 50%;
  }
  .col-md-push-5 {
    left: 41.66666667%;
  }
  .col-md-push-4 {
    left: 33.33333333%;
  }
  .col-md-push-3 {
    left: 25%;
  }
  .col-md-push-2 {
    left: 16.66666667%;
  }
  .col-md-push-1 {
    left: 8.33333333%;
  }
  .col-md-push-0 {
    left: auto;
  }
  .col-md-offset-12 {
    margin-left: 100%;
  }
  .col-md-offset-11 {
    margin-left: 91.66666667%;
  }
  .col-md-offset-10 {
    margin-left: 83.33333333%;
  }
  .col-md-offset-9 {
    margin-left: 75%;
  }
  .col-md-offset-8 {
    margin-left: 66.66666667%;
  }
  .col-md-offset-7 {
    margin-left: 58.33333333%;
  }
  .col-md-offset-6 {
    margin-left: 50%;
  }
  .col-md-offset-5 {
    margin-left: 41.66666667%;
  }
  .col-md-offset-4 {
    margin-left: 33.33333333%;
  }
  .col-md-offset-3 {
    margin-left: 25%;
  }
  .col-md-offset-2 {
    margin-left: 16.66666667%;
  }
  .col-md-offset-1 {
    margin-left: 8.33333333%;
  }
  .col-md-offset-0 {
    margin-left: 0%;
  }
}
@media (min-width: 1200px) {
  .col-lg-1, .col-lg-2, .col-lg-3, .col-lg-4, .col-lg-5, .col-lg-6, .col-lg-7, .col-lg-8, .col-lg-9, .col-lg-10, .col-lg-11, .col-lg-12 {
    float: left;
  }
  .col-lg-12 {
    width: 100%;
  }
  .col-lg-11 {
    width: 91.66666667%;
  }
  .col-lg-10 {
    width: 83.33333333%;
  }
  .col-lg-9 {
    width: 75%;
  }
  .col-lg-8 {
    width: 66.66666667%;
  }
  .col-lg-7 {
    width: 58.33333333%;
  }
  .col-lg-6 {
    width: 50%;
  }
  .col-lg-5 {
    width: 41.66666667%;
  }
  .col-lg-4 {
    width: 33.33333333%;
  }
  .col-lg-3 {
    width: 25%;
  }
  .col-lg-2 {
    width: 16.66666667%;
  }
  .col-lg-1 {
    width: 8.33333333%;
  }
  .col-lg-pull-12 {
    right: 100%;
  }
  .col-lg-pull-11 {
    right: 91.66666667%;
  }
  .col-lg-pull-10 {
    right: 83.33333333%;
  }
  .col-lg-pull-9 {
    right: 75%;
  }
  .col-lg-pull-8 {
    right: 66.66666667%;
  }
  .col-lg-pull-7 {
    right: 58.33333333%;
  }
  .col-lg-pull-6 {
    right: 50%;
  }
  .col-lg-pull-5 {
    right: 41.66666667%;
  }
  .col-lg-pull-4 {
    right: 33.33333333%;
  }
  .col-lg-pull-3 {
    right: 25%;
  }
  .col-lg-pull-2 {
    right: 16.66666667%;
  }
  .col-lg-pull-1 {
    right: 8.33333333%;
  }
  .col-lg-pull-0 {
    right: auto;
  }
  .col-lg-push-12 {
    left: 100%;
  }
  .col-lg-push-11 {
    left: 91.66666667%;
  }
  .col-lg-push-10 {
    left: 83.33333333%;
  }
  .col-lg-push-9 {
    left: 75%;
  }
  .col-lg-push-8 {
    left: 66.66666667%;
  }
  .col-lg-push-7 {
    left: 58.33333333%;
  }
  .col-lg-push-6 {
    left: 50%;
  }
  .col-lg-push-5 {
    left: 41.66666667%;
  }
  .col-lg-push-4 {
    left: 33.33333333%;
  }
  .col-lg-push-3 {
    left: 25%;
  }
  .col-lg-push-2 {
    left: 16.66666667%;
  }
  .col-lg-push-1 {
    left: 8.33333333%;
  }
  .col-lg-push-0 {
    left: auto;
  }
  .col-lg-offset-12 {
    margin-left: 100%;
  }
  .col-lg-offset-11 {
    margin-left: 91.66666667%;
  }
  .col-lg-offset-10 {
    margin-left: 83.33333333%;
  }
  .col-lg-offset-9 {
    margin-left: 75%;
  }
  .col-lg-offset-8 {
    margin-left: 66.66666667%;
  }
  .col-lg-offset-7 {
    margin-left: 58.33333333%;
  }
  .col-lg-offset-6 {
    margin-left: 50%;
  }
  .col-lg-offset-5 {
    margin-left: 41.66666667%;
  }
  .col-lg-offset-4 {
    margin-left: 33.33333333%;
  }
  .col-lg-offset-3 {
    margin-left: 25%;
  }
  .col-lg-offset-2 {
    margin-left: 16.66666667%;
  }
  .col-lg-offset-1 {
    margin-left: 8.33333333%;
  }
  .col-lg-offset-0 {
    margin-left: 0%;
  }
}
table {
  background-color: transparent;
}
caption {
  padding-top: 8px;
  padding-bottom: 8px;
  color: #777777;
  text-align: left;
}
th {
  text-align: left;
}
.table {
  width: 100%;
  max-width: 100%;
  margin-bottom: 18px;
}
.table > thead > tr > th,
.table > tbody > tr > th,
.table > tfoot > tr > th,
.table > thead > tr > td,
.table > tbody > tr > td,
.table > tfoot > tr > td {
  padding: 8px;
  line-height: 1.42857143;
  vertical-align: top;
  border-top: 1px solid #ddd;
}
.table > thead > tr > th {
  vertical-align: bottom;
  border-bottom: 2px solid #ddd;
}
.table > caption + thead > tr:first-child > th,
.table > colgroup + thead > tr:first-child > th,
.table > thead:first-child > tr:first-child > th,
.table > caption + thead > tr:first-child > td,
.table > colgroup + thead > tr:first-child > td,
.table > thead:first-child > tr:first-child > td {
  border-top: 0;
}
.table > tbody + tbody {
  border-top: 2px solid #ddd;
}
.table .table {
  background-color: #fff;
}
.table-condensed > thead > tr > th,
.table-condensed > tbody > tr > th,
.table-condensed > tfoot > tr > th,
.table-condensed > thead > tr > td,
.table-condensed > tbody > tr > td,
.table-condensed > tfoot > tr > td {
  padding: 5px;
}
.table-bordered {
  border: 1px solid #ddd;
}
.table-bordered > thead > tr > th,
.table-bordered > tbody > tr > th,
.table-bordered > tfoot > tr > th,
.table-bordered > thead > tr > td,
.table-bordered > tbody > tr > td,
.table-bordered > tfoot > tr > td {
  border: 1px solid #ddd;
}
.table-bordered > thead > tr > th,
.table-bordered > thead > tr > td {
  border-bottom-width: 2px;
}
.table-striped > tbody > tr:nth-of-type(odd) {
  background-color: #f9f9f9;
}
.table-hover > tbody > tr:hover {
  background-color: #f5f5f5;
}
table col[class*="col-"] {
  position: static;
  float: none;
  display: table-column;
}
table td[class*="col-"],
table th[class*="col-"] {
  position: static;
  float: none;
  display: table-cell;
}
.table > thead > tr > td.active,
.table > tbody > tr > td.active,
.table > tfoot > tr > td.active,
.table > thead > tr > th.active,
.table > tbody > tr > th.active,
.table > tfoot > tr > th.active,
.table > thead > tr.active > td,
.table > tbody > tr.active > td,
.table > tfoot > tr.active > td,
.table > thead > tr.active > th,
.table > tbody > tr.active > th,
.table > tfoot > tr.active > th {
  background-color: #f5f5f5;
}
.table-hover > tbody > tr > td.active:hover,
.table-hover > tbody > tr > th.active:hover,
.table-hover > tbody > tr.active:hover > td,
.table-hover > tbody > tr:hover > .active,
.table-hover > tbody > tr.active:hover > th {
  background-color: #e8e8e8;
}
.table > thead > tr > td.success,
.table > tbody > tr > td.success,
.table > tfoot > tr > td.success,
.table > thead > tr > th.success,
.table > tbody > tr > th.success,
.table > tfoot > tr > th.success,
.table > thead > tr.success > td,
.table > tbody > tr.success > td,
.table > tfoot > tr.success > td,
.table > thead > tr.success > th,
.table > tbody > tr.success > th,
.table > tfoot > tr.success > th {
  background-color: #dff0d8;
}
.table-hover > tbody > tr > td.success:hover,
.table-hover > tbody > tr > th.success:hover,
.table-hover > tbody > tr.success:hover > td,
.table-hover > tbody > tr:hover > .success,
.table-hover > tbody > tr.success:hover > th {
  background-color: #d0e9c6;
}
.table > thead > tr > td.info,
.table > tbody > tr > td.info,
.table > tfoot > tr > td.info,
.table > thead > tr > th.info,
.table > tbody > tr > th.info,
.table > tfoot > tr > th.info,
.table > thead > tr.info > td,
.table > tbody > tr.info > td,
.table > tfoot > tr.info > td,
.table > thead > tr.info > th,
.table > tbody > tr.info > th,
.table > tfoot > tr.info > th {
  background-color: #d9edf7;
}
.table-hover > tbody > tr > td.info:hover,
.table-hover > tbody > tr > th.info:hover,
.table-hover > tbody > tr.info:hover > td,
.table-hover > tbody > tr:hover > .info,
.table-hover > tbody > tr.info:hover > th {
  background-color: #c4e3f3;
}
.table > thead > tr > td.warning,
.table > tbody > tr > td.warning,
.table > tfoot > tr > td.warning,
.table > thead > tr > th.warning,
.table > tbody > tr > th.warning,
.table > tfoot > tr > th.warning,
.table > thead > tr.warning > td,
.table > tbody > tr.warning > td,
.table > tfoot > tr.warning > td,
.table > thead > tr.warning > th,
.table > tbody > tr.warning > th,
.table > tfoot > tr.warning > th {
  background-color: #fcf8e3;
}
.table-hover > tbody > tr > td.warning:hover,
.table-hover > tbody > tr > th.warning:hover,
.table-hover > tbody > tr.warning:hover > td,
.table-hover > tbody > tr:hover > .warning,
.table-hover > tbody > tr.warning:hover > th {
  background-color: #faf2cc;
}
.table > thead > tr > td.danger,
.table > tbody > tr > td.danger,
.table > tfoot > tr > td.danger,
.table > thead > tr > th.danger,
.table > tbody > tr > th.danger,
.table > tfoot > tr > th.danger,
.table > thead > tr.danger > td,
.table > tbody > tr.danger > td,
.table > tfoot > tr.danger > td,
.table > thead > tr.danger > th,
.table > tbody > tr.danger > th,
.table > tfoot > tr.danger > th {
  background-color: #f2dede;
}
.table-hover > tbody > tr > td.danger:hover,
.table-hover > tbody > tr > th.danger:hover,
.table-hover > tbody > tr.danger:hover > td,
.table-hover > tbody > tr:hover > .danger,
.table-hover > tbody > tr.danger:hover > th {
  background-color: #ebcccc;
}
.table-responsive {
  overflow-x: auto;
  min-height: 0.01%;
}
@media screen and (max-width: 767px) {
  .table-responsive {
    width: 100%;
    margin-bottom: 13.5px;
    overflow-y: hidden;
    -ms-overflow-style: -ms-autohiding-scrollbar;
    border: 1px solid #ddd;
  }
  .table-responsive > .table {
    margin-bottom: 0;
  }
  .table-responsive > .table > thead > tr > th,
  .table-responsive > .table > tbody > tr > th,
  .table-responsive > .table > tfoot > tr > th,
  .table-responsive > .table > thead > tr > td,
  .table-responsive > .table > tbody > tr > td,
  .table-responsive > .table > tfoot > tr > td {
    white-space: nowrap;
  }
  .table-responsive > .table-bordered {
    border: 0;
  }
  .table-responsive > .table-bordered > thead > tr > th:first-child,
  .table-responsive > .table-bordered > tbody > tr > th:first-child,
  .table-responsive > .table-bordered > tfoot > tr > th:first-child,
  .table-responsive > .table-bordered > thead > tr > td:first-child,
  .table-responsive > .table-bordered > tbody > tr > td:first-child,
  .table-responsive > .table-bordered > tfoot > tr > td:first-child {
    border-left: 0;
  }
  .table-responsive > .table-bordered > thead > tr > th:last-child,
  .table-responsive > .table-bordered > tbody > tr > th:last-child,
  .table-responsive > .table-bordered > tfoot > tr > th:last-child,
  .table-responsive > .table-bordered > thead > tr > td:last-child,
  .table-responsive > .table-bordered > tbody > tr > td:last-child,
  .table-responsive > .table-bordered > tfoot > tr > td:last-child {
    border-right: 0;
  }
  .table-responsive > .table-bordered > tbody > tr:last-child > th,
  .table-responsive > .table-bordered > tfoot > tr:last-child > th,
  .table-responsive > .table-bordered > tbody > tr:last-child > td,
  .table-responsive > .table-bordered > tfoot > tr:last-child > td {
    border-bottom: 0;
  }
}
fieldset {
  padding: 0;
  margin: 0;
  border: 0;
  min-width: 0;
}
legend {
  display: block;
  width: 100%;
  padding: 0;
  margin-bottom: 18px;
  font-size: 19.5px;
  line-height: inherit;
  color: #333333;
  border: 0;
  border-bottom: 1px solid #e5e5e5;
}
label {
  display: inline-block;
  max-width: 100%;
  margin-bottom: 5px;
  font-weight: bold;
}
input[type="search"] {
  -webkit-box-sizing: border-box;
  -moz-box-sizing: border-box;
  box-sizing: border-box;
}
input[type="radio"],
input[type="checkbox"] {
  margin: 4px 0 0;
  margin-top: 1px \9;
  line-height: normal;
}
input[type="file"] {
  display: block;
}
input[type="range"] {
  display: block;
  width: 100%;
}
select[multiple],
select[size] {
  height: auto;
}
input[type="file"]:focus,
input[type="radio"]:focus,
input[type="checkbox"]:focus {
  outline: 5px auto -webkit-focus-ring-color;
  outline-offset: -2px;
}
output {
  display: block;
  padding-top: 7px;
  font-size: 13px;
  line-height: 1.42857143;
  color: #555555;
}
.form-control {
  display: block;
  width: 100%;
  height: 32px;
  padding: 6px 12px;
  font-size: 13px;
  line-height: 1.42857143;
  color: #555555;
  background-color: #fff;
  background-image: none;
  border: 1px solid #ccc;
  border-radius: 2px;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  -webkit-transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
  -o-transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
  transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
}
.form-control:focus {
  border-color: #66afe9;
  outline: 0;
  -webkit-box-shadow: inset 0 1px 1px rgba(0,0,0,.075), 0 0 8px rgba(102, 175, 233, 0.6);
  box-shadow: inset 0 1px 1px rgba(0,0,0,.075), 0 0 8px rgba(102, 175, 233, 0.6);
}
.form-control::-moz-placeholder {
  color: #999;
  opacity: 1;
}
.form-control:-ms-input-placeholder {
  color: #999;
}
.form-control::-webkit-input-placeholder {
  color: #999;
}
.form-control::-ms-expand {
  border: 0;
  background-color: transparent;
}
.form-control[disabled],
.form-control[readonly],
fieldset[disabled] .form-control {
  background-color: #eeeeee;
  opacity: 1;
}
.form-control[disabled],
fieldset[disabled] .form-control {
  cursor: not-allowed;
}
textarea.form-control {
  height: auto;
}
input[type="search"] {
  -webkit-appearance: none;
}
@media screen and (-webkit-min-device-pixel-ratio: 0) {
  input[type="date"].form-control,
  input[type="time"].form-control,
  input[type="datetime-local"].form-control,
  input[type="month"].form-control {
    line-height: 32px;
  }
  input[type="date"].input-sm,
  input[type="time"].input-sm,
  input[type="datetime-local"].input-sm,
  input[type="month"].input-sm,
  .input-group-sm input[type="date"],
  .input-group-sm input[type="time"],
  .input-group-sm input[type="datetime-local"],
  .input-group-sm input[type="month"] {
    line-height: 30px;
  }
  input[type="date"].input-lg,
  input[type="time"].input-lg,
  input[type="datetime-local"].input-lg,
  input[type="month"].input-lg,
  .input-group-lg input[type="date"],
  .input-group-lg input[type="time"],
  .input-group-lg input[type="datetime-local"],
  .input-group-lg input[type="month"] {
    line-height: 45px;
  }
}
.form-group {
  margin-bottom: 15px;
}
.radio,
.checkbox {
  position: relative;
  display: block;
  margin-top: 10px;
  margin-bottom: 10px;
}
.radio label,
.checkbox label {
  min-height: 18px;
  padding-left: 20px;
  margin-bottom: 0;
  font-weight: normal;
  cursor: pointer;
}
.radio input[type="radio"],
.radio-inline input[type="radio"],
.checkbox input[type="checkbox"],
.checkbox-inline input[type="checkbox"] {
  position: absolute;
  margin-left: -20px;
  margin-top: 4px \9;
}
.radio + .radio,
.checkbox + .checkbox {
  margin-top: -5px;
}
.radio-inline,
.checkbox-inline {
  position: relative;
  display: inline-block;
  padding-left: 20px;
  margin-bottom: 0;
  vertical-align: middle;
  font-weight: normal;
  cursor: pointer;
}
.radio-inline + .radio-inline,
.checkbox-inline + .checkbox-inline {
  margin-top: 0;
  margin-left: 10px;
}
input[type="radio"][disabled],
input[type="checkbox"][disabled],
input[type="radio"].disabled,
input[type="checkbox"].disabled,
fieldset[disabled] input[type="radio"],
fieldset[disabled] input[type="checkbox"] {
  cursor: not-allowed;
}
.radio-inline.disabled,
.checkbox-inline.disabled,
fieldset[disabled] .radio-inline,
fieldset[disabled] .checkbox-inline {
  cursor: not-allowed;
}
.radio.disabled label,
.checkbox.disabled label,
fieldset[disabled] .radio label,
fieldset[disabled] .checkbox label {
  cursor: not-allowed;
}
.form-control-static {
  padding-top: 7px;
  padding-bottom: 7px;
  margin-bottom: 0;
  min-height: 31px;
}
.form-control-static.input-lg,
.form-control-static.input-sm {
  padding-left: 0;
  padding-right: 0;
}
.input-sm {
  height: 30px;
  padding: 5px 10px;
  font-size: 12px;
  line-height: 1.5;
  border-radius: 1px;
}
select.input-sm {
  height: 30px;
  line-height: 30px;
}
textarea.input-sm,
select[multiple].input-sm {
  height: auto;
}
.form-group-sm .form-control {
  height: 30px;
  padding: 5px 10px;
  font-size: 12px;
  line-height: 1.5;
  border-radius: 1px;
}
.form-group-sm select.form-control {
  height: 30px;
  line-height: 30px;
}
.form-group-sm textarea.form-control,
.form-group-sm select[multiple].form-control {
  height: auto;
}
.form-group-sm .form-control-static {
  height: 30px;
  min-height: 30px;
  padding: 6px 10px;
  font-size: 12px;
  line-height: 1.5;
}
.input-lg {
  height: 45px;
  padding: 10px 16px;
  font-size: 17px;
  line-height: 1.3333333;
  border-radius: 3px;
}
select.input-lg {
  height: 45px;
  line-height: 45px;
}
textarea.input-lg,
select[multiple].input-lg {
  height: auto;
}
.form-group-lg .form-control {
  height: 45px;
  padding: 10px 16px;
  font-size: 17px;
  line-height: 1.3333333;
  border-radius: 3px;
}
.form-group-lg select.form-control {
  height: 45px;
  line-height: 45px;
}
.form-group-lg textarea.form-control,
.form-group-lg select[multiple].form-control {
  height: auto;
}
.form-group-lg .form-control-static {
  height: 45px;
  min-height: 35px;
  padding: 11px 16px;
  font-size: 17px;
  line-height: 1.3333333;
}
.has-feedback {
  position: relative;
}
.has-feedback .form-control {
  padding-right: 40px;
}
.form-control-feedback {
  position: absolute;
  top: 0;
  right: 0;
  z-index: 2;
  display: block;
  width: 32px;
  height: 32px;
  line-height: 32px;
  text-align: center;
  pointer-events: none;
}
.input-lg + .form-control-feedback,
.input-group-lg + .form-control-feedback,
.form-group-lg .form-control + .form-control-feedback {
  width: 45px;
  height: 45px;
  line-height: 45px;
}
.input-sm + .form-control-feedback,
.input-group-sm + .form-control-feedback,
.form-group-sm .form-control + .form-control-feedback {
  width: 30px;
  height: 30px;
  line-height: 30px;
}
.has-success .help-block,
.has-success .control-label,
.has-success .radio,
.has-success .checkbox,
.has-success .radio-inline,
.has-success .checkbox-inline,
.has-success.radio label,
.has-success.checkbox label,
.has-success.radio-inline label,
.has-success.checkbox-inline label {
  color: #3c763d;
}
.has-success .form-control {
  border-color: #3c763d;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
}
.has-success .form-control:focus {
  border-color: #2b542c;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075), 0 0 6px #67b168;
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075), 0 0 6px #67b168;
}
.has-success .input-group-addon {
  color: #3c763d;
  border-color: #3c763d;
  background-color: #dff0d8;
}
.has-success .form-control-feedback {
  color: #3c763d;
}
.has-warning .help-block,
.has-warning .control-label,
.has-warning .radio,
.has-warning .checkbox,
.has-warning .radio-inline,
.has-warning .checkbox-inline,
.has-warning.radio label,
.has-warning.checkbox label,
.has-warning.radio-inline label,
.has-warning.checkbox-inline label {
  color: #8a6d3b;
}
.has-warning .form-control {
  border-color: #8a6d3b;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
}
.has-warning .form-control:focus {
  border-color: #66512c;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075), 0 0 6px #c0a16b;
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075), 0 0 6px #c0a16b;
}
.has-warning .input-group-addon {
  color: #8a6d3b;
  border-color: #8a6d3b;
  background-color: #fcf8e3;
}
.has-warning .form-control-feedback {
  color: #8a6d3b;
}
.has-error .help-block,
.has-error .control-label,
.has-error .radio,
.has-error .checkbox,
.has-error .radio-inline,
.has-error .checkbox-inline,
.has-error.radio label,
.has-error.checkbox label,
.has-error.radio-inline label,
.has-error.checkbox-inline label {
  color: #a94442;
}
.has-error .form-control {
  border-color: #a94442;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
}
.has-error .form-control:focus {
  border-color: #843534;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075), 0 0 6px #ce8483;
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075), 0 0 6px #ce8483;
}
.has-error .input-group-addon {
  color: #a94442;
  border-color: #a94442;
  background-color: #f2dede;
}
.has-error .form-control-feedback {
  color: #a94442;
}
.has-feedback label ~ .form-control-feedback {
  top: 23px;
}
.has-feedback label.sr-only ~ .form-control-feedback {
  top: 0;
}
.help-block {
  display: block;
  margin-top: 5px;
  margin-bottom: 10px;
  color: #404040;
}
@media (min-width: 768px) {
  .form-inline .form-group {
    display: inline-block;
    margin-bottom: 0;
    vertical-align: middle;
  }
  .form-inline .form-control {
    display: inline-block;
    width: auto;
    vertical-align: middle;
  }
  .form-inline .form-control-static {
    display: inline-block;
  }
  .form-inline .input-group {
    display: inline-table;
    vertical-align: middle;
  }
  .form-inline .input-group .input-group-addon,
  .form-inline .input-group .input-group-btn,
  .form-inline .input-group .form-control {
    width: auto;
  }
  .form-inline .input-group > .form-control {
    width: 100%;
  }
  .form-inline .control-label {
    margin-bottom: 0;
    vertical-align: middle;
  }
  .form-inline .radio,
  .form-inline .checkbox {
    display: inline-block;
    margin-top: 0;
    margin-bottom: 0;
    vertical-align: middle;
  }
  .form-inline .radio label,
  .form-inline .checkbox label {
    padding-left: 0;
  }
  .form-inline .radio input[type="radio"],
  .form-inline .checkbox input[type="checkbox"] {
    position: relative;
    margin-left: 0;
  }
  .form-inline .has-feedback .form-control-feedback {
    top: 0;
  }
}
.form-horizontal .radio,
.form-horizontal .checkbox,
.form-horizontal .radio-inline,
.form-horizontal .checkbox-inline {
  margin-top: 0;
  margin-bottom: 0;
  padding-top: 7px;
}
.form-horizontal .radio,
.form-horizontal .checkbox {
  min-height: 25px;
}
.form-horizontal .form-group {
  margin-left: 0px;
  margin-right: 0px;
}
@media (min-width: 768px) {
  .form-horizontal .control-label {
    text-align: right;
    margin-bottom: 0;
    padding-top: 7px;
  }
}
.form-horizontal .has-feedback .form-control-feedback {
  right: 0px;
}
@media (min-width: 768px) {
  .form-horizontal .form-group-lg .control-label {
    padding-top: 11px;
    font-size: 17px;
  }
}
@media (min-width: 768px) {
  .form-horizontal .form-group-sm .control-label {
    padding-top: 6px;
    font-size: 12px;
  }
}
.btn {
  display: inline-block;
  margin-bottom: 0;
  font-weight: normal;
  text-align: center;
  vertical-align: middle;
  touch-action: manipulation;
  cursor: pointer;
  background-image: none;
  border: 1px solid transparent;
  white-space: nowrap;
  padding: 6px 12px;
  font-size: 13px;
  line-height: 1.42857143;
  border-radius: 2px;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}
.btn:focus,
.btn:active:focus,
.btn.active:focus,
.btn.focus,
.btn:active.focus,
.btn.active.focus {
  outline: 5px auto -webkit-focus-ring-color;
  outline-offset: -2px;
}
.btn:hover,
.btn:focus,
.btn.focus {
  color: #333;
  text-decoration: none;
}
.btn:active,
.btn.active {
  outline: 0;
  background-image: none;
  -webkit-box-shadow: inset 0 3px 5px rgba(0, 0, 0, 0.125);
  box-shadow: inset 0 3px 5px rgba(0, 0, 0, 0.125);
}
.btn.disabled,
.btn[disabled],
fieldset[disabled] .btn {
  cursor: not-allowed;
  opacity: 0.65;
  filter: alpha(opacity=65);
  -webkit-box-shadow: none;
  box-shadow: none;
}
a.btn.disabled,
fieldset[disabled] a.btn {
  pointer-events: none;
}
.btn-default {
  color: #333;
  background-color: #fff;
  border-color: #ccc;
}
.btn-default:focus,
.btn-default.focus {
  color: #333;
  background-color: #e6e6e6;
  border-color: #8c8c8c;
}
.btn-default:hover {
  color: #333;
  background-color: #e6e6e6;
  border-color: #adadad;
}
.btn-default:active,
.btn-default.active,
.open > .dropdown-toggle.btn-default {
  color: #333;
  background-color: #e6e6e6;
  border-color: #adadad;
}
.btn-default:active:hover,
.btn-default.active:hover,
.open > .dropdown-toggle.btn-default:hover,
.btn-default:active:focus,
.btn-default.active:focus,
.open > .dropdown-toggle.btn-default:focus,
.btn-default:active.focus,
.btn-default.active.focus,
.open > .dropdown-toggle.btn-default.focus {
  color: #333;
  background-color: #d4d4d4;
  border-color: #8c8c8c;
}
.btn-default:active,
.btn-default.active,
.open > .dropdown-toggle.btn-default {
  background-image: none;
}
.btn-default.disabled:hover,
.btn-default[disabled]:hover,
fieldset[disabled] .btn-default:hover,
.btn-default.disabled:focus,
.btn-default[disabled]:focus,
fieldset[disabled] .btn-default:focus,
.btn-default.disabled.focus,
.btn-default[disabled].focus,
fieldset[disabled] .btn-default.focus {
  background-color: #fff;
  border-color: #ccc;
}
.btn-default .badge {
  color: #fff;
  background-color: #333;
}
.btn-primary {
  color: #fff;
  background-color: #337ab7;
  border-color: #2e6da4;
}
.btn-primary:focus,
.btn-primary.focus {
  color: #fff;
  background-color: #286090;
  border-color: #122b40;
}
.btn-primary:hover {
  color: #fff;
  background-color: #286090;
  border-color: #204d74;
}
.btn-primary:active,
.btn-primary.active,
.open > .dropdown-toggle.btn-primary {
  color: #fff;
  background-color: #286090;
  border-color: #204d74;
}
.btn-primary:active:hover,
.btn-primary.active:hover,
.open > .dropdown-toggle.btn-primary:hover,
.btn-primary:active:focus,
.btn-primary.active:focus,
.open > .dropdown-toggle.btn-primary:focus,
.btn-primary:active.focus,
.btn-primary.active.focus,
.open > .dropdown-toggle.btn-primary.focus {
  color: #fff;
  background-color: #204d74;
  border-color: #122b40;
}
.btn-primary:active,
.btn-primary.active,
.open > .dropdown-toggle.btn-primary {
  background-image: none;
}
.btn-primary.disabled:hover,
.btn-primary[disabled]:hover,
fieldset[disabled] .btn-primary:hover,
.btn-primary.disabled:focus,
.btn-primary[disabled]:focus,
fieldset[disabled] .btn-primary:focus,
.btn-primary.disabled.focus,
.btn-primary[disabled].focus,
fieldset[disabled] .btn-primary.focus {
  background-color: #337ab7;
  border-color: #2e6da4;
}
.btn-primary .badge {
  color: #337ab7;
  background-color: #fff;
}
.btn-success {
  color: #fff;
  background-color: #5cb85c;
  border-color: #4cae4c;
}
.btn-success:focus,
.btn-success.focus {
  color: #fff;
  background-color: #449d44;
  border-color: #255625;
}
.btn-success:hover {
  color: #fff;
  background-color: #449d44;
  border-color: #398439;
}
.btn-success:active,
.btn-success.active,
.open > .dropdown-toggle.btn-success {
  color: #fff;
  background-color: #449d44;
  border-color: #398439;
}
.btn-success:active:hover,
.btn-success.active:hover,
.open > .dropdown-toggle.btn-success:hover,
.btn-success:active:focus,
.btn-success.active:focus,
.open > .dropdown-toggle.btn-success:focus,
.btn-success:active.focus,
.btn-success.active.focus,
.open > .dropdown-toggle.btn-success.focus {
  color: #fff;
  background-color: #398439;
  border-color: #255625;
}
.btn-success:active,
.btn-success.active,
.open > .dropdown-toggle.btn-success {
  background-image: none;
}
.btn-success.disabled:hover,
.btn-success[disabled]:hover,
fieldset[disabled] .btn-success:hover,
.btn-success.disabled:focus,
.btn-success[disabled]:focus,
fieldset[disabled] .btn-success:focus,
.btn-success.disabled.focus,
.btn-success[disabled].focus,
fieldset[disabled] .btn-success.focus {
  background-color: #5cb85c;
  border-color: #4cae4c;
}
.btn-success .badge {
  color: #5cb85c;
  background-color: #fff;
}
.btn-info {
  color: #fff;
  background-color: #5bc0de;
  border-color: #46b8da;
}
.btn-info:focus,
.btn-info.focus {
  color: #fff;
  background-color: #31b0d5;
  border-color: #1b6d85;
}
.btn-info:hover {
  color: #fff;
  background-color: #31b0d5;
  border-color: #269abc;
}
.btn-info:active,
.btn-info.active,
.open > .dropdown-toggle.btn-info {
  color: #fff;
  background-color: #31b0d5;
  border-color: #269abc;
}
.btn-info:active:hover,
.btn-info.active:hover,
.open > .dropdown-toggle.btn-info:hover,
.btn-info:active:focus,
.btn-info.active:focus,
.open > .dropdown-toggle.btn-info:focus,
.btn-info:active.focus,
.btn-info.active.focus,
.open > .dropdown-toggle.btn-info.focus {
  color: #fff;
  background-color: #269abc;
  border-color: #1b6d85;
}
.btn-info:active,
.btn-info.active,
.open > .dropdown-toggle.btn-info {
  background-image: none;
}
.btn-info.disabled:hover,
.btn-info[disabled]:hover,
fieldset[disabled] .btn-info:hover,
.btn-info.disabled:focus,
.btn-info[disabled]:focus,
fieldset[disabled] .btn-info:focus,
.btn-info.disabled.focus,
.btn-info[disabled].focus,
fieldset[disabled] .btn-info.focus {
  background-color: #5bc0de;
  border-color: #46b8da;
}
.btn-info .badge {
  color: #5bc0de;
  background-color: #fff;
}
.btn-warning {
  color: #fff;
  background-color: #f0ad4e;
  border-color: #eea236;
}
.btn-warning:focus,
.btn-warning.focus {
  color: #fff;
  background-color: #ec971f;
  border-color: #985f0d;
}
.btn-warning:hover {
  color: #fff;
  background-color: #ec971f;
  border-color: #d58512;
}
.btn-warning:active,
.btn-warning.active,
.open > .dropdown-toggle.btn-warning {
  color: #fff;
  background-color: #ec971f;
  border-color: #d58512;
}
.btn-warning:active:hover,
.btn-warning.active:hover,
.open > .dropdown-toggle.btn-warning:hover,
.btn-warning:active:focus,
.btn-warning.active:focus,
.open > .dropdown-toggle.btn-warning:focus,
.btn-warning:active.focus,
.btn-warning.active.focus,
.open > .dropdown-toggle.btn-warning.focus {
  color: #fff;
  background-color: #d58512;
  border-color: #985f0d;
}
.btn-warning:active,
.btn-warning.active,
.open > .dropdown-toggle.btn-warning {
  background-image: none;
}
.btn-warning.disabled:hover,
.btn-warning[disabled]:hover,
fieldset[disabled] .btn-warning:hover,
.btn-warning.disabled:focus,
.btn-warning[disabled]:focus,
fieldset[disabled] .btn-warning:focus,
.btn-warning.disabled.focus,
.btn-warning[disabled].focus,
fieldset[disabled] .btn-warning.focus {
  background-color: #f0ad4e;
  border-color: #eea236;
}
.btn-warning .badge {
  color: #f0ad4e;
  background-color: #fff;
}
.btn-danger {
  color: #fff;
  background-color: #d9534f;
  border-color: #d43f3a;
}
.btn-danger:focus,
.btn-danger.focus {
  color: #fff;
  background-color: #c9302c;
  border-color: #761c19;
}
.btn-danger:hover {
  color: #fff;
  background-color: #c9302c;
  border-color: #ac2925;
}
.btn-danger:active,
.btn-danger.active,
.open > .dropdown-toggle.btn-danger {
  color: #fff;
  background-color: #c9302c;
  border-color: #ac2925;
}
.btn-danger:active:hover,
.btn-danger.active:hover,
.open > .dropdown-toggle.btn-danger:hover,
.btn-danger:active:focus,
.btn-danger.active:focus,
.open > .dropdown-toggle.btn-danger:focus,
.btn-danger:active.focus,
.btn-danger.active.focus,
.open > .dropdown-toggle.btn-danger.focus {
  color: #fff;
  background-color: #ac2925;
  border-color: #761c19;
}
.btn-danger:active,
.btn-danger.active,
.open > .dropdown-toggle.btn-danger {
  background-image: none;
}
.btn-danger.disabled:hover,
.btn-danger[disabled]:hover,
fieldset[disabled] .btn-danger:hover,
.btn-danger.disabled:focus,
.btn-danger[disabled]:focus,
fieldset[disabled] .btn-danger:focus,
.btn-danger.disabled.focus,
.btn-danger[disabled].focus,
fieldset[disabled] .btn-danger.focus {
  background-color: #d9534f;
  border-color: #d43f3a;
}
.btn-danger .badge {
  color: #d9534f;
  background-color: #fff;
}
.btn-link {
  color: #337ab7;
  font-weight: normal;
  border-radius: 0;
}
.btn-link,
.btn-link:active,
.btn-link.active,
.btn-link[disabled],
fieldset[disabled] .btn-link {
  background-color: transparent;
  -webkit-box-shadow: none;
  box-shadow: none;
}
.btn-link,
.btn-link:hover,
.btn-link:focus,
.btn-link:active {
  border-color: transparent;
}
.btn-link:hover,
.btn-link:focus {
  color: #23527c;
  text-decoration: underline;
  background-color: transparent;
}
.btn-link[disabled]:hover,
fieldset[disabled] .btn-link:hover,
.btn-link[disabled]:focus,
fieldset[disabled] .btn-link:focus {
  color: #777777;
  text-decoration: none;
}
.btn-lg,
.btn-group-lg > .btn {
  padding: 10px 16px;
  font-size: 17px;
  line-height: 1.3333333;
  border-radius: 3px;
}
.btn-sm,
.btn-group-sm > .btn {
  padding: 5px 10px;
  font-size: 12px;
  line-height: 1.5;
  border-radius: 1px;
}
.btn-xs,
.btn-group-xs > .btn {
  padding: 1px 5px;
  font-size: 12px;
  line-height: 1.5;
  border-radius: 1px;
}
.btn-block {
  display: block;
  width: 100%;
}
.btn-block + .btn-block {
  margin-top: 5px;
}
input[type="submit"].btn-block,
input[type="reset"].btn-block,
input[type="button"].btn-block {
  width: 100%;
}
.fade {
  opacity: 0;
  -webkit-transition: opacity 0.15s linear;
  -o-transition: opacity 0.15s linear;
  transition: opacity 0.15s linear;
}
.fade.in {
  opacity: 1;
}
.collapse {
  display: none;
}
.collapse.in {
  display: block;
}
tr.collapse.in {
  display: table-row;
}
tbody.collapse.in {
  display: table-row-group;
}
.collapsing {
  position: relative;
  height: 0;
  overflow: hidden;
  -webkit-transition-property: height, visibility;
  transition-property: height, visibility;
  -webkit-transition-duration: 0.35s;
  transition-duration: 0.35s;
  -webkit-transition-timing-function: ease;
  transition-timing-function: ease;
}
.caret {
  display: inline-block;
  width: 0;
  height: 0;
  margin-left: 2px;
  vertical-align: middle;
  border-top: 4px dashed;
  border-top: 4px solid \9;
  border-right: 4px solid transparent;
  border-left: 4px solid transparent;
}
.dropup,
.dropdown {
  position: relative;
}
.dropdown-toggle:focus {
  outline: 0;
}
.dropdown-menu {
  position: absolute;
  top: 100%;
  left: 0;
  z-index: 1000;
  display: none;
  float: left;
  min-width: 160px;
  padding: 5px 0;
  margin: 2px 0 0;
  list-style: none;
  font-size: 13px;
  text-align: left;
  background-color: #fff;
  border: 1px solid #ccc;
  border: 1px solid rgba(0, 0, 0, 0.15);
  border-radius: 2px;
  -webkit-box-shadow: 0 6px 12px rgba(0, 0, 0, 0.175);
  box-shadow: 0 6px 12px rgba(0, 0, 0, 0.175);
  background-clip: padding-box;
}
.dropdown-menu.pull-right {
  right: 0;
  left: auto;
}
.dropdown-menu .divider {
  height: 1px;
  margin: 8px 0;
  overflow: hidden;
  background-color: #e5e5e5;
}
.dropdown-menu > li > a {
  display: block;
  padding: 3px 20px;
  clear: both;
  font-weight: normal;
  line-height: 1.42857143;
  color: #333333;
  white-space: nowrap;
}
.dropdown-menu > li > a:hover,
.dropdown-menu > li > a:focus {
  text-decoration: none;
  color: #262626;
  background-color: #f5f5f5;
}
.dropdown-menu > .active > a,
.dropdown-menu > .active > a:hover,
.dropdown-menu > .active > a:focus {
  color: #fff;
  text-decoration: none;
  outline: 0;
  background-color: #337ab7;
}
.dropdown-menu > .disabled > a,
.dropdown-menu > .disabled > a:hover,
.dropdown-menu > .disabled > a:focus {
  color: #777777;
}
.dropdown-menu > .disabled > a:hover,
.dropdown-menu > .disabled > a:focus {
  text-decoration: none;
  background-color: transparent;
  background-image: none;
  filter: progid:DXImageTransform.Microsoft.gradient(enabled = false);
  cursor: not-allowed;
}
.open > .dropdown-menu {
  display: block;
}
.open > a {
  outline: 0;
}
.dropdown-menu-right {
  left: auto;
  right: 0;
}
.dropdown-menu-left {
  left: 0;
  right: auto;
}
.dropdown-header {
  display: block;
  padding: 3px 20px;
  font-size: 12px;
  line-height: 1.42857143;
  color: #777777;
  white-space: nowrap;
}
.dropdown-backdrop {
  position: fixed;
  left: 0;
  right: 0;
  bottom: 0;
  top: 0;
  z-index: 990;
}
.pull-right > .dropdown-menu {
  right: 0;
  left: auto;
}
.dropup .caret,
.navbar-fixed-bottom .dropdown .caret {
  border-top: 0;
  border-bottom: 4px dashed;
  border-bottom: 4px solid \9;
  content: "";
}
.dropup .dropdown-menu,
.navbar-fixed-bottom .dropdown .dropdown-menu {
  top: auto;
  bottom: 100%;
  margin-bottom: 2px;
}
@media (min-width: 541px) {
  .navbar-right .dropdown-menu {
    left: auto;
    right: 0;
  }
  .navbar-right .dropdown-menu-left {
    left: 0;
    right: auto;
  }
}
.btn-group,
.btn-group-vertical {
  position: relative;
  display: inline-block;
  vertical-align: middle;
}
.btn-group > .btn,
.btn-group-vertical > .btn {
  position: relative;
  float: left;
}
.btn-group > .btn:hover,
.btn-group-vertical > .btn:hover,
.btn-group > .btn:focus,
.btn-group-vertical > .btn:focus,
.btn-group > .btn:active,
.btn-group-vertical > .btn:active,
.btn-group > .btn.active,
.btn-group-vertical > .btn.active {
  z-index: 2;
}
.btn-group .btn + .btn,
.btn-group .btn + .btn-group,
.btn-group .btn-group + .btn,
.btn-group .btn-group + .btn-group {
  margin-left: -1px;
}
.btn-toolbar {
  margin-left: -5px;
}
.btn-toolbar .btn,
.btn-toolbar .btn-group,
.btn-toolbar .input-group {
  float: left;
}
.btn-toolbar > .btn,
.btn-toolbar > .btn-group,
.btn-toolbar > .input-group {
  margin-left: 5px;
}
.btn-group > .btn:not(:first-child):not(:last-child):not(.dropdown-toggle) {
  border-radius: 0;
}
.btn-group > .btn:first-child {
  margin-left: 0;
}
.btn-group > .btn:first-child:not(:last-child):not(.dropdown-toggle) {
  border-bottom-right-radius: 0;
  border-top-right-radius: 0;
}
.btn-group > .btn:last-child:not(:first-child),
.btn-group > .dropdown-toggle:not(:first-child) {
  border-bottom-left-radius: 0;
  border-top-left-radius: 0;
}
.btn-group > .btn-group {
  float: left;
}
.btn-group > .btn-group:not(:first-child):not(:last-child) > .btn {
  border-radius: 0;
}
.btn-group > .btn-group:first-child:not(:last-child) > .btn:last-child,
.btn-group > .btn-group:first-child:not(:last-child) > .dropdown-toggle {
  border-bottom-right-radius: 0;
  border-top-right-radius: 0;
}
.btn-group > .btn-group:last-child:not(:first-child) > .btn:first-child {
  border-bottom-left-radius: 0;
  border-top-left-radius: 0;
}
.btn-group .dropdown-toggle:active,
.btn-group.open .dropdown-toggle {
  outline: 0;
}
.btn-group > .btn + .dropdown-toggle {
  padding-left: 8px;
  padding-right: 8px;
}
.btn-group > .btn-lg + .dropdown-toggle {
  padding-left: 12px;
  padding-right: 12px;
}
.btn-group.open .dropdown-toggle {
  -webkit-box-shadow: inset 0 3px 5px rgba(0, 0, 0, 0.125);
  box-shadow: inset 0 3px 5px rgba(0, 0, 0, 0.125);
}
.btn-group.open .dropdown-toggle.btn-link {
  -webkit-box-shadow: none;
  box-shadow: none;
}
.btn .caret {
  margin-left: 0;
}
.btn-lg .caret {
  border-width: 5px 5px 0;
  border-bottom-width: 0;
}
.dropup .btn-lg .caret {
  border-width: 0 5px 5px;
}
.btn-group-vertical > .btn,
.btn-group-vertical > .btn-group,
.btn-group-vertical > .btn-group > .btn {
  display: block;
  float: none;
  width: 100%;
  max-width: 100%;
}
.btn-group-vertical > .btn-group > .btn {
  float: none;
}
.btn-group-vertical > .btn + .btn,
.btn-group-vertical > .btn + .btn-group,
.btn-group-vertical > .btn-group + .btn,
.btn-group-vertical > .btn-group + .btn-group {
  margin-top: -1px;
  margin-left: 0;
}
.btn-group-vertical > .btn:not(:first-child):not(:last-child) {
  border-radius: 0;
}
.btn-group-vertical > .btn:first-child:not(:last-child) {
  border-top-right-radius: 2px;
  border-top-left-radius: 2px;
  border-bottom-right-radius: 0;
  border-bottom-left-radius: 0;
}
.btn-group-vertical > .btn:last-child:not(:first-child) {
  border-top-right-radius: 0;
  border-top-left-radius: 0;
  border-bottom-right-radius: 2px;
  border-bottom-left-radius: 2px;
}
.btn-group-vertical > .btn-group:not(:first-child):not(:last-child) > .btn {
  border-radius: 0;
}
.btn-group-vertical > .btn-group:first-child:not(:last-child) > .btn:last-child,
.btn-group-vertical > .btn-group:first-child:not(:last-child) > .dropdown-toggle {
  border-bottom-right-radius: 0;
  border-bottom-left-radius: 0;
}
.btn-group-vertical > .btn-group:last-child:not(:first-child) > .btn:first-child {
  border-top-right-radius: 0;
  border-top-left-radius: 0;
}
.btn-group-justified {
  display: table;
  width: 100%;
  table-layout: fixed;
  border-collapse: separate;
}
.btn-group-justified > .btn,
.btn-group-justified > .btn-group {
  float: none;
  display: table-cell;
  width: 1%;
}
.btn-group-justified > .btn-group .btn {
  width: 100%;
}
.btn-group-justified > .btn-group .dropdown-menu {
  left: auto;
}
[data-toggle="buttons"] > .btn input[type="radio"],
[data-toggle="buttons"] > .btn-group > .btn input[type="radio"],
[data-toggle="buttons"] > .btn input[type="checkbox"],
[data-toggle="buttons"] > .btn-group > .btn input[type="checkbox"] {
  position: absolute;
  clip: rect(0, 0, 0, 0);
  pointer-events: none;
}
.input-group {
  position: relative;
  display: table;
  border-collapse: separate;
}
.input-group[class*="col-"] {
  float: none;
  padding-left: 0;
  padding-right: 0;
}
.input-group .form-control {
  position: relative;
  z-index: 2;
  float: left;
  width: 100%;
  margin-bottom: 0;
}
.input-group .form-control:focus {
  z-index: 3;
}
.input-group-lg > .form-control,
.input-group-lg > .input-group-addon,
.input-group-lg > .input-group-btn > .btn {
  height: 45px;
  padding: 10px 16px;
  font-size: 17px;
  line-height: 1.3333333;
  border-radius: 3px;
}
select.input-group-lg > .form-control,
select.input-group-lg > .input-group-addon,
select.input-group-lg > .input-group-btn > .btn {
  height: 45px;
  line-height: 45px;
}
textarea.input-group-lg > .form-control,
textarea.input-group-lg > .input-group-addon,
textarea.input-group-lg > .input-group-btn > .btn,
select[multiple].input-group-lg > .form-control,
select[multiple].input-group-lg > .input-group-addon,
select[multiple].input-group-lg > .input-group-btn > .btn {
  height: auto;
}
.input-group-sm > .form-control,
.input-group-sm > .input-group-addon,
.input-group-sm > .input-group-btn > .btn {
  height: 30px;
  padding: 5px 10px;
  font-size: 12px;
  line-height: 1.5;
  border-radius: 1px;
}
select.input-group-sm > .form-control,
select.input-group-sm > .input-group-addon,
select.input-group-sm > .input-group-btn > .btn {
  height: 30px;
  line-height: 30px;
}
textarea.input-group-sm > .form-control,
textarea.input-group-sm > .input-group-addon,
textarea.input-group-sm > .input-group-btn > .btn,
select[multiple].input-group-sm > .form-control,
select[multiple].input-group-sm > .input-group-addon,
select[multiple].input-group-sm > .input-group-btn > .btn {
  height: auto;
}
.input-group-addon,
.input-group-btn,
.input-group .form-control {
  display: table-cell;
}
.input-group-addon:not(:first-child):not(:last-child),
.input-group-btn:not(:first-child):not(:last-child),
.input-group .form-control:not(:first-child):not(:last-child) {
  border-radius: 0;
}
.input-group-addon,
.input-group-btn {
  width: 1%;
  white-space: nowrap;
  vertical-align: middle;
}
.input-group-addon {
  padding: 6px 12px;
  font-size: 13px;
  font-weight: normal;
  line-height: 1;
  color: #555555;
  text-align: center;
  background-color: #eeeeee;
  border: 1px solid #ccc;
  border-radius: 2px;
}
.input-group-addon.input-sm {
  padding: 5px 10px;
  font-size: 12px;
  border-radius: 1px;
}
.input-group-addon.input-lg {
  padding: 10px 16px;
  font-size: 17px;
  border-radius: 3px;
}
.input-group-addon input[type="radio"],
.input-group-addon input[type="checkbox"] {
  margin-top: 0;
}
.input-group .form-control:first-child,
.input-group-addon:first-child,
.input-group-btn:first-child > .btn,
.input-group-btn:first-child > .btn-group > .btn,
.input-group-btn:first-child > .dropdown-toggle,
.input-group-btn:last-child > .btn:not(:last-child):not(.dropdown-toggle),
.input-group-btn:last-child > .btn-group:not(:last-child) > .btn {
  border-bottom-right-radius: 0;
  border-top-right-radius: 0;
}
.input-group-addon:first-child {
  border-right: 0;
}
.input-group .form-control:last-child,
.input-group-addon:last-child,
.input-group-btn:last-child > .btn,
.input-group-btn:last-child > .btn-group > .btn,
.input-group-btn:last-child > .dropdown-toggle,
.input-group-btn:first-child > .btn:not(:first-child),
.input-group-btn:first-child > .btn-group:not(:first-child) > .btn {
  border-bottom-left-radius: 0;
  border-top-left-radius: 0;
}
.input-group-addon:last-child {
  border-left: 0;
}
.input-group-btn {
  position: relative;
  font-size: 0;
  white-space: nowrap;
}
.input-group-btn > .btn {
  position: relative;
}
.input-group-btn > .btn + .btn {
  margin-left: -1px;
}
.input-group-btn > .btn:hover,
.input-group-btn > .btn:focus,
.input-group-btn > .btn:active {
  z-index: 2;
}
.input-group-btn:first-child > .btn,
.input-group-btn:first-child > .btn-group {
  margin-right: -1px;
}
.input-group-btn:last-child > .btn,
.input-group-btn:last-child > .btn-group {
  z-index: 2;
  margin-left: -1px;
}
.nav {
  margin-bottom: 0;
  padding-left: 0;
  list-style: none;
}
.nav > li {
  position: relative;
  display: block;
}
.nav > li > a {
  position: relative;
  display: block;
  padding: 10px 15px;
}
.nav > li > a:hover,
.nav > li > a:focus {
  text-decoration: none;
  background-color: #eeeeee;
}
.nav > li.disabled > a {
  color: #777777;
}
.nav > li.disabled > a:hover,
.nav > li.disabled > a:focus {
  color: #777777;
  text-decoration: none;
  background-color: transparent;
  cursor: not-allowed;
}
.nav .open > a,
.nav .open > a:hover,
.nav .open > a:focus {
  background-color: #eeeeee;
  border-color: #337ab7;
}
.nav .nav-divider {
  height: 1px;
  margin: 8px 0;
  overflow: hidden;
  background-color: #e5e5e5;
}
.nav > li > a > img {
  max-width: none;
}
.nav-tabs {
  border-bottom: 1px solid #ddd;
}
.nav-tabs > li {
  float: left;
  margin-bottom: -1px;
}
.nav-tabs > li > a {
  margin-right: 2px;
  line-height: 1.42857143;
  border: 1px solid transparent;
  border-radius: 2px 2px 0 0;
}
.nav-tabs > li > a:hover {
  border-color: #eeeeee #eeeeee #ddd;
}
.nav-tabs > li.active > a,
.nav-tabs > li.active > a:hover,
.nav-tabs > li.active > a:focus {
  color: #555555;
  background-color: #fff;
  border: 1px solid #ddd;
  border-bottom-color: transparent;
  cursor: default;
}
.nav-tabs.nav-justified {
  width: 100%;
  border-bottom: 0;
}
.nav-tabs.nav-justified > li {
  float: none;
}
.nav-tabs.nav-justified > li > a {
  text-align: center;
  margin-bottom: 5px;
}
.nav-tabs.nav-justified > .dropdown .dropdown-menu {
  top: auto;
  left: auto;
}
@media (min-width: 768px) {
  .nav-tabs.nav-justified > li {
    display: table-cell;
    width: 1%;
  }
  .nav-tabs.nav-justified > li > a {
    margin-bottom: 0;
  }
}
.nav-tabs.nav-justified > li > a {
  margin-right: 0;
  border-radius: 2px;
}
.nav-tabs.nav-justified > .active > a,
.nav-tabs.nav-justified > .active > a:hover,
.nav-tabs.nav-justified > .active > a:focus {
  border: 1px solid #ddd;
}
@media (min-width: 768px) {
  .nav-tabs.nav-justified > li > a {
    border-bottom: 1px solid #ddd;
    border-radius: 2px 2px 0 0;
  }
  .nav-tabs.nav-justified > .active > a,
  .nav-tabs.nav-justified > .active > a:hover,
  .nav-tabs.nav-justified > .active > a:focus {
    border-bottom-color: #fff;
  }
}
.nav-pills > li {
  float: left;
}
.nav-pills > li > a {
  border-radius: 2px;
}
.nav-pills > li + li {
  margin-left: 2px;
}
.nav-pills > li.active > a,
.nav-pills > li.active > a:hover,
.nav-pills > li.active > a:focus {
  color: #fff;
  background-color: #337ab7;
}
.nav-stacked > li {
  float: none;
}
.nav-stacked > li + li {
  margin-top: 2px;
  margin-left: 0;
}
.nav-justified {
  width: 100%;
}
.nav-justified > li {
  float: none;
}
.nav-justified > li > a {
  text-align: center;
  margin-bottom: 5px;
}
.nav-justified > .dropdown .dropdown-menu {
  top: auto;
  left: auto;
}
@media (min-width: 768px) {
  .nav-justified > li {
    display: table-cell;
    width: 1%;
  }
  .nav-justified > li > a {
    margin-bottom: 0;
  }
}
.nav-tabs-justified {
  border-bottom: 0;
}
.nav-tabs-justified > li > a {
  margin-right: 0;
  border-radius: 2px;
}
.nav-tabs-justified > .active > a,
.nav-tabs-justified > .active > a:hover,
.nav-tabs-justified > .active > a:focus {
  border: 1px solid #ddd;
}
@media (min-width: 768px) {
  .nav-tabs-justified > li > a {
    border-bottom: 1px solid #ddd;
    border-radius: 2px 2px 0 0;
  }
  .nav-tabs-justified > .active > a,
  .nav-tabs-justified > .active > a:hover,
  .nav-tabs-justified > .active > a:focus {
    border-bottom-color: #fff;
  }
}
.tab-content > .tab-pane {
  display: none;
}
.tab-content > .active {
  display: block;
}
.nav-tabs .dropdown-menu {
  margin-top: -1px;
  border-top-right-radius: 0;
  border-top-left-radius: 0;
}
.navbar {
  position: relative;
  min-height: 30px;
  margin-bottom: 18px;
  border: 1px solid transparent;
}
@media (min-width: 541px) {
  .navbar {
    border-radius: 2px;
  }
}
@media (min-width: 541px) {
  .navbar-header {
    float: left;
  }
}
.navbar-collapse {
  overflow-x: visible;
  padding-right: 0px;
  padding-left: 0px;
  border-top: 1px solid transparent;
  box-shadow: inset 0 1px 0 rgba(255, 255, 255, 0.1);
  -webkit-overflow-scrolling: touch;
}
.navbar-collapse.in {
  overflow-y: auto;
}
@media (min-width: 541px) {
  .navbar-collapse {
    width: auto;
    border-top: 0;
    box-shadow: none;
  }
  .navbar-collapse.collapse {
    display: block !important;
    height: auto !important;
    padding-bottom: 0;
    overflow: visible !important;
  }
  .navbar-collapse.in {
    overflow-y: visible;
  }
  .navbar-fixed-top .navbar-collapse,
  .navbar-static-top .navbar-collapse,
  .navbar-fixed-bottom .navbar-collapse {
    padding-left: 0;
    padding-right: 0;
  }
}
.navbar-fixed-top .navbar-collapse,
.navbar-fixed-bottom .navbar-collapse {
  max-height: 340px;
}
@media (max-device-width: 540px) and (orientation: landscape) {
  .navbar-fixed-top .navbar-collapse,
  .navbar-fixed-bottom .navbar-collapse {
    max-height: 200px;
  }
}
.container > .navbar-header,
.container-fluid > .navbar-header,
.container > .navbar-collapse,
.container-fluid > .navbar-collapse {
  margin-right: 0px;
  margin-left: 0px;
}
@media (min-width: 541px) {
  .container > .navbar-header,
  .container-fluid > .navbar-header,
  .container > .navbar-collapse,
  .container-fluid > .navbar-collapse {
    margin-right: 0;
    margin-left: 0;
  }
}
.navbar-static-top {
  z-index: 1000;
  border-width: 0 0 1px;
}
@media (min-width: 541px) {
  .navbar-static-top {
    border-radius: 0;
  }
}
.navbar-fixed-top,
.navbar-fixed-bottom {
  position: fixed;
  right: 0;
  left: 0;
  z-index: 1030;
}
@media (min-width: 541px) {
  .navbar-fixed-top,
  .navbar-fixed-bottom {
    border-radius: 0;
  }
}
.navbar-fixed-top {
  top: 0;
  border-width: 0 0 1px;
}
.navbar-fixed-bottom {
  bottom: 0;
  margin-bottom: 0;
  border-width: 1px 0 0;
}
.navbar-brand {
  float: left;
  padding: 6px 0px;
  font-size: 17px;
  line-height: 18px;
  height: 30px;
}
.navbar-brand:hover,
.navbar-brand:focus {
  text-decoration: none;
}
.navbar-brand > img {
  display: block;
}
@media (min-width: 541px) {
  .navbar > .container .navbar-brand,
  .navbar > .container-fluid .navbar-brand {
    margin-left: 0px;
  }
}
.navbar-toggle {
  position: relative;
  float: right;
  margin-right: 0px;
  padding: 9px 10px;
  margin-top: -2px;
  margin-bottom: -2px;
  background-color: transparent;
  background-image: none;
  border: 1px solid transparent;
  border-radius: 2px;
}
.navbar-toggle:focus {
  outline: 0;
}
.navbar-toggle .icon-bar {
  display: block;
  width: 22px;
  height: 2px;
  border-radius: 1px;
}
.navbar-toggle .icon-bar + .icon-bar {
  margin-top: 4px;
}
@media (min-width: 541px) {
  .navbar-toggle {
    display: none;
  }
}
.navbar-nav {
  margin: 3px 0px;
}
.navbar-nav > li > a {
  padding-top: 10px;
  padding-bottom: 10px;
  line-height: 18px;
}
@media (max-width: 540px) {
  .navbar-nav .open .dropdown-menu {
    position: static;
    float: none;
    width: auto;
    margin-top: 0;
    background-color: transparent;
    border: 0;
    box-shadow: none;
  }
  .navbar-nav .open .dropdown-menu > li > a,
  .navbar-nav .open .dropdown-menu .dropdown-header {
    padding: 5px 15px 5px 25px;
  }
  .navbar-nav .open .dropdown-menu > li > a {
    line-height: 18px;
  }
  .navbar-nav .open .dropdown-menu > li > a:hover,
  .navbar-nav .open .dropdown-menu > li > a:focus {
    background-image: none;
  }
}
@media (min-width: 541px) {
  .navbar-nav {
    float: left;
    margin: 0;
  }
  .navbar-nav > li {
    float: left;
  }
  .navbar-nav > li > a {
    padding-top: 6px;
    padding-bottom: 6px;
  }
}
.navbar-form {
  margin-left: 0px;
  margin-right: 0px;
  padding: 10px 0px;
  border-top: 1px solid transparent;
  border-bottom: 1px solid transparent;
  -webkit-box-shadow: inset 0 1px 0 rgba(255, 255, 255, 0.1), 0 1px 0 rgba(255, 255, 255, 0.1);
  box-shadow: inset 0 1px 0 rgba(255, 255, 255, 0.1), 0 1px 0 rgba(255, 255, 255, 0.1);
  margin-top: -1px;
  margin-bottom: -1px;
}
@media (min-width: 768px) {
  .navbar-form .form-group {
    display: inline-block;
    margin-bottom: 0;
    vertical-align: middle;
  }
  .navbar-form .form-control {
    display: inline-block;
    width: auto;
    vertical-align: middle;
  }
  .navbar-form .form-control-static {
    display: inline-block;
  }
  .navbar-form .input-group {
    display: inline-table;
    vertical-align: middle;
  }
  .navbar-form .input-group .input-group-addon,
  .navbar-form .input-group .input-group-btn,
  .navbar-form .input-group .form-control {
    width: auto;
  }
  .navbar-form .input-group > .form-control {
    width: 100%;
  }
  .navbar-form .control-label {
    margin-bottom: 0;
    vertical-align: middle;
  }
  .navbar-form .radio,
  .navbar-form .checkbox {
    display: inline-block;
    margin-top: 0;
    margin-bottom: 0;
    vertical-align: middle;
  }
  .navbar-form .radio label,
  .navbar-form .checkbox label {
    padding-left: 0;
  }
  .navbar-form .radio input[type="radio"],
  .navbar-form .checkbox input[type="checkbox"] {
    position: relative;
    margin-left: 0;
  }
  .navbar-form .has-feedback .form-control-feedback {
    top: 0;
  }
}
@media (max-width: 540px) {
  .navbar-form .form-group {
    margin-bottom: 5px;
  }
  .navbar-form .form-group:last-child {
    margin-bottom: 0;
  }
}
@media (min-width: 541px) {
  .navbar-form {
    width: auto;
    border: 0;
    margin-left: 0;
    margin-right: 0;
    padding-top: 0;
    padding-bottom: 0;
    -webkit-box-shadow: none;
    box-shadow: none;
  }
}
.navbar-nav > li > .dropdown-menu {
  margin-top: 0;
  border-top-right-radius: 0;
  border-top-left-radius: 0;
}
.navbar-fixed-bottom .navbar-nav > li > .dropdown-menu {
  margin-bottom: 0;
  border-top-right-radius: 2px;
  border-top-left-radius: 2px;
  border-bottom-right-radius: 0;
  border-bottom-left-radius: 0;
}
.navbar-btn {
  margin-top: -1px;
  margin-bottom: -1px;
}
.navbar-btn.btn-sm {
  margin-top: 0px;
  margin-bottom: 0px;
}
.navbar-btn.btn-xs {
  margin-top: 4px;
  margin-bottom: 4px;
}
.navbar-text {
  margin-top: 6px;
  margin-bottom: 6px;
}
@media (min-width: 541px) {
  .navbar-text {
    float: left;
    margin-left: 0px;
    margin-right: 0px;
  }
}
@media (min-width: 541px) {
  .navbar-left {
    float: left !important;
    float: left;
  }
  .navbar-right {
    float: right !important;
    float: right;
    margin-right: 0px;
  }
  .navbar-right ~ .navbar-right {
    margin-right: 0;
  }
}
.navbar-default {
  background-color: #f8f8f8;
  border-color: #e7e7e7;
}
.navbar-default .navbar-brand {
  color: #777;
}
.navbar-default .navbar-brand:hover,
.navbar-default .navbar-brand:focus {
  color: #5e5e5e;
  background-color: transparent;
}
.navbar-default .navbar-text {
  color: #777;
}
.navbar-default .navbar-nav > li > a {
  color: #777;
}
.navbar-default .navbar-nav > li > a:hover,
.navbar-default .navbar-nav > li > a:focus {
  color: #333;
  background-color: transparent;
}
.navbar-default .navbar-nav > .active > a,
.navbar-default .navbar-nav > .active > a:hover,
.navbar-default .navbar-nav > .active > a:focus {
  color: #555;
  background-color: #e7e7e7;
}
.navbar-default .navbar-nav > .disabled > a,
.navbar-default .navbar-nav > .disabled > a:hover,
.navbar-default .navbar-nav > .disabled > a:focus {
  color: #ccc;
  background-color: transparent;
}
.navbar-default .navbar-toggle {
  border-color: #ddd;
}
.navbar-default .navbar-toggle:hover,
.navbar-default .navbar-toggle:focus {
  background-color: #ddd;
}
.navbar-default .navbar-toggle .icon-bar {
  background-color: #888;
}
.navbar-default .navbar-collapse,
.navbar-default .navbar-form {
  border-color: #e7e7e7;
}
.navbar-default .navbar-nav > .open > a,
.navbar-default .navbar-nav > .open > a:hover,
.navbar-default .navbar-nav > .open > a:focus {
  background-color: #e7e7e7;
  color: #555;
}
@media (max-width: 540px) {
  .navbar-default .navbar-nav .open .dropdown-menu > li > a {
    color: #777;
  }
  .navbar-default .navbar-nav .open .dropdown-menu > li > a:hover,
  .navbar-default .navbar-nav .open .dropdown-menu > li > a:focus {
    color: #333;
    background-color: transparent;
  }
  .navbar-default .navbar-nav .open .dropdown-menu > .active > a,
  .navbar-default .navbar-nav .open .dropdown-menu > .active > a:hover,
  .navbar-default .navbar-nav .open .dropdown-menu > .active > a:focus {
    color: #555;
    background-color: #e7e7e7;
  }
  .navbar-default .navbar-nav .open .dropdown-menu > .disabled > a,
  .navbar-default .navbar-nav .open .dropdown-menu > .disabled > a:hover,
  .navbar-default .navbar-nav .open .dropdown-menu > .disabled > a:focus {
    color: #ccc;
    background-color: transparent;
  }
}
.navbar-default .navbar-link {
  color: #777;
}
.navbar-default .navbar-link:hover {
  color: #333;
}
.navbar-default .btn-link {
  color: #777;
}
.navbar-default .btn-link:hover,
.navbar-default .btn-link:focus {
  color: #333;
}
.navbar-default .btn-link[disabled]:hover,
fieldset[disabled] .navbar-default .btn-link:hover,
.navbar-default .btn-link[disabled]:focus,
fieldset[disabled] .navbar-default .btn-link:focus {
  color: #ccc;
}
.navbar-inverse {
  background-color: #222;
  border-color: #080808;
}
.navbar-inverse .navbar-brand {
  color: #9d9d9d;
}
.navbar-inverse .navbar-brand:hover,
.navbar-inverse .navbar-brand:focus {
  color: #fff;
  background-color: transparent;
}
.navbar-inverse .navbar-text {
  color: #9d9d9d;
}
.navbar-inverse .navbar-nav > li > a {
  color: #9d9d9d;
}
.navbar-inverse .navbar-nav > li > a:hover,
.navbar-inverse .navbar-nav > li > a:focus {
  color: #fff;
  background-color: transparent;
}
.navbar-inverse .navbar-nav > .active > a,
.navbar-inverse .navbar-nav > .active > a:hover,
.navbar-inverse .navbar-nav > .active > a:focus {
  color: #fff;
  background-color: #080808;
}
.navbar-inverse .navbar-nav > .disabled > a,
.navbar-inverse .navbar-nav > .disabled > a:hover,
.navbar-inverse .navbar-nav > .disabled > a:focus {
  color: #444;
  background-color: transparent;
}
.navbar-inverse .navbar-toggle {
  border-color: #333;
}
.navbar-inverse .navbar-toggle:hover,
.navbar-inverse .navbar-toggle:focus {
  background-color: #333;
}
.navbar-inverse .navbar-toggle .icon-bar {
  background-color: #fff;
}
.navbar-inverse .navbar-collapse,
.navbar-inverse .navbar-form {
  border-color: #101010;
}
.navbar-inverse .navbar-nav > .open > a,
.navbar-inverse .navbar-nav > .open > a:hover,
.navbar-inverse .navbar-nav > .open > a:focus {
  background-color: #080808;
  color: #fff;
}
@media (max-width: 540px) {
  .navbar-inverse .navbar-nav .open .dropdown-menu > .dropdown-header {
    border-color: #080808;
  }
  .navbar-inverse .navbar-nav .open .dropdown-menu .divider {
    background-color: #080808;
  }
  .navbar-inverse .navbar-nav .open .dropdown-menu > li > a {
    color: #9d9d9d;
  }
  .navbar-inverse .navbar-nav .open .dropdown-menu > li > a:hover,
  .navbar-inverse .navbar-nav .open .dropdown-menu > li > a:focus {
    color: #fff;
    background-color: transparent;
  }
  .navbar-inverse .navbar-nav .open .dropdown-menu > .active > a,
  .navbar-inverse .navbar-nav .open .dropdown-menu > .active > a:hover,
  .navbar-inverse .navbar-nav .open .dropdown-menu > .active > a:focus {
    color: #fff;
    background-color: #080808;
  }
  .navbar-inverse .navbar-nav .open .dropdown-menu > .disabled > a,
  .navbar-inverse .navbar-nav .open .dropdown-menu > .disabled > a:hover,
  .navbar-inverse .navbar-nav .open .dropdown-menu > .disabled > a:focus {
    color: #444;
    background-color: transparent;
  }
}
.navbar-inverse .navbar-link {
  color: #9d9d9d;
}
.navbar-inverse .navbar-link:hover {
  color: #fff;
}
.navbar-inverse .btn-link {
  color: #9d9d9d;
}
.navbar-inverse .btn-link:hover,
.navbar-inverse .btn-link:focus {
  color: #fff;
}
.navbar-inverse .btn-link[disabled]:hover,
fieldset[disabled] .navbar-inverse .btn-link:hover,
.navbar-inverse .btn-link[disabled]:focus,
fieldset[disabled] .navbar-inverse .btn-link:focus {
  color: #444;
}
.breadcrumb {
  padding: 8px 15px;
  margin-bottom: 18px;
  list-style: none;
  background-color: #f5f5f5;
  border-radius: 2px;
}
.breadcrumb > li {
  display: inline-block;
}
.breadcrumb > li + li:before {
  content: "/\00a0";
  padding: 0 5px;
  color: #5e5e5e;
}
.breadcrumb > .active {
  color: #777777;
}
.pagination {
  display: inline-block;
  padding-left: 0;
  margin: 18px 0;
  border-radius: 2px;
}
.pagination > li {
  display: inline;
}
.pagination > li > a,
.pagination > li > span {
  position: relative;
  float: left;
  padding: 6px 12px;
  line-height: 1.42857143;
  text-decoration: none;
  color: #337ab7;
  background-color: #fff;
  border: 1px solid #ddd;
  margin-left: -1px;
}
.pagination > li:first-child > a,
.pagination > li:first-child > span {
  margin-left: 0;
  border-bottom-left-radius: 2px;
  border-top-left-radius: 2px;
}
.pagination > li:last-child > a,
.pagination > li:last-child > span {
  border-bottom-right-radius: 2px;
  border-top-right-radius: 2px;
}
.pagination > li > a:hover,
.pagination > li > span:hover,
.pagination > li > a:focus,
.pagination > li > span:focus {
  z-index: 2;
  color: #23527c;
  background-color: #eeeeee;
  border-color: #ddd;
}
.pagination > .active > a,
.pagination > .active > span,
.pagination > .active > a:hover,
.pagination > .active > span:hover,
.pagination > .active > a:focus,
.pagination > .active > span:focus {
  z-index: 3;
  color: #fff;
  background-color: #337ab7;
  border-color: #337ab7;
  cursor: default;
}
.pagination > .disabled > span,
.pagination > .disabled > span:hover,
.pagination > .disabled > span:focus,
.pagination > .disabled > a,
.pagination > .disabled > a:hover,
.pagination > .disabled > a:focus {
  color: #777777;
  background-color: #fff;
  border-color: #ddd;
  cursor: not-allowed;
}
.pagination-lg > li > a,
.pagination-lg > li > span {
  padding: 10px 16px;
  font-size: 17px;
  line-height: 1.3333333;
}
.pagination-lg > li:first-child > a,
.pagination-lg > li:first-child > span {
  border-bottom-left-radius: 3px;
  border-top-left-radius: 3px;
}
.pagination-lg > li:last-child > a,
.pagination-lg > li:last-child > span {
  border-bottom-right-radius: 3px;
  border-top-right-radius: 3px;
}
.pagination-sm > li > a,
.pagination-sm > li > span {
  padding: 5px 10px;
  font-size: 12px;
  line-height: 1.5;
}
.pagination-sm > li:first-child > a,
.pagination-sm > li:first-child > span {
  border-bottom-left-radius: 1px;
  border-top-left-radius: 1px;
}
.pagination-sm > li:last-child > a,
.pagination-sm > li:last-child > span {
  border-bottom-right-radius: 1px;
  border-top-right-radius: 1px;
}
.pager {
  padding-left: 0;
  margin: 18px 0;
  list-style: none;
  text-align: center;
}
.pager li {
  display: inline;
}
.pager li > a,
.pager li > span {
  display: inline-block;
  padding: 5px 14px;
  background-color: #fff;
  border: 1px solid #ddd;
  border-radius: 15px;
}
.pager li > a:hover,
.pager li > a:focus {
  text-decoration: none;
  background-color: #eeeeee;
}
.pager .next > a,
.pager .next > span {
  float: right;
}
.pager .previous > a,
.pager .previous > span {
  float: left;
}
.pager .disabled > a,
.pager .disabled > a:hover,
.pager .disabled > a:focus,
.pager .disabled > span {
  color: #777777;
  background-color: #fff;
  cursor: not-allowed;
}
.label {
  display: inline;
  padding: .2em .6em .3em;
  font-size: 75%;
  font-weight: bold;
  line-height: 1;
  color: #fff;
  text-align: center;
  white-space: nowrap;
  vertical-align: baseline;
  border-radius: .25em;
}
a.label:hover,
a.label:focus {
  color: #fff;
  text-decoration: none;
  cursor: pointer;
}
.label:empty {
  display: none;
}
.btn .label {
  position: relative;
  top: -1px;
}
.label-default {
  background-color: #777777;
}
.label-default[href]:hover,
.label-default[href]:focus {
  background-color: #5e5e5e;
}
.label-primary {
  background-color: #337ab7;
}
.label-primary[href]:hover,
.label-primary[href]:focus {
  background-color: #286090;
}
.label-success {
  background-color: #5cb85c;
}
.label-success[href]:hover,
.label-success[href]:focus {
  background-color: #449d44;
}
.label-info {
  background-color: #5bc0de;
}
.label-info[href]:hover,
.label-info[href]:focus {
  background-color: #31b0d5;
}
.label-warning {
  background-color: #f0ad4e;
}
.label-warning[href]:hover,
.label-warning[href]:focus {
  background-color: #ec971f;
}
.label-danger {
  background-color: #d9534f;
}
.label-danger[href]:hover,
.label-danger[href]:focus {
  background-color: #c9302c;
}
.badge {
  display: inline-block;
  min-width: 10px;
  padding: 3px 7px;
  font-size: 12px;
  font-weight: bold;
  color: #fff;
  line-height: 1;
  vertical-align: middle;
  white-space: nowrap;
  text-align: center;
  background-color: #777777;
  border-radius: 10px;
}
.badge:empty {
  display: none;
}
.btn .badge {
  position: relative;
  top: -1px;
}
.btn-xs .badge,
.btn-group-xs > .btn .badge {
  top: 0;
  padding: 1px 5px;
}
a.badge:hover,
a.badge:focus {
  color: #fff;
  text-decoration: none;
  cursor: pointer;
}
.list-group-item.active > .badge,
.nav-pills > .active > a > .badge {
  color: #337ab7;
  background-color: #fff;
}
.list-group-item > .badge {
  float: right;
}
.list-group-item > .badge + .badge {
  margin-right: 5px;
}
.nav-pills > li > a > .badge {
  margin-left: 3px;
}
.jumbotron {
  padding-top: 30px;
  padding-bottom: 30px;
  margin-bottom: 30px;
  color: inherit;
  background-color: #eeeeee;
}
.jumbotron h1,
.jumbotron .h1 {
  color: inherit;
}
.jumbotron p {
  margin-bottom: 15px;
  font-size: 20px;
  font-weight: 200;
}
.jumbotron > hr {
  border-top-color: #d5d5d5;
}
.container .jumbotron,
.container-fluid .jumbotron {
  border-radius: 3px;
  padding-left: 0px;
  padding-right: 0px;
}
.jumbotron .container {
  max-width: 100%;
}
@media screen and (min-width: 768px) {
  .jumbotron {
    padding-top: 48px;
    padding-bottom: 48px;
  }
  .container .jumbotron,
  .container-fluid .jumbotron {
    padding-left: 60px;
    padding-right: 60px;
  }
  .jumbotron h1,
  .jumbotron .h1 {
    font-size: 59px;
  }
}
.thumbnail {
  display: block;
  padding: 4px;
  margin-bottom: 18px;
  line-height: 1.42857143;
  background-color: #fff;
  border: 1px solid #ddd;
  border-radius: 2px;
  -webkit-transition: border 0.2s ease-in-out;
  -o-transition: border 0.2s ease-in-out;
  transition: border 0.2s ease-in-out;
}
.thumbnail > img,
.thumbnail a > img {
  margin-left: auto;
  margin-right: auto;
}
a.thumbnail:hover,
a.thumbnail:focus,
a.thumbnail.active {
  border-color: #337ab7;
}
.thumbnail .caption {
  padding: 9px;
  color: #000;
}
.alert {
  padding: 15px;
  margin-bottom: 18px;
  border: 1px solid transparent;
  border-radius: 2px;
}
.alert h4 {
  margin-top: 0;
  color: inherit;
}
.alert .alert-link {
  font-weight: bold;
}
.alert > p,
.alert > ul {
  margin-bottom: 0;
}
.alert > p + p {
  margin-top: 5px;
}
.alert-dismissable,
.alert-dismissible {
  padding-right: 35px;
}
.alert-dismissable .close,
.alert-dismissible .close {
  position: relative;
  top: -2px;
  right: -21px;
  color: inherit;
}
.alert-success {
  background-color: #dff0d8;
  border-color: #d6e9c6;
  color: #3c763d;
}
.alert-success hr {
  border-top-color: #c9e2b3;
}
.alert-success .alert-link {
  color: #2b542c;
}
.alert-info {
  background-color: #d9edf7;
  border-color: #bce8f1;
  color: #31708f;
}
.alert-info hr {
  border-top-color: #a6e1ec;
}
.alert-info .alert-link {
  color: #245269;
}
.alert-warning {
  background-color: #fcf8e3;
  border-color: #faebcc;
  color: #8a6d3b;
}
.alert-warning hr {
  border-top-color: #f7e1b5;
}
.alert-warning .alert-link {
  color: #66512c;
}
.alert-danger {
  background-color: #f2dede;
  border-color: #ebccd1;
  color: #a94442;
}
.alert-danger hr {
  border-top-color: #e4b9c0;
}
.alert-danger .alert-link {
  color: #843534;
}
@-webkit-keyframes progress-bar-stripes {
  from {
    background-position: 40px 0;
  }
  to {
    background-position: 0 0;
  }
}
@keyframes progress-bar-stripes {
  from {
    background-position: 40px 0;
  }
  to {
    background-position: 0 0;
  }
}
.progress {
  overflow: hidden;
  height: 18px;
  margin-bottom: 18px;
  background-color: #f5f5f5;
  border-radius: 2px;
  -webkit-box-shadow: inset 0 1px 2px rgba(0, 0, 0, 0.1);
  box-shadow: inset 0 1px 2px rgba(0, 0, 0, 0.1);
}
.progress-bar {
  float: left;
  width: 0%;
  height: 100%;
  font-size: 12px;
  line-height: 18px;
  color: #fff;
  text-align: center;
  background-color: #337ab7;
  -webkit-box-shadow: inset 0 -1px 0 rgba(0, 0, 0, 0.15);
  box-shadow: inset 0 -1px 0 rgba(0, 0, 0, 0.15);
  -webkit-transition: width 0.6s ease;
  -o-transition: width 0.6s ease;
  transition: width 0.6s ease;
}
.progress-striped .progress-bar,
.progress-bar-striped {
  background-image: -webkit-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: -o-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-size: 40px 40px;
}
.progress.active .progress-bar,
.progress-bar.active {
  -webkit-animation: progress-bar-stripes 2s linear infinite;
  -o-animation: progress-bar-stripes 2s linear infinite;
  animation: progress-bar-stripes 2s linear infinite;
}
.progress-bar-success {
  background-color: #5cb85c;
}
.progress-striped .progress-bar-success {
  background-image: -webkit-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: -o-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
}
.progress-bar-info {
  background-color: #5bc0de;
}
.progress-striped .progress-bar-info {
  background-image: -webkit-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: -o-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
}
.progress-bar-warning {
  background-color: #f0ad4e;
}
.progress-striped .progress-bar-warning {
  background-image: -webkit-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: -o-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
}
.progress-bar-danger {
  background-color: #d9534f;
}
.progress-striped .progress-bar-danger {
  background-image: -webkit-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: -o-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
}
.media {
  margin-top: 15px;
}
.media:first-child {
  margin-top: 0;
}
.media,
.media-body {
  zoom: 1;
  overflow: hidden;
}
.media-body {
  width: 10000px;
}
.media-object {
  display: block;
}
.media-object.img-thumbnail {
  max-width: none;
}
.media-right,
.media > .pull-right {
  padding-left: 10px;
}
.media-left,
.media > .pull-left {
  padding-right: 10px;
}
.media-left,
.media-right,
.media-body {
  display: table-cell;
  vertical-align: top;
}
.media-middle {
  vertical-align: middle;
}
.media-bottom {
  vertical-align: bottom;
}
.media-heading {
  margin-top: 0;
  margin-bottom: 5px;
}
.media-list {
  padding-left: 0;
  list-style: none;
}
.list-group {
  margin-bottom: 20px;
  padding-left: 0;
}
.list-group-item {
  position: relative;
  display: block;
  padding: 10px 15px;
  margin-bottom: -1px;
  background-color: #fff;
  border: 1px solid #ddd;
}
.list-group-item:first-child {
  border-top-right-radius: 2px;
  border-top-left-radius: 2px;
}
.list-group-item:last-child {
  margin-bottom: 0;
  border-bottom-right-radius: 2px;
  border-bottom-left-radius: 2px;
}
a.list-group-item,
button.list-group-item {
  color: #555;
}
a.list-group-item .list-group-item-heading,
button.list-group-item .list-group-item-heading {
  color: #333;
}
a.list-group-item:hover,
button.list-group-item:hover,
a.list-group-item:focus,
button.list-group-item:focus {
  text-decoration: none;
  color: #555;
  background-color: #f5f5f5;
}
button.list-group-item {
  width: 100%;
  text-align: left;
}
.list-group-item.disabled,
.list-group-item.disabled:hover,
.list-group-item.disabled:focus {
  background-color: #eeeeee;
  color: #777777;
  cursor: not-allowed;
}
.list-group-item.disabled .list-group-item-heading,
.list-group-item.disabled:hover .list-group-item-heading,
.list-group-item.disabled:focus .list-group-item-heading {
  color: inherit;
}
.list-group-item.disabled .list-group-item-text,
.list-group-item.disabled:hover .list-group-item-text,
.list-group-item.disabled:focus .list-group-item-text {
  color: #777777;
}
.list-group-item.active,
.list-group-item.active:hover,
.list-group-item.active:focus {
  z-index: 2;
  color: #fff;
  background-color: #337ab7;
  border-color: #337ab7;
}
.list-group-item.active .list-group-item-heading,
.list-group-item.active:hover .list-group-item-heading,
.list-group-item.active:focus .list-group-item-heading,
.list-group-item.active .list-group-item-heading > small,
.list-group-item.active:hover .list-group-item-heading > small,
.list-group-item.active:focus .list-group-item-heading > small,
.list-group-item.active .list-group-item-heading > .small,
.list-group-item.active:hover .list-group-item-heading > .small,
.list-group-item.active:focus .list-group-item-heading > .small {
  color: inherit;
}
.list-group-item.active .list-group-item-text,
.list-group-item.active:hover .list-group-item-text,
.list-group-item.active:focus .list-group-item-text {
  color: #c7ddef;
}
.list-group-item-success {
  color: #3c763d;
  background-color: #dff0d8;
}
a.list-group-item-success,
button.list-group-item-success {
  color: #3c763d;
}
a.list-group-item-success .list-group-item-heading,
button.list-group-item-success .list-group-item-heading {
  color: inherit;
}
a.list-group-item-success:hover,
button.list-group-item-success:hover,
a.list-group-item-success:focus,
button.list-group-item-success:focus {
  color: #3c763d;
  background-color: #d0e9c6;
}
a.list-group-item-success.active,
button.list-group-item-success.active,
a.list-group-item-success.active:hover,
button.list-group-item-success.active:hover,
a.list-group-item-success.active:focus,
button.list-group-item-success.active:focus {
  color: #fff;
  background-color: #3c763d;
  border-color: #3c763d;
}
.list-group-item-info {
  color: #31708f;
  background-color: #d9edf7;
}
a.list-group-item-info,
button.list-group-item-info {
  color: #31708f;
}
a.list-group-item-info .list-group-item-heading,
button.list-group-item-info .list-group-item-heading {
  color: inherit;
}
a.list-group-item-info:hover,
button.list-group-item-info:hover,
a.list-group-item-info:focus,
button.list-group-item-info:focus {
  color: #31708f;
  background-color: #c4e3f3;
}
a.list-group-item-info.active,
button.list-group-item-info.active,
a.list-group-item-info.active:hover,
button.list-group-item-info.active:hover,
a.list-group-item-info.active:focus,
button.list-group-item-info.active:focus {
  color: #fff;
  background-color: #31708f;
  border-color: #31708f;
}
.list-group-item-warning {
  color: #8a6d3b;
  background-color: #fcf8e3;
}
a.list-group-item-warning,
button.list-group-item-warning {
  color: #8a6d3b;
}
a.list-group-item-warning .list-group-item-heading,
button.list-group-item-warning .list-group-item-heading {
  color: inherit;
}
a.list-group-item-warning:hover,
button.list-group-item-warning:hover,
a.list-group-item-warning:focus,
button.list-group-item-warning:focus {
  color: #8a6d3b;
  background-color: #faf2cc;
}
a.list-group-item-warning.active,
button.list-group-item-warning.active,
a.list-group-item-warning.active:hover,
button.list-group-item-warning.active:hover,
a.list-group-item-warning.active:focus,
button.list-group-item-warning.active:focus {
  color: #fff;
  background-color: #8a6d3b;
  border-color: #8a6d3b;
}
.list-group-item-danger {
  color: #a94442;
  background-color: #f2dede;
}
a.list-group-item-danger,
button.list-group-item-danger {
  color: #a94442;
}
a.list-group-item-danger .list-group-item-heading,
button.list-group-item-danger .list-group-item-heading {
  color: inherit;
}
a.list-group-item-danger:hover,
button.list-group-item-danger:hover,
a.list-group-item-danger:focus,
button.list-group-item-danger:focus {
  color: #a94442;
  background-color: #ebcccc;
}
a.list-group-item-danger.active,
button.list-group-item-danger.active,
a.list-group-item-danger.active:hover,
button.list-group-item-danger.active:hover,
a.list-group-item-danger.active:focus,
button.list-group-item-danger.active:focus {
  color: #fff;
  background-color: #a94442;
  border-color: #a94442;
}
.list-group-item-heading {
  margin-top: 0;
  margin-bottom: 5px;
}
.list-group-item-text {
  margin-bottom: 0;
  line-height: 1.3;
}
.panel {
  margin-bottom: 18px;
  background-color: #fff;
  border: 1px solid transparent;
  border-radius: 2px;
  -webkit-box-shadow: 0 1px 1px rgba(0, 0, 0, 0.05);
  box-shadow: 0 1px 1px rgba(0, 0, 0, 0.05);
}
.panel-body {
  padding: 15px;
}
.panel-heading {
  padding: 10px 15px;
  border-bottom: 1px solid transparent;
  border-top-right-radius: 1px;
  border-top-left-radius: 1px;
}
.panel-heading > .dropdown .dropdown-toggle {
  color: inherit;
}
.panel-title {
  margin-top: 0;
  margin-bottom: 0;
  font-size: 15px;
  color: inherit;
}
.panel-title > a,
.panel-title > small,
.panel-title > .small,
.panel-title > small > a,
.panel-title > .small > a {
  color: inherit;
}
.panel-footer {
  padding: 10px 15px;
  background-color: #f5f5f5;
  border-top: 1px solid #ddd;
  border-bottom-right-radius: 1px;
  border-bottom-left-radius: 1px;
}
.panel > .list-group,
.panel > .panel-collapse > .list-group {
  margin-bottom: 0;
}
.panel > .list-group .list-group-item,
.panel > .panel-collapse > .list-group .list-group-item {
  border-width: 1px 0;
  border-radius: 0;
}
.panel > .list-group:first-child .list-group-item:first-child,
.panel > .panel-collapse > .list-group:first-child .list-group-item:first-child {
  border-top: 0;
  border-top-right-radius: 1px;
  border-top-left-radius: 1px;
}
.panel > .list-group:last-child .list-group-item:last-child,
.panel > .panel-collapse > .list-group:last-child .list-group-item:last-child {
  border-bottom: 0;
  border-bottom-right-radius: 1px;
  border-bottom-left-radius: 1px;
}
.panel > .panel-heading + .panel-collapse > .list-group .list-group-item:first-child {
  border-top-right-radius: 0;
  border-top-left-radius: 0;
}
.panel-heading + .list-group .list-group-item:first-child {
  border-top-width: 0;
}
.list-group + .panel-footer {
  border-top-width: 0;
}
.panel > .table,
.panel > .table-responsive > .table,
.panel > .panel-collapse > .table {
  margin-bottom: 0;
}
.panel > .table caption,
.panel > .table-responsive > .table caption,
.panel > .panel-collapse > .table caption {
  padding-left: 15px;
  padding-right: 15px;
}
.panel > .table:first-child,
.panel > .table-responsive:first-child > .table:first-child {
  border-top-right-radius: 1px;
  border-top-left-radius: 1px;
}
.panel > .table:first-child > thead:first-child > tr:first-child,
.panel > .table-responsive:first-child > .table:first-child > thead:first-child > tr:first-child,
.panel > .table:first-child > tbody:first-child > tr:first-child,
.panel > .table-responsive:first-child > .table:first-child > tbody:first-child > tr:first-child {
  border-top-left-radius: 1px;
  border-top-right-radius: 1px;
}
.panel > .table:first-child > thead:first-child > tr:first-child td:first-child,
.panel > .table-responsive:first-child > .table:first-child > thead:first-child > tr:first-child td:first-child,
.panel > .table:first-child > tbody:first-child > tr:first-child td:first-child,
.panel > .table-responsive:first-child > .table:first-child > tbody:first-child > tr:first-child td:first-child,
.panel > .table:first-child > thead:first-child > tr:first-child th:first-child,
.panel > .table-responsive:first-child > .table:first-child > thead:first-child > tr:first-child th:first-child,
.panel > .table:first-child > tbody:first-child > tr:first-child th:first-child,
.panel > .table-responsive:first-child > .table:first-child > tbody:first-child > tr:first-child th:first-child {
  border-top-left-radius: 1px;
}
.panel > .table:first-child > thead:first-child > tr:first-child td:last-child,
.panel > .table-responsive:first-child > .table:first-child > thead:first-child > tr:first-child td:last-child,
.panel > .table:first-child > tbody:first-child > tr:first-child td:last-child,
.panel > .table-responsive:first-child > .table:first-child > tbody:first-child > tr:first-child td:last-child,
.panel > .table:first-child > thead:first-child > tr:first-child th:last-child,
.panel > .table-responsive:first-child > .table:first-child > thead:first-child > tr:first-child th:last-child,
.panel > .table:first-child > tbody:first-child > tr:first-child th:last-child,
.panel > .table-responsive:first-child > .table:first-child > tbody:first-child > tr:first-child th:last-child {
  border-top-right-radius: 1px;
}
.panel > .table:last-child,
.panel > .table-responsive:last-child > .table:last-child {
  border-bottom-right-radius: 1px;
  border-bottom-left-radius: 1px;
}
.panel > .table:last-child > tbody:last-child > tr:last-child,
.panel > .table-responsive:last-child > .table:last-child > tbody:last-child > tr:last-child,
.panel > .table:last-child > tfoot:last-child > tr:last-child,
.panel > .table-responsive:last-child > .table:last-child > tfoot:last-child > tr:last-child {
  border-bottom-left-radius: 1px;
  border-bottom-right-radius: 1px;
}
.panel > .table:last-child > tbody:last-child > tr:last-child td:first-child,
.panel > .table-responsive:last-child > .table:last-child > tbody:last-child > tr:last-child td:first-child,
.panel > .table:last-child > tfoot:last-child > tr:last-child td:first-child,
.panel > .table-responsive:last-child > .table:last-child > tfoot:last-child > tr:last-child td:first-child,
.panel > .table:last-child > tbody:last-child > tr:last-child th:first-child,
.panel > .table-responsive:last-child > .table:last-child > tbody:last-child > tr:last-child th:first-child,
.panel > .table:last-child > tfoot:last-child > tr:last-child th:first-child,
.panel > .table-responsive:last-child > .table:last-child > tfoot:last-child > tr:last-child th:first-child {
  border-bottom-left-radius: 1px;
}
.panel > .table:last-child > tbody:last-child > tr:last-child td:last-child,
.panel > .table-responsive:last-child > .table:last-child > tbody:last-child > tr:last-child td:last-child,
.panel > .table:last-child > tfoot:last-child > tr:last-child td:last-child,
.panel > .table-responsive:last-child > .table:last-child > tfoot:last-child > tr:last-child td:last-child,
.panel > .table:last-child > tbody:last-child > tr:last-child th:last-child,
.panel > .table-responsive:last-child > .table:last-child > tbody:last-child > tr:last-child th:last-child,
.panel > .table:last-child > tfoot:last-child > tr:last-child th:last-child,
.panel > .table-responsive:last-child > .table:last-child > tfoot:last-child > tr:last-child th:last-child {
  border-bottom-right-radius: 1px;
}
.panel > .panel-body + .table,
.panel > .panel-body + .table-responsive,
.panel > .table + .panel-body,
.panel > .table-responsive + .panel-body {
  border-top: 1px solid #ddd;
}
.panel > .table > tbody:first-child > tr:first-child th,
.panel > .table > tbody:first-child > tr:first-child td {
  border-top: 0;
}
.panel > .table-bordered,
.panel > .table-responsive > .table-bordered {
  border: 0;
}
.panel > .table-bordered > thead > tr > th:first-child,
.panel > .table-responsive > .table-bordered > thead > tr > th:first-child,
.panel > .table-bordered > tbody > tr > th:first-child,
.panel > .table-responsive > .table-bordered > tbody > tr > th:first-child,
.panel > .table-bordered > tfoot > tr > th:first-child,
.panel > .table-responsive > .table-bordered > tfoot > tr > th:first-child,
.panel > .table-bordered > thead > tr > td:first-child,
.panel > .table-responsive > .table-bordered > thead > tr > td:first-child,
.panel > .table-bordered > tbody > tr > td:first-child,
.panel > .table-responsive > .table-bordered > tbody > tr > td:first-child,
.panel > .table-bordered > tfoot > tr > td:first-child,
.panel > .table-responsive > .table-bordered > tfoot > tr > td:first-child {
  border-left: 0;
}
.panel > .table-bordered > thead > tr > th:last-child,
.panel > .table-responsive > .table-bordered > thead > tr > th:last-child,
.panel > .table-bordered > tbody > tr > th:last-child,
.panel > .table-responsive > .table-bordered > tbody > tr > th:last-child,
.panel > .table-bordered > tfoot > tr > th:last-child,
.panel > .table-responsive > .table-bordered > tfoot > tr > th:last-child,
.panel > .table-bordered > thead > tr > td:last-child,
.panel > .table-responsive > .table-bordered > thead > tr > td:last-child,
.panel > .table-bordered > tbody > tr > td:last-child,
.panel > .table-responsive > .table-bordered > tbody > tr > td:last-child,
.panel > .table-bordered > tfoot > tr > td:last-child,
.panel > .table-responsive > .table-bordered > tfoot > tr > td:last-child {
  border-right: 0;
}
.panel > .table-bordered > thead > tr:first-child > td,
.panel > .table-responsive > .table-bordered > thead > tr:first-child > td,
.panel > .table-bordered > tbody > tr:first-child > td,
.panel > .table-responsive > .table-bordered > tbody > tr:first-child > td,
.panel > .table-bordered > thead > tr:first-child > th,
.panel > .table-responsive > .table-bordered > thead > tr:first-child > th,
.panel > .table-bordered > tbody > tr:first-child > th,
.panel > .table-responsive > .table-bordered > tbody > tr:first-child > th {
  border-bottom: 0;
}
.panel > .table-bordered > tbody > tr:last-child > td,
.panel > .table-responsive > .table-bordered > tbody > tr:last-child > td,
.panel > .table-bordered > tfoot > tr:last-child > td,
.panel > .table-responsive > .table-bordered > tfoot > tr:last-child > td,
.panel > .table-bordered > tbody > tr:last-child > th,
.panel > .table-responsive > .table-bordered > tbody > tr:last-child > th,
.panel > .table-bordered > tfoot > tr:last-child > th,
.panel > .table-responsive > .table-bordered > tfoot > tr:last-child > th {
  border-bottom: 0;
}
.panel > .table-responsive {
  border: 0;
  margin-bottom: 0;
}
.panel-group {
  margin-bottom: 18px;
}
.panel-group .panel {
  margin-bottom: 0;
  border-radius: 2px;
}
.panel-group .panel + .panel {
  margin-top: 5px;
}
.panel-group .panel-heading {
  border-bottom: 0;
}
.panel-group .panel-heading + .panel-collapse > .panel-body,
.panel-group .panel-heading + .panel-collapse > .list-group {
  border-top: 1px solid #ddd;
}
.panel-group .panel-footer {
  border-top: 0;
}
.panel-group .panel-footer + .panel-collapse .panel-body {
  border-bottom: 1px solid #ddd;
}
.panel-default {
  border-color: #ddd;
}
.panel-default > .panel-heading {
  color: #333333;
  background-color: #f5f5f5;
  border-color: #ddd;
}
.panel-default > .panel-heading + .panel-collapse > .panel-body {
  border-top-color: #ddd;
}
.panel-default > .panel-heading .badge {
  color: #f5f5f5;
  background-color: #333333;
}
.panel-default > .panel-footer + .panel-collapse > .panel-body {
  border-bottom-color: #ddd;
}
.panel-primary {
  border-color: #337ab7;
}
.panel-primary > .panel-heading {
  color: #fff;
  background-color: #337ab7;
  border-color: #337ab7;
}
.panel-primary > .panel-heading + .panel-collapse > .panel-body {
  border-top-color: #337ab7;
}
.panel-primary > .panel-heading .badge {
  color: #337ab7;
  background-color: #fff;
}
.panel-primary > .panel-footer + .panel-collapse > .panel-body {
  border-bottom-color: #337ab7;
}
.panel-success {
  border-color: #d6e9c6;
}
.panel-success > .panel-heading {
  color: #3c763d;
  background-color: #dff0d8;
  border-color: #d6e9c6;
}
.panel-success > .panel-heading + .panel-collapse > .panel-body {
  border-top-color: #d6e9c6;
}
.panel-success > .panel-heading .badge {
  color: #dff0d8;
  background-color: #3c763d;
}
.panel-success > .panel-footer + .panel-collapse > .panel-body {
  border-bottom-color: #d6e9c6;
}
.panel-info {
  border-color: #bce8f1;
}
.panel-info > .panel-heading {
  color: #31708f;
  background-color: #d9edf7;
  border-color: #bce8f1;
}
.panel-info > .panel-heading + .panel-collapse > .panel-body {
  border-top-color: #bce8f1;
}
.panel-info > .panel-heading .badge {
  color: #d9edf7;
  background-color: #31708f;
}
.panel-info > .panel-footer + .panel-collapse > .panel-body {
  border-bottom-color: #bce8f1;
}
.panel-warning {
  border-color: #faebcc;
}
.panel-warning > .panel-heading {
  color: #8a6d3b;
  background-color: #fcf8e3;
  border-color: #faebcc;
}
.panel-warning > .panel-heading + .panel-collapse > .panel-body {
  border-top-color: #faebcc;
}
.panel-warning > .panel-heading .badge {
  color: #fcf8e3;
  background-color: #8a6d3b;
}
.panel-warning > .panel-footer + .panel-collapse > .panel-body {
  border-bottom-color: #faebcc;
}
.panel-danger {
  border-color: #ebccd1;
}
.panel-danger > .panel-heading {
  color: #a94442;
  background-color: #f2dede;
  border-color: #ebccd1;
}
.panel-danger > .panel-heading + .panel-collapse > .panel-body {
  border-top-color: #ebccd1;
}
.panel-danger > .panel-heading .badge {
  color: #f2dede;
  background-color: #a94442;
}
.panel-danger > .panel-footer + .panel-collapse > .panel-body {
  border-bottom-color: #ebccd1;
}
.embed-responsive {
  position: relative;
  display: block;
  height: 0;
  padding: 0;
  overflow: hidden;
}
.embed-responsive .embed-responsive-item,
.embed-responsive iframe,
.embed-responsive embed,
.embed-responsive object,
.embed-responsive video {
  position: absolute;
  top: 0;
  left: 0;
  bottom: 0;
  height: 100%;
  width: 100%;
  border: 0;
}
.embed-responsive-16by9 {
  padding-bottom: 56.25%;
}
.embed-responsive-4by3 {
  padding-bottom: 75%;
}
.well {
  min-height: 20px;
  padding: 19px;
  margin-bottom: 20px;
  background-color: #f5f5f5;
  border: 1px solid #e3e3e3;
  border-radius: 2px;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.05);
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.05);
}
.well blockquote {
  border-color: #ddd;
  border-color: rgba(0, 0, 0, 0.15);
}
.well-lg {
  padding: 24px;
  border-radius: 3px;
}
.well-sm {
  padding: 9px;
  border-radius: 1px;
}
.close {
  float: right;
  font-size: 19.5px;
  font-weight: bold;
  line-height: 1;
  color: #000;
  text-shadow: 0 1px 0 #fff;
  opacity: 0.2;
  filter: alpha(opacity=20);
}
.close:hover,
.close:focus {
  color: #000;
  text-decoration: none;
  cursor: pointer;
  opacity: 0.5;
  filter: alpha(opacity=50);
}
button.close {
  padding: 0;
  cursor: pointer;
  background: transparent;
  border: 0;
  -webkit-appearance: none;
}
.modal-open {
  overflow: hidden;
}
.modal {
  display: none;
  overflow: hidden;
  position: fixed;
  top: 0;
  right: 0;
  bottom: 0;
  left: 0;
  z-index: 1050;
  -webkit-overflow-scrolling: touch;
  outline: 0;
}
.modal.fade .modal-dialog {
  -webkit-transform: translate(0, -25%);
  -ms-transform: translate(0, -25%);
  -o-transform: translate(0, -25%);
  transform: translate(0, -25%);
  -webkit-transition: -webkit-transform 0.3s ease-out;
  -moz-transition: -moz-transform 0.3s ease-out;
  -o-transition: -o-transform 0.3s ease-out;
  transition: transform 0.3s ease-out;
}
.modal.in .modal-dialog {
  -webkit-transform: translate(0, 0);
  -ms-transform: translate(0, 0);
  -o-transform: translate(0, 0);
  transform: translate(0, 0);
}
.modal-open .modal {
  overflow-x: hidden;
  overflow-y: auto;
}
.modal-dialog {
  position: relative;
  width: auto;
  margin: 10px;
}
.modal-content {
  position: relative;
  background-color: #fff;
  border: 1px solid #999;
  border: 1px solid rgba(0, 0, 0, 0.2);
  border-radius: 3px;
  -webkit-box-shadow: 0 3px 9px rgba(0, 0, 0, 0.5);
  box-shadow: 0 3px 9px rgba(0, 0, 0, 0.5);
  background-clip: padding-box;
  outline: 0;
}
.modal-backdrop {
  position: fixed;
  top: 0;
  right: 0;
  bottom: 0;
  left: 0;
  z-index: 1040;
  background-color: #000;
}
.modal-backdrop.fade {
  opacity: 0;
  filter: alpha(opacity=0);
}
.modal-backdrop.in {
  opacity: 0.5;
  filter: alpha(opacity=50);
}
.modal-header {
  padding: 15px;
  border-bottom: 1px solid #e5e5e5;
}
.modal-header .close {
  margin-top: -2px;
}
.modal-title {
  margin: 0;
  line-height: 1.42857143;
}
.modal-body {
  position: relative;
  padding: 15px;
}
.modal-footer {
  padding: 15px;
  text-align: right;
  border-top: 1px solid #e5e5e5;
}
.modal-footer .btn + .btn {
  margin-left: 5px;
  margin-bottom: 0;
}
.modal-footer .btn-group .btn + .btn {
  margin-left: -1px;
}
.modal-footer .btn-block + .btn-block {
  margin-left: 0;
}
.modal-scrollbar-measure {
  position: absolute;
  top: -9999px;
  width: 50px;
  height: 50px;
  overflow: scroll;
}
@media (min-width: 768px) {
  .modal-dialog {
    width: 600px;
    margin: 30px auto;
  }
  .modal-content {
    -webkit-box-shadow: 0 5px 15px rgba(0, 0, 0, 0.5);
    box-shadow: 0 5px 15px rgba(0, 0, 0, 0.5);
  }
  .modal-sm {
    width: 300px;
  }
}
@media (min-width: 992px) {
  .modal-lg {
    width: 900px;
  }
}
.tooltip {
  position: absolute;
  z-index: 1070;
  display: block;
  font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;
  font-style: normal;
  font-weight: normal;
  letter-spacing: normal;
  line-break: auto;
  line-height: 1.42857143;
  text-align: left;
  text-align: start;
  text-decoration: none;
  text-shadow: none;
  text-transform: none;
  white-space: normal;
  word-break: normal;
  word-spacing: normal;
  word-wrap: normal;
  font-size: 12px;
  opacity: 0;
  filter: alpha(opacity=0);
}
.tooltip.in {
  opacity: 0.9;
  filter: alpha(opacity=90);
}
.tooltip.top {
  margin-top: -3px;
  padding: 5px 0;
}
.tooltip.right {
  margin-left: 3px;
  padding: 0 5px;
}
.tooltip.bottom {
  margin-top: 3px;
  padding: 5px 0;
}
.tooltip.left {
  margin-left: -3px;
  padding: 0 5px;
}
.tooltip-inner {
  max-width: 200px;
  padding: 3px 8px;
  color: #fff;
  text-align: center;
  background-color: #000;
  border-radius: 2px;
}
.tooltip-arrow {
  position: absolute;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
}
.tooltip.top .tooltip-arrow {
  bottom: 0;
  left: 50%;
  margin-left: -5px;
  border-width: 5px 5px 0;
  border-top-color: #000;
}
.tooltip.top-left .tooltip-arrow {
  bottom: 0;
  right: 5px;
  margin-bottom: -5px;
  border-width: 5px 5px 0;
  border-top-color: #000;
}
.tooltip.top-right .tooltip-arrow {
  bottom: 0;
  left: 5px;
  margin-bottom: -5px;
  border-width: 5px 5px 0;
  border-top-color: #000;
}
.tooltip.right .tooltip-arrow {
  top: 50%;
  left: 0;
  margin-top: -5px;
  border-width: 5px 5px 5px 0;
  border-right-color: #000;
}
.tooltip.left .tooltip-arrow {
  top: 50%;
  right: 0;
  margin-top: -5px;
  border-width: 5px 0 5px 5px;
  border-left-color: #000;
}
.tooltip.bottom .tooltip-arrow {
  top: 0;
  left: 50%;
  margin-left: -5px;
  border-width: 0 5px 5px;
  border-bottom-color: #000;
}
.tooltip.bottom-left .tooltip-arrow {
  top: 0;
  right: 5px;
  margin-top: -5px;
  border-width: 0 5px 5px;
  border-bottom-color: #000;
}
.tooltip.bottom-right .tooltip-arrow {
  top: 0;
  left: 5px;
  margin-top: -5px;
  border-width: 0 5px 5px;
  border-bottom-color: #000;
}
.popover {
  position: absolute;
  top: 0;
  left: 0;
  z-index: 1060;
  display: none;
  max-width: 276px;
  padding: 1px;
  font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;
  font-style: normal;
  font-weight: normal;
  letter-spacing: normal;
  line-break: auto;
  line-height: 1.42857143;
  text-align: left;
  text-align: start;
  text-decoration: none;
  text-shadow: none;
  text-transform: none;
  white-space: normal;
  word-break: normal;
  word-spacing: normal;
  word-wrap: normal;
  font-size: 13px;
  background-color: #fff;
  background-clip: padding-box;
  border: 1px solid #ccc;
  border: 1px solid rgba(0, 0, 0, 0.2);
  border-radius: 3px;
  -webkit-box-shadow: 0 5px 10px rgba(0, 0, 0, 0.2);
  box-shadow: 0 5px 10px rgba(0, 0, 0, 0.2);
}
.popover.top {
  margin-top: -10px;
}
.popover.right {
  margin-left: 10px;
}
.popover.bottom {
  margin-top: 10px;
}
.popover.left {
  margin-left: -10px;
}
.popover-title {
  margin: 0;
  padding: 8px 14px;
  font-size: 13px;
  background-color: #f7f7f7;
  border-bottom: 1px solid #ebebeb;
  border-radius: 2px 2px 0 0;
}
.popover-content {
  padding: 9px 14px;
}
.popover > .arrow,
.popover > .arrow:after {
  position: absolute;
  display: block;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
}
.popover > .arrow {
  border-width: 11px;
}
.popover > .arrow:after {
  border-width: 10px;
  content: "";
}
.popover.top > .arrow {
  left: 50%;
  margin-left: -11px;
  border-bottom-width: 0;
  border-top-color: #999999;
  border-top-color: rgba(0, 0, 0, 0.25);
  bottom: -11px;
}
.popover.top > .arrow:after {
  content: " ";
  bottom: 1px;
  margin-left: -10px;
  border-bottom-width: 0;
  border-top-color: #fff;
}
.popover.right > .arrow {
  top: 50%;
  left: -11px;
  margin-top: -11px;
  border-left-width: 0;
  border-right-color: #999999;
  border-right-color: rgba(0, 0, 0, 0.25);
}
.popover.right > .arrow:after {
  content: " ";
  left: 1px;
  bottom: -10px;
  border-left-width: 0;
  border-right-color: #fff;
}
.popover.bottom > .arrow {
  left: 50%;
  margin-left: -11px;
  border-top-width: 0;
  border-bottom-color: #999999;
  border-bottom-color: rgba(0, 0, 0, 0.25);
  top: -11px;
}
.popover.bottom > .arrow:after {
  content: " ";
  top: 1px;
  margin-left: -10px;
  border-top-width: 0;
  border-bottom-color: #fff;
}
.popover.left > .arrow {
  top: 50%;
  right: -11px;
  margin-top: -11px;
  border-right-width: 0;
  border-left-color: #999999;
  border-left-color: rgba(0, 0, 0, 0.25);
}
.popover.left > .arrow:after {
  content: " ";
  right: 1px;
  border-right-width: 0;
  border-left-color: #fff;
  bottom: -10px;
}
.carousel {
  position: relative;
}
.carousel-inner {
  position: relative;
  overflow: hidden;
  width: 100%;
}
.carousel-inner > .item {
  display: none;
  position: relative;
  -webkit-transition: 0.6s ease-in-out left;
  -o-transition: 0.6s ease-in-out left;
  transition: 0.6s ease-in-out left;
}
.carousel-inner > .item > img,
.carousel-inner > .item > a > img {
  line-height: 1;
}
@media all and (transform-3d), (-webkit-transform-3d) {
  .carousel-inner > .item {
    -webkit-transition: -webkit-transform 0.6s ease-in-out;
    -moz-transition: -moz-transform 0.6s ease-in-out;
    -o-transition: -o-transform 0.6s ease-in-out;
    transition: transform 0.6s ease-in-out;
    -webkit-backface-visibility: hidden;
    -moz-backface-visibility: hidden;
    backface-visibility: hidden;
    -webkit-perspective: 1000px;
    -moz-perspective: 1000px;
    perspective: 1000px;
  }
  .carousel-inner > .item.next,
  .carousel-inner > .item.active.right {
    -webkit-transform: translate3d(100%, 0, 0);
    transform: translate3d(100%, 0, 0);
    left: 0;
  }
  .carousel-inner > .item.prev,
  .carousel-inner > .item.active.left {
    -webkit-transform: translate3d(-100%, 0, 0);
    transform: translate3d(-100%, 0, 0);
    left: 0;
  }
  .carousel-inner > .item.next.left,
  .carousel-inner > .item.prev.right,
  .carousel-inner > .item.active {
    -webkit-transform: translate3d(0, 0, 0);
    transform: translate3d(0, 0, 0);
    left: 0;
  }
}
.carousel-inner > .active,
.carousel-inner > .next,
.carousel-inner > .prev {
  display: block;
}
.carousel-inner > .active {
  left: 0;
}
.carousel-inner > .next,
.carousel-inner > .prev {
  position: absolute;
  top: 0;
  width: 100%;
}
.carousel-inner > .next {
  left: 100%;
}
.carousel-inner > .prev {
  left: -100%;
}
.carousel-inner > .next.left,
.carousel-inner > .prev.right {
  left: 0;
}
.carousel-inner > .active.left {
  left: -100%;
}
.carousel-inner > .active.right {
  left: 100%;
}
.carousel-control {
  position: absolute;
  top: 0;
  left: 0;
  bottom: 0;
  width: 15%;
  opacity: 0.5;
  filter: alpha(opacity=50);
  font-size: 20px;
  color: #fff;
  text-align: center;
  text-shadow: 0 1px 2px rgba(0, 0, 0, 0.6);
  background-color: rgba(0, 0, 0, 0);
}
.carousel-control.left {
  background-image: -webkit-linear-gradient(left, rgba(0, 0, 0, 0.5) 0%, rgba(0, 0, 0, 0.0001) 100%);
  background-image: -o-linear-gradient(left, rgba(0, 0, 0, 0.5) 0%, rgba(0, 0, 0, 0.0001) 100%);
  background-image: linear-gradient(to right, rgba(0, 0, 0, 0.5) 0%, rgba(0, 0, 0, 0.0001) 100%);
  background-repeat: repeat-x;
  filter: progid:DXImageTransform.Microsoft.gradient(startColorstr='#80000000', endColorstr='#00000000', GradientType=1);
}
.carousel-control.right {
  left: auto;
  right: 0;
  background-image: -webkit-linear-gradient(left, rgba(0, 0, 0, 0.0001) 0%, rgba(0, 0, 0, 0.5) 100%);
  background-image: -o-linear-gradient(left, rgba(0, 0, 0, 0.0001) 0%, rgba(0, 0, 0, 0.5) 100%);
  background-image: linear-gradient(to right, rgba(0, 0, 0, 0.0001) 0%, rgba(0, 0, 0, 0.5) 100%);
  background-repeat: repeat-x;
  filter: progid:DXImageTransform.Microsoft.gradient(startColorstr='#00000000', endColorstr='#80000000', GradientType=1);
}
.carousel-control:hover,
.carousel-control:focus {
  outline: 0;
  color: #fff;
  text-decoration: none;
  opacity: 0.9;
  filter: alpha(opacity=90);
}
.carousel-control .icon-prev,
.carousel-control .icon-next,
.carousel-control .glyphicon-chevron-left,
.carousel-control .glyphicon-chevron-right {
  position: absolute;
  top: 50%;
  margin-top: -10px;
  z-index: 5;
  display: inline-block;
}
.carousel-control .icon-prev,
.carousel-control .glyphicon-chevron-left {
  left: 50%;
  margin-left: -10px;
}
.carousel-control .icon-next,
.carousel-control .glyphicon-chevron-right {
  right: 50%;
  margin-right: -10px;
}
.carousel-control .icon-prev,
.carousel-control .icon-next {
  width: 20px;
  height: 20px;
  line-height: 1;
  font-family: serif;
}
.carousel-control .icon-prev:before {
  content: '\2039';
}
.carousel-control .icon-next:before {
  content: '\203a';
}
.carousel-indicators {
  position: absolute;
  bottom: 10px;
  left: 50%;
  z-index: 15;
  width: 60%;
  margin-left: -30%;
  padding-left: 0;
  list-style: none;
  text-align: center;
}
.carousel-indicators li {
  display: inline-block;
  width: 10px;
  height: 10px;
  margin: 1px;
  text-indent: -999px;
  border: 1px solid #fff;
  border-radius: 10px;
  cursor: pointer;
  background-color: #000 \9;
  background-color: rgba(0, 0, 0, 0);
}
.carousel-indicators .active {
  margin: 0;
  width: 12px;
  height: 12px;
  background-color: #fff;
}
.carousel-caption {
  position: absolute;
  left: 15%;
  right: 15%;
  bottom: 20px;
  z-index: 10;
  padding-top: 20px;
  padding-bottom: 20px;
  color: #fff;
  text-align: center;
  text-shadow: 0 1px 2px rgba(0, 0, 0, 0.6);
}
.carousel-caption .btn {
  text-shadow: none;
}
@media screen and (min-width: 768px) {
  .carousel-control .glyphicon-chevron-left,
  .carousel-control .glyphicon-chevron-right,
  .carousel-control .icon-prev,
  .carousel-control .icon-next {
    width: 30px;
    height: 30px;
    margin-top: -10px;
    font-size: 30px;
  }
  .carousel-control .glyphicon-chevron-left,
  .carousel-control .icon-prev {
    margin-left: -10px;
  }
  .carousel-control .glyphicon-chevron-right,
  .carousel-control .icon-next {
    margin-right: -10px;
  }
  .carousel-caption {
    left: 20%;
    right: 20%;
    padding-bottom: 30px;
  }
  .carousel-indicators {
    bottom: 20px;
  }
}
.clearfix:before,
.clearfix:after,
.dl-horizontal dd:before,
.dl-horizontal dd:after,
.container:before,
.container:after,
.container-fluid:before,
.container-fluid:after,
.row:before,
.row:after,
.form-horizontal .form-group:before,
.form-horizontal .form-group:after,
.btn-toolbar:before,
.btn-toolbar:after,
.btn-group-vertical > .btn-group:before,
.btn-group-vertical > .btn-group:after,
.nav:before,
.nav:after,
.navbar:before,
.navbar:after,
.navbar-header:before,
.navbar-header:after,
.navbar-collapse:before,
.navbar-collapse:after,
.pager:before,
.pager:after,
.panel-body:before,
.panel-body:after,
.modal-header:before,
.modal-header:after,
.modal-footer:before,
.modal-footer:after,
.item_buttons:before,
.item_buttons:after {
  content: " ";
  display: table;
}
.clearfix:after,
.dl-horizontal dd:after,
.container:after,
.container-fluid:after,
.row:after,
.form-horizontal .form-group:after,
.btn-toolbar:after,
.btn-group-vertical > .btn-group:after,
.nav:after,
.navbar:after,
.navbar-header:after,
.navbar-collapse:after,
.pager:after,
.panel-body:after,
.modal-header:after,
.modal-footer:after,
.item_buttons:after {
  clear: both;
}
.center-block {
  display: block;
  margin-left: auto;
  margin-right: auto;
}
.pull-right {
  float: right !important;
}
.pull-left {
  float: left !important;
}
.hide {
  display: none !important;
}
.show {
  display: block !important;
}
.invisible {
  visibility: hidden;
}
.text-hide {
  font: 0/0 a;
  color: transparent;
  text-shadow: none;
  background-color: transparent;
  border: 0;
}
.hidden {
  display: none !important;
}
.affix {
  position: fixed;
}
@-ms-viewport {
  width: device-width;
}
.visible-xs,
.visible-sm,
.visible-md,
.visible-lg {
  display: none !important;
}
.visible-xs-block,
.visible-xs-inline,
.visible-xs-inline-block,
.visible-sm-block,
.visible-sm-inline,
.visible-sm-inline-block,
.visible-md-block,
.visible-md-inline,
.visible-md-inline-block,
.visible-lg-block,
.visible-lg-inline,
.visible-lg-inline-block {
  display: none !important;
}
@media (max-width: 767px) {
  .visible-xs {
    display: block !important;
  }
  table.visible-xs {
    display: table !important;
  }
  tr.visible-xs {
    display: table-row !important;
  }
  th.visible-xs,
  td.visible-xs {
    display: table-cell !important;
  }
}
@media (max-width: 767px) {
  .visible-xs-block {
    display: block !important;
  }
}
@media (max-width: 767px) {
  .visible-xs-inline {
    display: inline !important;
  }
}
@media (max-width: 767px) {
  .visible-xs-inline-block {
    display: inline-block !important;
  }
}
@media (min-width: 768px) and (max-width: 991px) {
  .visible-sm {
    display: block !important;
  }
  table.visible-sm {
    display: table !important;
  }
  tr.visible-sm {
    display: table-row !important;
  }
  th.visible-sm,
  td.visible-sm {
    display: table-cell !important;
  }
}
@media (min-width: 768px) and (max-width: 991px) {
  .visible-sm-block {
    display: block !important;
  }
}
@media (min-width: 768px) and (max-width: 991px) {
  .visible-sm-inline {
    display: inline !important;
  }
}
@media (min-width: 768px) and (max-width: 991px) {
  .visible-sm-inline-block {
    display: inline-block !important;
  }
}
@media (min-width: 992px) and (max-width: 1199px) {
  .visible-md {
    display: block !important;
  }
  table.visible-md {
    display: table !important;
  }
  tr.visible-md {
    display: table-row !important;
  }
  th.visible-md,
  td.visible-md {
    display: table-cell !important;
  }
}
@media (min-width: 992px) and (max-width: 1199px) {
  .visible-md-block {
    display: block !important;
  }
}
@media (min-width: 992px) and (max-width: 1199px) {
  .visible-md-inline {
    display: inline !important;
  }
}
@media (min-width: 992px) and (max-width: 1199px) {
  .visible-md-inline-block {
    display: inline-block !important;
  }
}
@media (min-width: 1200px) {
  .visible-lg {
    display: block !important;
  }
  table.visible-lg {
    display: table !important;
  }
  tr.visible-lg {
    display: table-row !important;
  }
  th.visible-lg,
  td.visible-lg {
    display: table-cell !important;
  }
}
@media (min-width: 1200px) {
  .visible-lg-block {
    display: block !important;
  }
}
@media (min-width: 1200px) {
  .visible-lg-inline {
    display: inline !important;
  }
}
@media (min-width: 1200px) {
  .visible-lg-inline-block {
    display: inline-block !important;
  }
}
@media (max-width: 767px) {
  .hidden-xs {
    display: none !important;
  }
}
@media (min-width: 768px) and (max-width: 991px) {
  .hidden-sm {
    display: none !important;
  }
}
@media (min-width: 992px) and (max-width: 1199px) {
  .hidden-md {
    display: none !important;
  }
}
@media (min-width: 1200px) {
  .hidden-lg {
    display: none !important;
  }
}
.visible-print {
  display: none !important;
}
@media print {
  .visible-print {
    display: block !important;
  }
  table.visible-print {
    display: table !important;
  }
  tr.visible-print {
    display: table-row !important;
  }
  th.visible-print,
  td.visible-print {
    display: table-cell !important;
  }
}
.visible-print-block {
  display: none !important;
}
@media print {
  .visible-print-block {
    display: block !important;
  }
}
.visible-print-inline {
  display: none !important;
}
@media print {
  .visible-print-inline {
    display: inline !important;
  }
}
.visible-print-inline-block {
  display: none !important;
}
@media print {
  .visible-print-inline-block {
    display: inline-block !important;
  }
}
@media print {
  .hidden-print {
    display: none !important;
  }
}
/*!
*
* Font Awesome
*
*/
/*!
 *  Font Awesome 4.7.0 by @davegandy - http://fontawesome.io - @fontawesome
 *  License - http://fontawesome.io/license (Font: SIL OFL 1.1, CSS: MIT License)
 */
/* FONT PATH
 * -------------------------- */
@font-face {
  font-family: 'FontAwesome';
  src: url('../components/font-awesome/fonts/fontawesome-webfont.eot?v=4.7.0');
  src: url('../components/font-awesome/fonts/fontawesome-webfont.eot?#iefix&v=4.7.0') format('embedded-opentype'), url('../components/font-awesome/fonts/fontawesome-webfont.woff2?v=4.7.0') format('woff2'), url('../components/font-awesome/fonts/fontawesome-webfont.woff?v=4.7.0') format('woff'), url('../components/font-awesome/fonts/fontawesome-webfont.ttf?v=4.7.0') format('truetype'), url('../components/font-awesome/fonts/fontawesome-webfont.svg?v=4.7.0#fontawesomeregular') format('svg');
  font-weight: normal;
  font-style: normal;
}
.fa {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
}
/* makes the font 33% larger relative to the icon container */
.fa-lg {
  font-size: 1.33333333em;
  line-height: 0.75em;
  vertical-align: -15%;
}
.fa-2x {
  font-size: 2em;
}
.fa-3x {
  font-size: 3em;
}
.fa-4x {
  font-size: 4em;
}
.fa-5x {
  font-size: 5em;
}
.fa-fw {
  width: 1.28571429em;
  text-align: center;
}
.fa-ul {
  padding-left: 0;
  margin-left: 2.14285714em;
  list-style-type: none;
}
.fa-ul > li {
  position: relative;
}
.fa-li {
  position: absolute;
  left: -2.14285714em;
  width: 2.14285714em;
  top: 0.14285714em;
  text-align: center;
}
.fa-li.fa-lg {
  left: -1.85714286em;
}
.fa-border {
  padding: .2em .25em .15em;
  border: solid 0.08em #eee;
  border-radius: .1em;
}
.fa-pull-left {
  float: left;
}
.fa-pull-right {
  float: right;
}
.fa.fa-pull-left {
  margin-right: .3em;
}
.fa.fa-pull-right {
  margin-left: .3em;
}
/* Deprecated as of 4.4.0 */
.pull-right {
  float: right;
}
.pull-left {
  float: left;
}
.fa.pull-left {
  margin-right: .3em;
}
.fa.pull-right {
  margin-left: .3em;
}
.fa-spin {
  -webkit-animation: fa-spin 2s infinite linear;
  animation: fa-spin 2s infinite linear;
}
.fa-pulse {
  -webkit-animation: fa-spin 1s infinite steps(8);
  animation: fa-spin 1s infinite steps(8);
}
@-webkit-keyframes fa-spin {
  0% {
    -webkit-transform: rotate(0deg);
    transform: rotate(0deg);
  }
  100% {
    -webkit-transform: rotate(359deg);
    transform: rotate(359deg);
  }
}
@keyframes fa-spin {
  0% {
    -webkit-transform: rotate(0deg);
    transform: rotate(0deg);
  }
  100% {
    -webkit-transform: rotate(359deg);
    transform: rotate(359deg);
  }
}
.fa-rotate-90 {
  -ms-filter: "progid:DXImageTransform.Microsoft.BasicImage(rotation=1)";
  -webkit-transform: rotate(90deg);
  -ms-transform: rotate(90deg);
  transform: rotate(90deg);
}
.fa-rotate-180 {
  -ms-filter: "progid:DXImageTransform.Microsoft.BasicImage(rotation=2)";
  -webkit-transform: rotate(180deg);
  -ms-transform: rotate(180deg);
  transform: rotate(180deg);
}
.fa-rotate-270 {
  -ms-filter: "progid:DXImageTransform.Microsoft.BasicImage(rotation=3)";
  -webkit-transform: rotate(270deg);
  -ms-transform: rotate(270deg);
  transform: rotate(270deg);
}
.fa-flip-horizontal {
  -ms-filter: "progid:DXImageTransform.Microsoft.BasicImage(rotation=0, mirror=1)";
  -webkit-transform: scale(-1, 1);
  -ms-transform: scale(-1, 1);
  transform: scale(-1, 1);
}
.fa-flip-vertical {
  -ms-filter: "progid:DXImageTransform.Microsoft.BasicImage(rotation=2, mirror=1)";
  -webkit-transform: scale(1, -1);
  -ms-transform: scale(1, -1);
  transform: scale(1, -1);
}
:root .fa-rotate-90,
:root .fa-rotate-180,
:root .fa-rotate-270,
:root .fa-flip-horizontal,
:root .fa-flip-vertical {
  filter: none;
}
.fa-stack {
  position: relative;
  display: inline-block;
  width: 2em;
  height: 2em;
  line-height: 2em;
  vertical-align: middle;
}
.fa-stack-1x,
.fa-stack-2x {
  position: absolute;
  left: 0;
  width: 100%;
  text-align: center;
}
.fa-stack-1x {
  line-height: inherit;
}
.fa-stack-2x {
  font-size: 2em;
}
.fa-inverse {
  color: #fff;
}
/* Font Awesome uses the Unicode Private Use Area (PUA) to ensure screen
   readers do not read off random characters that represent icons */
.fa-glass:before {
  content: "\f000";
}
.fa-music:before {
  content: "\f001";
}
.fa-search:before {
  content: "\f002";
}
.fa-envelope-o:before {
  content: "\f003";
}
.fa-heart:before {
  content: "\f004";
}
.fa-star:before {
  content: "\f005";
}
.fa-star-o:before {
  content: "\f006";
}
.fa-user:before {
  content: "\f007";
}
.fa-film:before {
  content: "\f008";
}
.fa-th-large:before {
  content: "\f009";
}
.fa-th:before {
  content: "\f00a";
}
.fa-th-list:before {
  content: "\f00b";
}
.fa-check:before {
  content: "\f00c";
}
.fa-remove:before,
.fa-close:before,
.fa-times:before {
  content: "\f00d";
}
.fa-search-plus:before {
  content: "\f00e";
}
.fa-search-minus:before {
  content: "\f010";
}
.fa-power-off:before {
  content: "\f011";
}
.fa-signal:before {
  content: "\f012";
}
.fa-gear:before,
.fa-cog:before {
  content: "\f013";
}
.fa-trash-o:before {
  content: "\f014";
}
.fa-home:before {
  content: "\f015";
}
.fa-file-o:before {
  content: "\f016";
}
.fa-clock-o:before {
  content: "\f017";
}
.fa-road:before {
  content: "\f018";
}
.fa-download:before {
  content: "\f019";
}
.fa-arrow-circle-o-down:before {
  content: "\f01a";
}
.fa-arrow-circle-o-up:before {
  content: "\f01b";
}
.fa-inbox:before {
  content: "\f01c";
}
.fa-play-circle-o:before {
  content: "\f01d";
}
.fa-rotate-right:before,
.fa-repeat:before {
  content: "\f01e";
}
.fa-refresh:before {
  content: "\f021";
}
.fa-list-alt:before {
  content: "\f022";
}
.fa-lock:before {
  content: "\f023";
}
.fa-flag:before {
  content: "\f024";
}
.fa-headphones:before {
  content: "\f025";
}
.fa-volume-off:before {
  content: "\f026";
}
.fa-volume-down:before {
  content: "\f027";
}
.fa-volume-up:before {
  content: "\f028";
}
.fa-qrcode:before {
  content: "\f029";
}
.fa-barcode:before {
  content: "\f02a";
}
.fa-tag:before {
  content: "\f02b";
}
.fa-tags:before {
  content: "\f02c";
}
.fa-book:before {
  content: "\f02d";
}
.fa-bookmark:before {
  content: "\f02e";
}
.fa-print:before {
  content: "\f02f";
}
.fa-camera:before {
  content: "\f030";
}
.fa-font:before {
  content: "\f031";
}
.fa-bold:before {
  content: "\f032";
}
.fa-italic:before {
  content: "\f033";
}
.fa-text-height:before {
  content: "\f034";
}
.fa-text-width:before {
  content: "\f035";
}
.fa-align-left:before {
  content: "\f036";
}
.fa-align-center:before {
  content: "\f037";
}
.fa-align-right:before {
  content: "\f038";
}
.fa-align-justify:before {
  content: "\f039";
}
.fa-list:before {
  content: "\f03a";
}
.fa-dedent:before,
.fa-outdent:before {
  content: "\f03b";
}
.fa-indent:before {
  content: "\f03c";
}
.fa-video-camera:before {
  content: "\f03d";
}
.fa-photo:before,
.fa-image:before,
.fa-picture-o:before {
  content: "\f03e";
}
.fa-pencil:before {
  content: "\f040";
}
.fa-map-marker:before {
  content: "\f041";
}
.fa-adjust:before {
  content: "\f042";
}
.fa-tint:before {
  content: "\f043";
}
.fa-edit:before,
.fa-pencil-square-o:before {
  content: "\f044";
}
.fa-share-square-o:before {
  content: "\f045";
}
.fa-check-square-o:before {
  content: "\f046";
}
.fa-arrows:before {
  content: "\f047";
}
.fa-step-backward:before {
  content: "\f048";
}
.fa-fast-backward:before {
  content: "\f049";
}
.fa-backward:before {
  content: "\f04a";
}
.fa-play:before {
  content: "\f04b";
}
.fa-pause:before {
  content: "\f04c";
}
.fa-stop:before {
  content: "\f04d";
}
.fa-forward:before {
  content: "\f04e";
}
.fa-fast-forward:before {
  content: "\f050";
}
.fa-step-forward:before {
  content: "\f051";
}
.fa-eject:before {
  content: "\f052";
}
.fa-chevron-left:before {
  content: "\f053";
}
.fa-chevron-right:before {
  content: "\f054";
}
.fa-plus-circle:before {
  content: "\f055";
}
.fa-minus-circle:before {
  content: "\f056";
}
.fa-times-circle:before {
  content: "\f057";
}
.fa-check-circle:before {
  content: "\f058";
}
.fa-question-circle:before {
  content: "\f059";
}
.fa-info-circle:before {
  content: "\f05a";
}
.fa-crosshairs:before {
  content: "\f05b";
}
.fa-times-circle-o:before {
  content: "\f05c";
}
.fa-check-circle-o:before {
  content: "\f05d";
}
.fa-ban:before {
  content: "\f05e";
}
.fa-arrow-left:before {
  content: "\f060";
}
.fa-arrow-right:before {
  content: "\f061";
}
.fa-arrow-up:before {
  content: "\f062";
}
.fa-arrow-down:before {
  content: "\f063";
}
.fa-mail-forward:before,
.fa-share:before {
  content: "\f064";
}
.fa-expand:before {
  content: "\f065";
}
.fa-compress:before {
  content: "\f066";
}
.fa-plus:before {
  content: "\f067";
}
.fa-minus:before {
  content: "\f068";
}
.fa-asterisk:before {
  content: "\f069";
}
.fa-exclamation-circle:before {
  content: "\f06a";
}
.fa-gift:before {
  content: "\f06b";
}
.fa-leaf:before {
  content: "\f06c";
}
.fa-fire:before {
  content: "\f06d";
}
.fa-eye:before {
  content: "\f06e";
}
.fa-eye-slash:before {
  content: "\f070";
}
.fa-warning:before,
.fa-exclamation-triangle:before {
  content: "\f071";
}
.fa-plane:before {
  content: "\f072";
}
.fa-calendar:before {
  content: "\f073";
}
.fa-random:before {
  content: "\f074";
}
.fa-comment:before {
  content: "\f075";
}
.fa-magnet:before {
  content: "\f076";
}
.fa-chevron-up:before {
  content: "\f077";
}
.fa-chevron-down:before {
  content: "\f078";
}
.fa-retweet:before {
  content: "\f079";
}
.fa-shopping-cart:before {
  content: "\f07a";
}
.fa-folder:before {
  content: "\f07b";
}
.fa-folder-open:before {
  content: "\f07c";
}
.fa-arrows-v:before {
  content: "\f07d";
}
.fa-arrows-h:before {
  content: "\f07e";
}
.fa-bar-chart-o:before,
.fa-bar-chart:before {
  content: "\f080";
}
.fa-twitter-square:before {
  content: "\f081";
}
.fa-facebook-square:before {
  content: "\f082";
}
.fa-camera-retro:before {
  content: "\f083";
}
.fa-key:before {
  content: "\f084";
}
.fa-gears:before,
.fa-cogs:before {
  content: "\f085";
}
.fa-comments:before {
  content: "\f086";
}
.fa-thumbs-o-up:before {
  content: "\f087";
}
.fa-thumbs-o-down:before {
  content: "\f088";
}
.fa-star-half:before {
  content: "\f089";
}
.fa-heart-o:before {
  content: "\f08a";
}
.fa-sign-out:before {
  content: "\f08b";
}
.fa-linkedin-square:before {
  content: "\f08c";
}
.fa-thumb-tack:before {
  content: "\f08d";
}
.fa-external-link:before {
  content: "\f08e";
}
.fa-sign-in:before {
  content: "\f090";
}
.fa-trophy:before {
  content: "\f091";
}
.fa-github-square:before {
  content: "\f092";
}
.fa-upload:before {
  content: "\f093";
}
.fa-lemon-o:before {
  content: "\f094";
}
.fa-phone:before {
  content: "\f095";
}
.fa-square-o:before {
  content: "\f096";
}
.fa-bookmark-o:before {
  content: "\f097";
}
.fa-phone-square:before {
  content: "\f098";
}
.fa-twitter:before {
  content: "\f099";
}
.fa-facebook-f:before,
.fa-facebook:before {
  content: "\f09a";
}
.fa-github:before {
  content: "\f09b";
}
.fa-unlock:before {
  content: "\f09c";
}
.fa-credit-card:before {
  content: "\f09d";
}
.fa-feed:before,
.fa-rss:before {
  content: "\f09e";
}
.fa-hdd-o:before {
  content: "\f0a0";
}
.fa-bullhorn:before {
  content: "\f0a1";
}
.fa-bell:before {
  content: "\f0f3";
}
.fa-certificate:before {
  content: "\f0a3";
}
.fa-hand-o-right:before {
  content: "\f0a4";
}
.fa-hand-o-left:before {
  content: "\f0a5";
}
.fa-hand-o-up:before {
  content: "\f0a6";
}
.fa-hand-o-down:before {
  content: "\f0a7";
}
.fa-arrow-circle-left:before {
  content: "\f0a8";
}
.fa-arrow-circle-right:before {
  content: "\f0a9";
}
.fa-arrow-circle-up:before {
  content: "\f0aa";
}
.fa-arrow-circle-down:before {
  content: "\f0ab";
}
.fa-globe:before {
  content: "\f0ac";
}
.fa-wrench:before {
  content: "\f0ad";
}
.fa-tasks:before {
  content: "\f0ae";
}
.fa-filter:before {
  content: "\f0b0";
}
.fa-briefcase:before {
  content: "\f0b1";
}
.fa-arrows-alt:before {
  content: "\f0b2";
}
.fa-group:before,
.fa-users:before {
  content: "\f0c0";
}
.fa-chain:before,
.fa-link:before {
  content: "\f0c1";
}
.fa-cloud:before {
  content: "\f0c2";
}
.fa-flask:before {
  content: "\f0c3";
}
.fa-cut:before,
.fa-scissors:before {
  content: "\f0c4";
}
.fa-copy:before,
.fa-files-o:before {
  content: "\f0c5";
}
.fa-paperclip:before {
  content: "\f0c6";
}
.fa-save:before,
.fa-floppy-o:before {
  content: "\f0c7";
}
.fa-square:before {
  content: "\f0c8";
}
.fa-navicon:before,
.fa-reorder:before,
.fa-bars:before {
  content: "\f0c9";
}
.fa-list-ul:before {
  content: "\f0ca";
}
.fa-list-ol:before {
  content: "\f0cb";
}
.fa-strikethrough:before {
  content: "\f0cc";
}
.fa-underline:before {
  content: "\f0cd";
}
.fa-table:before {
  content: "\f0ce";
}
.fa-magic:before {
  content: "\f0d0";
}
.fa-truck:before {
  content: "\f0d1";
}
.fa-pinterest:before {
  content: "\f0d2";
}
.fa-pinterest-square:before {
  content: "\f0d3";
}
.fa-google-plus-square:before {
  content: "\f0d4";
}
.fa-google-plus:before {
  content: "\f0d5";
}
.fa-money:before {
  content: "\f0d6";
}
.fa-caret-down:before {
  content: "\f0d7";
}
.fa-caret-up:before {
  content: "\f0d8";
}
.fa-caret-left:before {
  content: "\f0d9";
}
.fa-caret-right:before {
  content: "\f0da";
}
.fa-columns:before {
  content: "\f0db";
}
.fa-unsorted:before,
.fa-sort:before {
  content: "\f0dc";
}
.fa-sort-down:before,
.fa-sort-desc:before {
  content: "\f0dd";
}
.fa-sort-up:before,
.fa-sort-asc:before {
  content: "\f0de";
}
.fa-envelope:before {
  content: "\f0e0";
}
.fa-linkedin:before {
  content: "\f0e1";
}
.fa-rotate-left:before,
.fa-undo:before {
  content: "\f0e2";
}
.fa-legal:before,
.fa-gavel:before {
  content: "\f0e3";
}
.fa-dashboard:before,
.fa-tachometer:before {
  content: "\f0e4";
}
.fa-comment-o:before {
  content: "\f0e5";
}
.fa-comments-o:before {
  content: "\f0e6";
}
.fa-flash:before,
.fa-bolt:before {
  content: "\f0e7";
}
.fa-sitemap:before {
  content: "\f0e8";
}
.fa-umbrella:before {
  content: "\f0e9";
}
.fa-paste:before,
.fa-clipboard:before {
  content: "\f0ea";
}
.fa-lightbulb-o:before {
  content: "\f0eb";
}
.fa-exchange:before {
  content: "\f0ec";
}
.fa-cloud-download:before {
  content: "\f0ed";
}
.fa-cloud-upload:before {
  content: "\f0ee";
}
.fa-user-md:before {
  content: "\f0f0";
}
.fa-stethoscope:before {
  content: "\f0f1";
}
.fa-suitcase:before {
  content: "\f0f2";
}
.fa-bell-o:before {
  content: "\f0a2";
}
.fa-coffee:before {
  content: "\f0f4";
}
.fa-cutlery:before {
  content: "\f0f5";
}
.fa-file-text-o:before {
  content: "\f0f6";
}
.fa-building-o:before {
  content: "\f0f7";
}
.fa-hospital-o:before {
  content: "\f0f8";
}
.fa-ambulance:before {
  content: "\f0f9";
}
.fa-medkit:before {
  content: "\f0fa";
}
.fa-fighter-jet:before {
  content: "\f0fb";
}
.fa-beer:before {
  content: "\f0fc";
}
.fa-h-square:before {
  content: "\f0fd";
}
.fa-plus-square:before {
  content: "\f0fe";
}
.fa-angle-double-left:before {
  content: "\f100";
}
.fa-angle-double-right:before {
  content: "\f101";
}
.fa-angle-double-up:before {
  content: "\f102";
}
.fa-angle-double-down:before {
  content: "\f103";
}
.fa-angle-left:before {
  content: "\f104";
}
.fa-angle-right:before {
  content: "\f105";
}
.fa-angle-up:before {
  content: "\f106";
}
.fa-angle-down:before {
  content: "\f107";
}
.fa-desktop:before {
  content: "\f108";
}
.fa-laptop:before {
  content: "\f109";
}
.fa-tablet:before {
  content: "\f10a";
}
.fa-mobile-phone:before,
.fa-mobile:before {
  content: "\f10b";
}
.fa-circle-o:before {
  content: "\f10c";
}
.fa-quote-left:before {
  content: "\f10d";
}
.fa-quote-right:before {
  content: "\f10e";
}
.fa-spinner:before {
  content: "\f110";
}
.fa-circle:before {
  content: "\f111";
}
.fa-mail-reply:before,
.fa-reply:before {
  content: "\f112";
}
.fa-github-alt:before {
  content: "\f113";
}
.fa-folder-o:before {
  content: "\f114";
}
.fa-folder-open-o:before {
  content: "\f115";
}
.fa-smile-o:before {
  content: "\f118";
}
.fa-frown-o:before {
  content: "\f119";
}
.fa-meh-o:before {
  content: "\f11a";
}
.fa-gamepad:before {
  content: "\f11b";
}
.fa-keyboard-o:before {
  content: "\f11c";
}
.fa-flag-o:before {
  content: "\f11d";
}
.fa-flag-checkered:before {
  content: "\f11e";
}
.fa-terminal:before {
  content: "\f120";
}
.fa-code:before {
  content: "\f121";
}
.fa-mail-reply-all:before,
.fa-reply-all:before {
  content: "\f122";
}
.fa-star-half-empty:before,
.fa-star-half-full:before,
.fa-star-half-o:before {
  content: "\f123";
}
.fa-location-arrow:before {
  content: "\f124";
}
.fa-crop:before {
  content: "\f125";
}
.fa-code-fork:before {
  content: "\f126";
}
.fa-unlink:before,
.fa-chain-broken:before {
  content: "\f127";
}
.fa-question:before {
  content: "\f128";
}
.fa-info:before {
  content: "\f129";
}
.fa-exclamation:before {
  content: "\f12a";
}
.fa-superscript:before {
  content: "\f12b";
}
.fa-subscript:before {
  content: "\f12c";
}
.fa-eraser:before {
  content: "\f12d";
}
.fa-puzzle-piece:before {
  content: "\f12e";
}
.fa-microphone:before {
  content: "\f130";
}
.fa-microphone-slash:before {
  content: "\f131";
}
.fa-shield:before {
  content: "\f132";
}
.fa-calendar-o:before {
  content: "\f133";
}
.fa-fire-extinguisher:before {
  content: "\f134";
}
.fa-rocket:before {
  content: "\f135";
}
.fa-maxcdn:before {
  content: "\f136";
}
.fa-chevron-circle-left:before {
  content: "\f137";
}
.fa-chevron-circle-right:before {
  content: "\f138";
}
.fa-chevron-circle-up:before {
  content: "\f139";
}
.fa-chevron-circle-down:before {
  content: "\f13a";
}
.fa-html5:before {
  content: "\f13b";
}
.fa-css3:before {
  content: "\f13c";
}
.fa-anchor:before {
  content: "\f13d";
}
.fa-unlock-alt:before {
  content: "\f13e";
}
.fa-bullseye:before {
  content: "\f140";
}
.fa-ellipsis-h:before {
  content: "\f141";
}
.fa-ellipsis-v:before {
  content: "\f142";
}
.fa-rss-square:before {
  content: "\f143";
}
.fa-play-circle:before {
  content: "\f144";
}
.fa-ticket:before {
  content: "\f145";
}
.fa-minus-square:before {
  content: "\f146";
}
.fa-minus-square-o:before {
  content: "\f147";
}
.fa-level-up:before {
  content: "\f148";
}
.fa-level-down:before {
  content: "\f149";
}
.fa-check-square:before {
  content: "\f14a";
}
.fa-pencil-square:before {
  content: "\f14b";
}
.fa-external-link-square:before {
  content: "\f14c";
}
.fa-share-square:before {
  content: "\f14d";
}
.fa-compass:before {
  content: "\f14e";
}
.fa-toggle-down:before,
.fa-caret-square-o-down:before {
  content: "\f150";
}
.fa-toggle-up:before,
.fa-caret-square-o-up:before {
  content: "\f151";
}
.fa-toggle-right:before,
.fa-caret-square-o-right:before {
  content: "\f152";
}
.fa-euro:before,
.fa-eur:before {
  content: "\f153";
}
.fa-gbp:before {
  content: "\f154";
}
.fa-dollar:before,
.fa-usd:before {
  content: "\f155";
}
.fa-rupee:before,
.fa-inr:before {
  content: "\f156";
}
.fa-cny:before,
.fa-rmb:before,
.fa-yen:before,
.fa-jpy:before {
  content: "\f157";
}
.fa-ruble:before,
.fa-rouble:before,
.fa-rub:before {
  content: "\f158";
}
.fa-won:before,
.fa-krw:before {
  content: "\f159";
}
.fa-bitcoin:before,
.fa-btc:before {
  content: "\f15a";
}
.fa-file:before {
  content: "\f15b";
}
.fa-file-text:before {
  content: "\f15c";
}
.fa-sort-alpha-asc:before {
  content: "\f15d";
}
.fa-sort-alpha-desc:before {
  content: "\f15e";
}
.fa-sort-amount-asc:before {
  content: "\f160";
}
.fa-sort-amount-desc:before {
  content: "\f161";
}
.fa-sort-numeric-asc:before {
  content: "\f162";
}
.fa-sort-numeric-desc:before {
  content: "\f163";
}
.fa-thumbs-up:before {
  content: "\f164";
}
.fa-thumbs-down:before {
  content: "\f165";
}
.fa-youtube-square:before {
  content: "\f166";
}
.fa-youtube:before {
  content: "\f167";
}
.fa-xing:before {
  content: "\f168";
}
.fa-xing-square:before {
  content: "\f169";
}
.fa-youtube-play:before {
  content: "\f16a";
}
.fa-dropbox:before {
  content: "\f16b";
}
.fa-stack-overflow:before {
  content: "\f16c";
}
.fa-instagram:before {
  content: "\f16d";
}
.fa-flickr:before {
  content: "\f16e";
}
.fa-adn:before {
  content: "\f170";
}
.fa-bitbucket:before {
  content: "\f171";
}
.fa-bitbucket-square:before {
  content: "\f172";
}
.fa-tumblr:before {
  content: "\f173";
}
.fa-tumblr-square:before {
  content: "\f174";
}
.fa-long-arrow-down:before {
  content: "\f175";
}
.fa-long-arrow-up:before {
  content: "\f176";
}
.fa-long-arrow-left:before {
  content: "\f177";
}
.fa-long-arrow-right:before {
  content: "\f178";
}
.fa-apple:before {
  content: "\f179";
}
.fa-windows:before {
  content: "\f17a";
}
.fa-android:before {
  content: "\f17b";
}
.fa-linux:before {
  content: "\f17c";
}
.fa-dribbble:before {
  content: "\f17d";
}
.fa-skype:before {
  content: "\f17e";
}
.fa-foursquare:before {
  content: "\f180";
}
.fa-trello:before {
  content: "\f181";
}
.fa-female:before {
  content: "\f182";
}
.fa-male:before {
  content: "\f183";
}
.fa-gittip:before,
.fa-gratipay:before {
  content: "\f184";
}
.fa-sun-o:before {
  content: "\f185";
}
.fa-moon-o:before {
  content: "\f186";
}
.fa-archive:before {
  content: "\f187";
}
.fa-bug:before {
  content: "\f188";
}
.fa-vk:before {
  content: "\f189";
}
.fa-weibo:before {
  content: "\f18a";
}
.fa-renren:before {
  content: "\f18b";
}
.fa-pagelines:before {
  content: "\f18c";
}
.fa-stack-exchange:before {
  content: "\f18d";
}
.fa-arrow-circle-o-right:before {
  content: "\f18e";
}
.fa-arrow-circle-o-left:before {
  content: "\f190";
}
.fa-toggle-left:before,
.fa-caret-square-o-left:before {
  content: "\f191";
}
.fa-dot-circle-o:before {
  content: "\f192";
}
.fa-wheelchair:before {
  content: "\f193";
}
.fa-vimeo-square:before {
  content: "\f194";
}
.fa-turkish-lira:before,
.fa-try:before {
  content: "\f195";
}
.fa-plus-square-o:before {
  content: "\f196";
}
.fa-space-shuttle:before {
  content: "\f197";
}
.fa-slack:before {
  content: "\f198";
}
.fa-envelope-square:before {
  content: "\f199";
}
.fa-wordpress:before {
  content: "\f19a";
}
.fa-openid:before {
  content: "\f19b";
}
.fa-institution:before,
.fa-bank:before,
.fa-university:before {
  content: "\f19c";
}
.fa-mortar-board:before,
.fa-graduation-cap:before {
  content: "\f19d";
}
.fa-yahoo:before {
  content: "\f19e";
}
.fa-google:before {
  content: "\f1a0";
}
.fa-reddit:before {
  content: "\f1a1";
}
.fa-reddit-square:before {
  content: "\f1a2";
}
.fa-stumbleupon-circle:before {
  content: "\f1a3";
}
.fa-stumbleupon:before {
  content: "\f1a4";
}
.fa-delicious:before {
  content: "\f1a5";
}
.fa-digg:before {
  content: "\f1a6";
}
.fa-pied-piper-pp:before {
  content: "\f1a7";
}
.fa-pied-piper-alt:before {
  content: "\f1a8";
}
.fa-drupal:before {
  content: "\f1a9";
}
.fa-joomla:before {
  content: "\f1aa";
}
.fa-language:before {
  content: "\f1ab";
}
.fa-fax:before {
  content: "\f1ac";
}
.fa-building:before {
  content: "\f1ad";
}
.fa-child:before {
  content: "\f1ae";
}
.fa-paw:before {
  content: "\f1b0";
}
.fa-spoon:before {
  content: "\f1b1";
}
.fa-cube:before {
  content: "\f1b2";
}
.fa-cubes:before {
  content: "\f1b3";
}
.fa-behance:before {
  content: "\f1b4";
}
.fa-behance-square:before {
  content: "\f1b5";
}
.fa-steam:before {
  content: "\f1b6";
}
.fa-steam-square:before {
  content: "\f1b7";
}
.fa-recycle:before {
  content: "\f1b8";
}
.fa-automobile:before,
.fa-car:before {
  content: "\f1b9";
}
.fa-cab:before,
.fa-taxi:before {
  content: "\f1ba";
}
.fa-tree:before {
  content: "\f1bb";
}
.fa-spotify:before {
  content: "\f1bc";
}
.fa-deviantart:before {
  content: "\f1bd";
}
.fa-soundcloud:before {
  content: "\f1be";
}
.fa-database:before {
  content: "\f1c0";
}
.fa-file-pdf-o:before {
  content: "\f1c1";
}
.fa-file-word-o:before {
  content: "\f1c2";
}
.fa-file-excel-o:before {
  content: "\f1c3";
}
.fa-file-powerpoint-o:before {
  content: "\f1c4";
}
.fa-file-photo-o:before,
.fa-file-picture-o:before,
.fa-file-image-o:before {
  content: "\f1c5";
}
.fa-file-zip-o:before,
.fa-file-archive-o:before {
  content: "\f1c6";
}
.fa-file-sound-o:before,
.fa-file-audio-o:before {
  content: "\f1c7";
}
.fa-file-movie-o:before,
.fa-file-video-o:before {
  content: "\f1c8";
}
.fa-file-code-o:before {
  content: "\f1c9";
}
.fa-vine:before {
  content: "\f1ca";
}
.fa-codepen:before {
  content: "\f1cb";
}
.fa-jsfiddle:before {
  content: "\f1cc";
}
.fa-life-bouy:before,
.fa-life-buoy:before,
.fa-life-saver:before,
.fa-support:before,
.fa-life-ring:before {
  content: "\f1cd";
}
.fa-circle-o-notch:before {
  content: "\f1ce";
}
.fa-ra:before,
.fa-resistance:before,
.fa-rebel:before {
  content: "\f1d0";
}
.fa-ge:before,
.fa-empire:before {
  content: "\f1d1";
}
.fa-git-square:before {
  content: "\f1d2";
}
.fa-git:before {
  content: "\f1d3";
}
.fa-y-combinator-square:before,
.fa-yc-square:before,
.fa-hacker-news:before {
  content: "\f1d4";
}
.fa-tencent-weibo:before {
  content: "\f1d5";
}
.fa-qq:before {
  content: "\f1d6";
}
.fa-wechat:before,
.fa-weixin:before {
  content: "\f1d7";
}
.fa-send:before,
.fa-paper-plane:before {
  content: "\f1d8";
}
.fa-send-o:before,
.fa-paper-plane-o:before {
  content: "\f1d9";
}
.fa-history:before {
  content: "\f1da";
}
.fa-circle-thin:before {
  content: "\f1db";
}
.fa-header:before {
  content: "\f1dc";
}
.fa-paragraph:before {
  content: "\f1dd";
}
.fa-sliders:before {
  content: "\f1de";
}
.fa-share-alt:before {
  content: "\f1e0";
}
.fa-share-alt-square:before {
  content: "\f1e1";
}
.fa-bomb:before {
  content: "\f1e2";
}
.fa-soccer-ball-o:before,
.fa-futbol-o:before {
  content: "\f1e3";
}
.fa-tty:before {
  content: "\f1e4";
}
.fa-binoculars:before {
  content: "\f1e5";
}
.fa-plug:before {
  content: "\f1e6";
}
.fa-slideshare:before {
  content: "\f1e7";
}
.fa-twitch:before {
  content: "\f1e8";
}
.fa-yelp:before {
  content: "\f1e9";
}
.fa-newspaper-o:before {
  content: "\f1ea";
}
.fa-wifi:before {
  content: "\f1eb";
}
.fa-calculator:before {
  content: "\f1ec";
}
.fa-paypal:before {
  content: "\f1ed";
}
.fa-google-wallet:before {
  content: "\f1ee";
}
.fa-cc-visa:before {
  content: "\f1f0";
}
.fa-cc-mastercard:before {
  content: "\f1f1";
}
.fa-cc-discover:before {
  content: "\f1f2";
}
.fa-cc-amex:before {
  content: "\f1f3";
}
.fa-cc-paypal:before {
  content: "\f1f4";
}
.fa-cc-stripe:before {
  content: "\f1f5";
}
.fa-bell-slash:before {
  content: "\f1f6";
}
.fa-bell-slash-o:before {
  content: "\f1f7";
}
.fa-trash:before {
  content: "\f1f8";
}
.fa-copyright:before {
  content: "\f1f9";
}
.fa-at:before {
  content: "\f1fa";
}
.fa-eyedropper:before {
  content: "\f1fb";
}
.fa-paint-brush:before {
  content: "\f1fc";
}
.fa-birthday-cake:before {
  content: "\f1fd";
}
.fa-area-chart:before {
  content: "\f1fe";
}
.fa-pie-chart:before {
  content: "\f200";
}
.fa-line-chart:before {
  content: "\f201";
}
.fa-lastfm:before {
  content: "\f202";
}
.fa-lastfm-square:before {
  content: "\f203";
}
.fa-toggle-off:before {
  content: "\f204";
}
.fa-toggle-on:before {
  content: "\f205";
}
.fa-bicycle:before {
  content: "\f206";
}
.fa-bus:before {
  content: "\f207";
}
.fa-ioxhost:before {
  content: "\f208";
}
.fa-angellist:before {
  content: "\f209";
}
.fa-cc:before {
  content: "\f20a";
}
.fa-shekel:before,
.fa-sheqel:before,
.fa-ils:before {
  content: "\f20b";
}
.fa-meanpath:before {
  content: "\f20c";
}
.fa-buysellads:before {
  content: "\f20d";
}
.fa-connectdevelop:before {
  content: "\f20e";
}
.fa-dashcube:before {
  content: "\f210";
}
.fa-forumbee:before {
  content: "\f211";
}
.fa-leanpub:before {
  content: "\f212";
}
.fa-sellsy:before {
  content: "\f213";
}
.fa-shirtsinbulk:before {
  content: "\f214";
}
.fa-simplybuilt:before {
  content: "\f215";
}
.fa-skyatlas:before {
  content: "\f216";
}
.fa-cart-plus:before {
  content: "\f217";
}
.fa-cart-arrow-down:before {
  content: "\f218";
}
.fa-diamond:before {
  content: "\f219";
}
.fa-ship:before {
  content: "\f21a";
}
.fa-user-secret:before {
  content: "\f21b";
}
.fa-motorcycle:before {
  content: "\f21c";
}
.fa-street-view:before {
  content: "\f21d";
}
.fa-heartbeat:before {
  content: "\f21e";
}
.fa-venus:before {
  content: "\f221";
}
.fa-mars:before {
  content: "\f222";
}
.fa-mercury:before {
  content: "\f223";
}
.fa-intersex:before,
.fa-transgender:before {
  content: "\f224";
}
.fa-transgender-alt:before {
  content: "\f225";
}
.fa-venus-double:before {
  content: "\f226";
}
.fa-mars-double:before {
  content: "\f227";
}
.fa-venus-mars:before {
  content: "\f228";
}
.fa-mars-stroke:before {
  content: "\f229";
}
.fa-mars-stroke-v:before {
  content: "\f22a";
}
.fa-mars-stroke-h:before {
  content: "\f22b";
}
.fa-neuter:before {
  content: "\f22c";
}
.fa-genderless:before {
  content: "\f22d";
}
.fa-facebook-official:before {
  content: "\f230";
}
.fa-pinterest-p:before {
  content: "\f231";
}
.fa-whatsapp:before {
  content: "\f232";
}
.fa-server:before {
  content: "\f233";
}
.fa-user-plus:before {
  content: "\f234";
}
.fa-user-times:before {
  content: "\f235";
}
.fa-hotel:before,
.fa-bed:before {
  content: "\f236";
}
.fa-viacoin:before {
  content: "\f237";
}
.fa-train:before {
  content: "\f238";
}
.fa-subway:before {
  content: "\f239";
}
.fa-medium:before {
  content: "\f23a";
}
.fa-yc:before,
.fa-y-combinator:before {
  content: "\f23b";
}
.fa-optin-monster:before {
  content: "\f23c";
}
.fa-opencart:before {
  content: "\f23d";
}
.fa-expeditedssl:before {
  content: "\f23e";
}
.fa-battery-4:before,
.fa-battery:before,
.fa-battery-full:before {
  content: "\f240";
}
.fa-battery-3:before,
.fa-battery-three-quarters:before {
  content: "\f241";
}
.fa-battery-2:before,
.fa-battery-half:before {
  content: "\f242";
}
.fa-battery-1:before,
.fa-battery-quarter:before {
  content: "\f243";
}
.fa-battery-0:before,
.fa-battery-empty:before {
  content: "\f244";
}
.fa-mouse-pointer:before {
  content: "\f245";
}
.fa-i-cursor:before {
  content: "\f246";
}
.fa-object-group:before {
  content: "\f247";
}
.fa-object-ungroup:before {
  content: "\f248";
}
.fa-sticky-note:before {
  content: "\f249";
}
.fa-sticky-note-o:before {
  content: "\f24a";
}
.fa-cc-jcb:before {
  content: "\f24b";
}
.fa-cc-diners-club:before {
  content: "\f24c";
}
.fa-clone:before {
  content: "\f24d";
}
.fa-balance-scale:before {
  content: "\f24e";
}
.fa-hourglass-o:before {
  content: "\f250";
}
.fa-hourglass-1:before,
.fa-hourglass-start:before {
  content: "\f251";
}
.fa-hourglass-2:before,
.fa-hourglass-half:before {
  content: "\f252";
}
.fa-hourglass-3:before,
.fa-hourglass-end:before {
  content: "\f253";
}
.fa-hourglass:before {
  content: "\f254";
}
.fa-hand-grab-o:before,
.fa-hand-rock-o:before {
  content: "\f255";
}
.fa-hand-stop-o:before,
.fa-hand-paper-o:before {
  content: "\f256";
}
.fa-hand-scissors-o:before {
  content: "\f257";
}
.fa-hand-lizard-o:before {
  content: "\f258";
}
.fa-hand-spock-o:before {
  content: "\f259";
}
.fa-hand-pointer-o:before {
  content: "\f25a";
}
.fa-hand-peace-o:before {
  content: "\f25b";
}
.fa-trademark:before {
  content: "\f25c";
}
.fa-registered:before {
  content: "\f25d";
}
.fa-creative-commons:before {
  content: "\f25e";
}
.fa-gg:before {
  content: "\f260";
}
.fa-gg-circle:before {
  content: "\f261";
}
.fa-tripadvisor:before {
  content: "\f262";
}
.fa-odnoklassniki:before {
  content: "\f263";
}
.fa-odnoklassniki-square:before {
  content: "\f264";
}
.fa-get-pocket:before {
  content: "\f265";
}
.fa-wikipedia-w:before {
  content: "\f266";
}
.fa-safari:before {
  content: "\f267";
}
.fa-chrome:before {
  content: "\f268";
}
.fa-firefox:before {
  content: "\f269";
}
.fa-opera:before {
  content: "\f26a";
}
.fa-internet-explorer:before {
  content: "\f26b";
}
.fa-tv:before,
.fa-television:before {
  content: "\f26c";
}
.fa-contao:before {
  content: "\f26d";
}
.fa-500px:before {
  content: "\f26e";
}
.fa-amazon:before {
  content: "\f270";
}
.fa-calendar-plus-o:before {
  content: "\f271";
}
.fa-calendar-minus-o:before {
  content: "\f272";
}
.fa-calendar-times-o:before {
  content: "\f273";
}
.fa-calendar-check-o:before {
  content: "\f274";
}
.fa-industry:before {
  content: "\f275";
}
.fa-map-pin:before {
  content: "\f276";
}
.fa-map-signs:before {
  content: "\f277";
}
.fa-map-o:before {
  content: "\f278";
}
.fa-map:before {
  content: "\f279";
}
.fa-commenting:before {
  content: "\f27a";
}
.fa-commenting-o:before {
  content: "\f27b";
}
.fa-houzz:before {
  content: "\f27c";
}
.fa-vimeo:before {
  content: "\f27d";
}
.fa-black-tie:before {
  content: "\f27e";
}
.fa-fonticons:before {
  content: "\f280";
}
.fa-reddit-alien:before {
  content: "\f281";
}
.fa-edge:before {
  content: "\f282";
}
.fa-credit-card-alt:before {
  content: "\f283";
}
.fa-codiepie:before {
  content: "\f284";
}
.fa-modx:before {
  content: "\f285";
}
.fa-fort-awesome:before {
  content: "\f286";
}
.fa-usb:before {
  content: "\f287";
}
.fa-product-hunt:before {
  content: "\f288";
}
.fa-mixcloud:before {
  content: "\f289";
}
.fa-scribd:before {
  content: "\f28a";
}
.fa-pause-circle:before {
  content: "\f28b";
}
.fa-pause-circle-o:before {
  content: "\f28c";
}
.fa-stop-circle:before {
  content: "\f28d";
}
.fa-stop-circle-o:before {
  content: "\f28e";
}
.fa-shopping-bag:before {
  content: "\f290";
}
.fa-shopping-basket:before {
  content: "\f291";
}
.fa-hashtag:before {
  content: "\f292";
}
.fa-bluetooth:before {
  content: "\f293";
}
.fa-bluetooth-b:before {
  content: "\f294";
}
.fa-percent:before {
  content: "\f295";
}
.fa-gitlab:before {
  content: "\f296";
}
.fa-wpbeginner:before {
  content: "\f297";
}
.fa-wpforms:before {
  content: "\f298";
}
.fa-envira:before {
  content: "\f299";
}
.fa-universal-access:before {
  content: "\f29a";
}
.fa-wheelchair-alt:before {
  content: "\f29b";
}
.fa-question-circle-o:before {
  content: "\f29c";
}
.fa-blind:before {
  content: "\f29d";
}
.fa-audio-description:before {
  content: "\f29e";
}
.fa-volume-control-phone:before {
  content: "\f2a0";
}
.fa-braille:before {
  content: "\f2a1";
}
.fa-assistive-listening-systems:before {
  content: "\f2a2";
}
.fa-asl-interpreting:before,
.fa-american-sign-language-interpreting:before {
  content: "\f2a3";
}
.fa-deafness:before,
.fa-hard-of-hearing:before,
.fa-deaf:before {
  content: "\f2a4";
}
.fa-glide:before {
  content: "\f2a5";
}
.fa-glide-g:before {
  content: "\f2a6";
}
.fa-signing:before,
.fa-sign-language:before {
  content: "\f2a7";
}
.fa-low-vision:before {
  content: "\f2a8";
}
.fa-viadeo:before {
  content: "\f2a9";
}
.fa-viadeo-square:before {
  content: "\f2aa";
}
.fa-snapchat:before {
  content: "\f2ab";
}
.fa-snapchat-ghost:before {
  content: "\f2ac";
}
.fa-snapchat-square:before {
  content: "\f2ad";
}
.fa-pied-piper:before {
  content: "\f2ae";
}
.fa-first-order:before {
  content: "\f2b0";
}
.fa-yoast:before {
  content: "\f2b1";
}
.fa-themeisle:before {
  content: "\f2b2";
}
.fa-google-plus-circle:before,
.fa-google-plus-official:before {
  content: "\f2b3";
}
.fa-fa:before,
.fa-font-awesome:before {
  content: "\f2b4";
}
.fa-handshake-o:before {
  content: "\f2b5";
}
.fa-envelope-open:before {
  content: "\f2b6";
}
.fa-envelope-open-o:before {
  content: "\f2b7";
}
.fa-linode:before {
  content: "\f2b8";
}
.fa-address-book:before {
  content: "\f2b9";
}
.fa-address-book-o:before {
  content: "\f2ba";
}
.fa-vcard:before,
.fa-address-card:before {
  content: "\f2bb";
}
.fa-vcard-o:before,
.fa-address-card-o:before {
  content: "\f2bc";
}
.fa-user-circle:before {
  content: "\f2bd";
}
.fa-user-circle-o:before {
  content: "\f2be";
}
.fa-user-o:before {
  content: "\f2c0";
}
.fa-id-badge:before {
  content: "\f2c1";
}
.fa-drivers-license:before,
.fa-id-card:before {
  content: "\f2c2";
}
.fa-drivers-license-o:before,
.fa-id-card-o:before {
  content: "\f2c3";
}
.fa-quora:before {
  content: "\f2c4";
}
.fa-free-code-camp:before {
  content: "\f2c5";
}
.fa-telegram:before {
  content: "\f2c6";
}
.fa-thermometer-4:before,
.fa-thermometer:before,
.fa-thermometer-full:before {
  content: "\f2c7";
}
.fa-thermometer-3:before,
.fa-thermometer-three-quarters:before {
  content: "\f2c8";
}
.fa-thermometer-2:before,
.fa-thermometer-half:before {
  content: "\f2c9";
}
.fa-thermometer-1:before,
.fa-thermometer-quarter:before {
  content: "\f2ca";
}
.fa-thermometer-0:before,
.fa-thermometer-empty:before {
  content: "\f2cb";
}
.fa-shower:before {
  content: "\f2cc";
}
.fa-bathtub:before,
.fa-s15:before,
.fa-bath:before {
  content: "\f2cd";
}
.fa-podcast:before {
  content: "\f2ce";
}
.fa-window-maximize:before {
  content: "\f2d0";
}
.fa-window-minimize:before {
  content: "\f2d1";
}
.fa-window-restore:before {
  content: "\f2d2";
}
.fa-times-rectangle:before,
.fa-window-close:before {
  content: "\f2d3";
}
.fa-times-rectangle-o:before,
.fa-window-close-o:before {
  content: "\f2d4";
}
.fa-bandcamp:before {
  content: "\f2d5";
}
.fa-grav:before {
  content: "\f2d6";
}
.fa-etsy:before {
  content: "\f2d7";
}
.fa-imdb:before {
  content: "\f2d8";
}
.fa-ravelry:before {
  content: "\f2d9";
}
.fa-eercast:before {
  content: "\f2da";
}
.fa-microchip:before {
  content: "\f2db";
}
.fa-snowflake-o:before {
  content: "\f2dc";
}
.fa-superpowers:before {
  content: "\f2dd";
}
.fa-wpexplorer:before {
  content: "\f2de";
}
.fa-meetup:before {
  content: "\f2e0";
}
.sr-only {
  position: absolute;
  width: 1px;
  height: 1px;
  padding: 0;
  margin: -1px;
  overflow: hidden;
  clip: rect(0, 0, 0, 0);
  border: 0;
}
.sr-only-focusable:active,
.sr-only-focusable:focus {
  position: static;
  width: auto;
  height: auto;
  margin: 0;
  overflow: visible;
  clip: auto;
}
.sr-only-focusable:active,
.sr-only-focusable:focus {
  position: static;
  width: auto;
  height: auto;
  margin: 0;
  overflow: visible;
  clip: auto;
}
/*!
*
* IPython base
*
*/
.modal.fade .modal-dialog {
  -webkit-transform: translate(0, 0);
  -ms-transform: translate(0, 0);
  -o-transform: translate(0, 0);
  transform: translate(0, 0);
}
code {
  color: #000;
}
pre {
  font-size: inherit;
  line-height: inherit;
}
label {
  font-weight: normal;
}
/* Make the page background atleast 100% the height of the view port */
/* Make the page itself atleast 70% the height of the view port */
.border-box-sizing {
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
}
.corner-all {
  border-radius: 2px;
}
.no-padding {
  padding: 0px;
}
/* Flexible box model classes */
/* Taken from Alex Russell http://infrequently.org/2009/08/css-3-progress/ */
/* This file is a compatability layer.  It allows the usage of flexible box 
model layouts accross multiple browsers, including older browsers.  The newest,
universal implementation of the flexible box model is used when available (see
`Modern browsers` comments below).  Browsers that are known to implement this 
new spec completely include:

    Firefox 28.0+
    Chrome 29.0+
    Internet Explorer 11+ 
    Opera 17.0+

Browsers not listed, including Safari, are supported via the styling under the
`Old browsers` comments below.
*/
.hbox {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
.hbox > * {
  /* Old browsers */
  -webkit-box-flex: 0;
  -moz-box-flex: 0;
  box-flex: 0;
  /* Modern browsers */
  flex: none;
}
.vbox {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
}
.vbox > * {
  /* Old browsers */
  -webkit-box-flex: 0;
  -moz-box-flex: 0;
  box-flex: 0;
  /* Modern browsers */
  flex: none;
}
.hbox.reverse,
.vbox.reverse,
.reverse {
  /* Old browsers */
  -webkit-box-direction: reverse;
  -moz-box-direction: reverse;
  box-direction: reverse;
  /* Modern browsers */
  flex-direction: row-reverse;
}
.hbox.box-flex0,
.vbox.box-flex0,
.box-flex0 {
  /* Old browsers */
  -webkit-box-flex: 0;
  -moz-box-flex: 0;
  box-flex: 0;
  /* Modern browsers */
  flex: none;
  width: auto;
}
.hbox.box-flex1,
.vbox.box-flex1,
.box-flex1 {
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
}
.hbox.box-flex,
.vbox.box-flex,
.box-flex {
  /* Old browsers */
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
}
.hbox.box-flex2,
.vbox.box-flex2,
.box-flex2 {
  /* Old browsers */
  -webkit-box-flex: 2;
  -moz-box-flex: 2;
  box-flex: 2;
  /* Modern browsers */
  flex: 2;
}
.box-group1 {
  /*  Deprecated */
  -webkit-box-flex-group: 1;
  -moz-box-flex-group: 1;
  box-flex-group: 1;
}
.box-group2 {
  /* Deprecated */
  -webkit-box-flex-group: 2;
  -moz-box-flex-group: 2;
  box-flex-group: 2;
}
.hbox.start,
.vbox.start,
.start {
  /* Old browsers */
  -webkit-box-pack: start;
  -moz-box-pack: start;
  box-pack: start;
  /* Modern browsers */
  justify-content: flex-start;
}
.hbox.end,
.vbox.end,
.end {
  /* Old browsers */
  -webkit-box-pack: end;
  -moz-box-pack: end;
  box-pack: end;
  /* Modern browsers */
  justify-content: flex-end;
}
.hbox.center,
.vbox.center,
.center {
  /* Old browsers */
  -webkit-box-pack: center;
  -moz-box-pack: center;
  box-pack: center;
  /* Modern browsers */
  justify-content: center;
}
.hbox.baseline,
.vbox.baseline,
.baseline {
  /* Old browsers */
  -webkit-box-pack: baseline;
  -moz-box-pack: baseline;
  box-pack: baseline;
  /* Modern browsers */
  justify-content: baseline;
}
.hbox.stretch,
.vbox.stretch,
.stretch {
  /* Old browsers */
  -webkit-box-pack: stretch;
  -moz-box-pack: stretch;
  box-pack: stretch;
  /* Modern browsers */
  justify-content: stretch;
}
.hbox.align-start,
.vbox.align-start,
.align-start {
  /* Old browsers */
  -webkit-box-align: start;
  -moz-box-align: start;
  box-align: start;
  /* Modern browsers */
  align-items: flex-start;
}
.hbox.align-end,
.vbox.align-end,
.align-end {
  /* Old browsers */
  -webkit-box-align: end;
  -moz-box-align: end;
  box-align: end;
  /* Modern browsers */
  align-items: flex-end;
}
.hbox.align-center,
.vbox.align-center,
.align-center {
  /* Old browsers */
  -webkit-box-align: center;
  -moz-box-align: center;
  box-align: center;
  /* Modern browsers */
  align-items: center;
}
.hbox.align-baseline,
.vbox.align-baseline,
.align-baseline {
  /* Old browsers */
  -webkit-box-align: baseline;
  -moz-box-align: baseline;
  box-align: baseline;
  /* Modern browsers */
  align-items: baseline;
}
.hbox.align-stretch,
.vbox.align-stretch,
.align-stretch {
  /* Old browsers */
  -webkit-box-align: stretch;
  -moz-box-align: stretch;
  box-align: stretch;
  /* Modern browsers */
  align-items: stretch;
}
div.error {
  margin: 2em;
  text-align: center;
}
div.error > h1 {
  font-size: 500%;
  line-height: normal;
}
div.error > p {
  font-size: 200%;
  line-height: normal;
}
div.traceback-wrapper {
  text-align: left;
  max-width: 800px;
  margin: auto;
}
div.traceback-wrapper pre.traceback {
  max-height: 600px;
  overflow: auto;
}
/**
 * Primary styles
 *
 * Author: Jupyter Development Team
 */
body {
  background-color: #fff;
  /* This makes sure that the body covers the entire window and needs to
       be in a different element than the display: box in wrapper below */
  position: absolute;
  left: 0px;
  right: 0px;
  top: 0px;
  bottom: 0px;
  overflow: visible;
}
body > #header {
  /* Initially hidden to prevent FLOUC */
  display: none;
  background-color: #fff;
  /* Display over codemirror */
  position: relative;
  z-index: 100;
}
body > #header #header-container {
  display: flex;
  flex-direction: row;
  justify-content: space-between;
  padding: 5px;
  padding-bottom: 5px;
  padding-top: 5px;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
}
body > #header .header-bar {
  width: 100%;
  height: 1px;
  background: #e7e7e7;
  margin-bottom: -1px;
}
@media print {
  body > #header {
    display: none !important;
  }
}
#header-spacer {
  width: 100%;
  visibility: hidden;
}
@media print {
  #header-spacer {
    display: none;
  }
}
#ipython_notebook {
  padding-left: 0px;
  padding-top: 1px;
  padding-bottom: 1px;
}
[dir="rtl"] #ipython_notebook {
  margin-right: 10px;
  margin-left: 0;
}
[dir="rtl"] #ipython_notebook.pull-left {
  float: right !important;
  float: right;
}
.flex-spacer {
  flex: 1;
}
#noscript {
  width: auto;
  padding-top: 16px;
  padding-bottom: 16px;
  text-align: center;
  font-size: 22px;
  color: red;
  font-weight: bold;
}
#ipython_notebook img {
  height: 28px;
}
#site {
  width: 100%;
  display: none;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
  overflow: auto;
}
@media print {
  #site {
    height: auto !important;
  }
}
/* Smaller buttons */
.ui-button .ui-button-text {
  padding: 0.2em 0.8em;
  font-size: 77%;
}
input.ui-button {
  padding: 0.3em 0.9em;
}
span#kernel_logo_widget {
  margin: 0 10px;
}
span#login_widget {
  float: right;
}
[dir="rtl"] span#login_widget {
  float: left;
}
span#login_widget > .button,
#logout {
  color: #333;
  background-color: #fff;
  border-color: #ccc;
}
span#login_widget > .button:focus,
#logout:focus,
span#login_widget > .button.focus,
#logout.focus {
  color: #333;
  background-color: #e6e6e6;
  border-color: #8c8c8c;
}
span#login_widget > .button:hover,
#logout:hover {
  color: #333;
  background-color: #e6e6e6;
  border-color: #adadad;
}
span#login_widget > .button:active,
#logout:active,
span#login_widget > .button.active,
#logout.active,
.open > .dropdown-togglespan#login_widget > .button,
.open > .dropdown-toggle#logout {
  color: #333;
  background-color: #e6e6e6;
  border-color: #adadad;
}
span#login_widget > .button:active:hover,
#logout:active:hover,
span#login_widget > .button.active:hover,
#logout.active:hover,
.open > .dropdown-togglespan#login_widget > .button:hover,
.open > .dropdown-toggle#logout:hover,
span#login_widget > .button:active:focus,
#logout:active:focus,
span#login_widget > .button.active:focus,
#logout.active:focus,
.open > .dropdown-togglespan#login_widget > .button:focus,
.open > .dropdown-toggle#logout:focus,
span#login_widget > .button:active.focus,
#logout:active.focus,
span#login_widget > .button.active.focus,
#logout.active.focus,
.open > .dropdown-togglespan#login_widget > .button.focus,
.open > .dropdown-toggle#logout.focus {
  color: #333;
  background-color: #d4d4d4;
  border-color: #8c8c8c;
}
span#login_widget > .button:active,
#logout:active,
span#login_widget > .button.active,
#logout.active,
.open > .dropdown-togglespan#login_widget > .button,
.open > .dropdown-toggle#logout {
  background-image: none;
}
span#login_widget > .button.disabled:hover,
#logout.disabled:hover,
span#login_widget > .button[disabled]:hover,
#logout[disabled]:hover,
fieldset[disabled] span#login_widget > .button:hover,
fieldset[disabled] #logout:hover,
span#login_widget > .button.disabled:focus,
#logout.disabled:focus,
span#login_widget > .button[disabled]:focus,
#logout[disabled]:focus,
fieldset[disabled] span#login_widget > .button:focus,
fieldset[disabled] #logout:focus,
span#login_widget > .button.disabled.focus,
#logout.disabled.focus,
span#login_widget > .button[disabled].focus,
#logout[disabled].focus,
fieldset[disabled] span#login_widget > .button.focus,
fieldset[disabled] #logout.focus {
  background-color: #fff;
  border-color: #ccc;
}
span#login_widget > .button .badge,
#logout .badge {
  color: #fff;
  background-color: #333;
}
.nav-header {
  text-transform: none;
}
#header > span {
  margin-top: 10px;
}
.modal_stretch .modal-dialog {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  min-height: 80vh;
}
.modal_stretch .modal-dialog .modal-body {
  max-height: calc(100vh - 200px);
  overflow: auto;
  flex: 1;
}
.modal-header {
  cursor: move;
}
@media (min-width: 768px) {
  .modal .modal-dialog {
    width: 700px;
  }
}
@media (min-width: 768px) {
  select.form-control {
    margin-left: 12px;
    margin-right: 12px;
  }
}
/*!
*
* IPython auth
*
*/
.center-nav {
  display: inline-block;
  margin-bottom: -4px;
}
[dir="rtl"] .center-nav form.pull-left {
  float: right !important;
  float: right;
}
[dir="rtl"] .center-nav .navbar-text {
  float: right;
}
[dir="rtl"] .navbar-inner {
  text-align: right;
}
[dir="rtl"] div.text-left {
  text-align: right;
}
/*!
*
* IPython tree view
*
*/
/* We need an invisible input field on top of the sentense*/
/* "Drag file onto the list ..." */
.alternate_upload {
  background-color: none;
  display: inline;
}
.alternate_upload.form {
  padding: 0;
  margin: 0;
}
.alternate_upload input.fileinput {
  position: absolute;
  display: block;
  width: 100%;
  height: 100%;
  overflow: hidden;
  cursor: pointer;
  opacity: 0;
  z-index: 2;
}
.alternate_upload .btn-xs > input.fileinput {
  margin: -1px -5px;
}
.alternate_upload .btn-upload {
  position: relative;
  height: 22px;
}
::-webkit-file-upload-button {
  cursor: pointer;
}
/**
 * Primary styles
 *
 * Author: Jupyter Development Team
 */
ul#tabs {
  margin-bottom: 4px;
}
ul#tabs a {
  padding-top: 6px;
  padding-bottom: 4px;
}
[dir="rtl"] ul#tabs.nav-tabs > li {
  float: right;
}
[dir="rtl"] ul#tabs.nav.nav-tabs {
  padding-right: 0;
}
ul.breadcrumb a:focus,
ul.breadcrumb a:hover {
  text-decoration: none;
}
ul.breadcrumb i.icon-home {
  font-size: 16px;
  margin-right: 4px;
}
ul.breadcrumb span {
  color: #5e5e5e;
}
.list_toolbar {
  padding: 4px 0 4px 0;
  vertical-align: middle;
}
.list_toolbar .tree-buttons {
  padding-top: 1px;
}
[dir="rtl"] .list_toolbar .tree-buttons .pull-right {
  float: left !important;
  float: left;
}
[dir="rtl"] .list_toolbar .col-sm-4,
[dir="rtl"] .list_toolbar .col-sm-8 {
  float: right;
}
.dynamic-buttons {
  padding-top: 3px;
  display: inline-block;
}
.list_toolbar [class*="span"] {
  min-height: 24px;
}
.list_header {
  font-weight: bold;
  background-color: #EEE;
}
.list_placeholder {
  font-weight: bold;
  padding-top: 4px;
  padding-bottom: 4px;
  padding-left: 7px;
  padding-right: 7px;
}
.list_container {
  margin-top: 4px;
  margin-bottom: 20px;
  border: 1px solid #ddd;
  border-radius: 2px;
}
.list_container > div {
  border-bottom: 1px solid #ddd;
}
.list_container > div:hover .list-item {
  background-color: red;
}
.list_container > div:last-child {
  border: none;
}
.list_item:hover .list_item {
  background-color: #ddd;
}
.list_item a {
  text-decoration: none;
}
.list_item:hover {
  background-color: #fafafa;
}
.list_header > div,
.list_item > div {
  padding-top: 4px;
  padding-bottom: 4px;
  padding-left: 7px;
  padding-right: 7px;
  line-height: 22px;
}
.list_header > div input,
.list_item > div input {
  margin-right: 7px;
  margin-left: 14px;
  vertical-align: text-bottom;
  line-height: 22px;
  position: relative;
  top: -1px;
}
.list_header > div .item_link,
.list_item > div .item_link {
  margin-left: -1px;
  vertical-align: baseline;
  line-height: 22px;
}
[dir="rtl"] .list_item > div input {
  margin-right: 0;
}
.new-file input[type=checkbox] {
  visibility: hidden;
}
.item_name {
  line-height: 22px;
  height: 24px;
}
.item_icon {
  font-size: 14px;
  color: #5e5e5e;
  margin-right: 7px;
  margin-left: 7px;
  line-height: 22px;
  vertical-align: baseline;
}
.item_modified {
  margin-right: 7px;
  margin-left: 7px;
}
[dir="rtl"] .item_modified.pull-right {
  float: left !important;
  float: left;
}
.item_buttons {
  line-height: 1em;
  margin-left: -5px;
}
.item_buttons .btn,
.item_buttons .btn-group,
.item_buttons .input-group {
  float: left;
}
.item_buttons > .btn,
.item_buttons > .btn-group,
.item_buttons > .input-group {
  margin-left: 5px;
}
.item_buttons .btn {
  min-width: 13ex;
}
.item_buttons .running-indicator {
  padding-top: 4px;
  color: #5cb85c;
}
.item_buttons .kernel-name {
  padding-top: 4px;
  color: #5bc0de;
  margin-right: 7px;
  float: left;
}
[dir="rtl"] .item_buttons.pull-right {
  float: left !important;
  float: left;
}
[dir="rtl"] .item_buttons .kernel-name {
  margin-left: 7px;
  float: right;
}
.toolbar_info {
  height: 24px;
  line-height: 24px;
}
.list_item input:not([type=checkbox]) {
  padding-top: 3px;
  padding-bottom: 3px;
  height: 22px;
  line-height: 14px;
  margin: 0px;
}
.highlight_text {
  color: blue;
}
#project_name {
  display: inline-block;
  padding-left: 7px;
  margin-left: -2px;
}
#project_name > .breadcrumb {
  padding: 0px;
  margin-bottom: 0px;
  background-color: transparent;
  font-weight: bold;
}
.sort_button {
  display: inline-block;
  padding-left: 7px;
}
[dir="rtl"] .sort_button.pull-right {
  float: left !important;
  float: left;
}
#tree-selector {
  padding-right: 0px;
}
#button-select-all {
  min-width: 50px;
}
[dir="rtl"] #button-select-all.btn {
  float: right ;
}
#select-all {
  margin-left: 7px;
  margin-right: 2px;
  margin-top: 2px;
  height: 16px;
}
[dir="rtl"] #select-all.pull-left {
  float: right !important;
  float: right;
}
.menu_icon {
  margin-right: 2px;
}
.tab-content .row {
  margin-left: 0px;
  margin-right: 0px;
}
.folder_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f114";
}
.folder_icon:before.fa-pull-left {
  margin-right: .3em;
}
.folder_icon:before.fa-pull-right {
  margin-left: .3em;
}
.folder_icon:before.pull-left {
  margin-right: .3em;
}
.folder_icon:before.pull-right {
  margin-left: .3em;
}
.notebook_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f02d";
  position: relative;
  top: -1px;
}
.notebook_icon:before.fa-pull-left {
  margin-right: .3em;
}
.notebook_icon:before.fa-pull-right {
  margin-left: .3em;
}
.notebook_icon:before.pull-left {
  margin-right: .3em;
}
.notebook_icon:before.pull-right {
  margin-left: .3em;
}
.running_notebook_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f02d";
  position: relative;
  top: -1px;
  color: #5cb85c;
}
.running_notebook_icon:before.fa-pull-left {
  margin-right: .3em;
}
.running_notebook_icon:before.fa-pull-right {
  margin-left: .3em;
}
.running_notebook_icon:before.pull-left {
  margin-right: .3em;
}
.running_notebook_icon:before.pull-right {
  margin-left: .3em;
}
.file_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f016";
  position: relative;
  top: -2px;
}
.file_icon:before.fa-pull-left {
  margin-right: .3em;
}
.file_icon:before.fa-pull-right {
  margin-left: .3em;
}
.file_icon:before.pull-left {
  margin-right: .3em;
}
.file_icon:before.pull-right {
  margin-left: .3em;
}
#notebook_toolbar .pull-right {
  padding-top: 0px;
  margin-right: -1px;
}
ul#new-menu {
  left: auto;
  right: 0;
}
#new-menu .dropdown-header {
  font-size: 10px;
  border-bottom: 1px solid #e5e5e5;
  padding: 0 0 3px;
  margin: -3px 20px 0;
}
.kernel-menu-icon {
  padding-right: 12px;
  width: 24px;
  content: "\f096";
}
.kernel-menu-icon:before {
  content: "\f096";
}
.kernel-menu-icon-current:before {
  content: "\f00c";
}
#tab_content {
  padding-top: 20px;
}
#running .panel-group .panel {
  margin-top: 3px;
  margin-bottom: 1em;
}
#running .panel-group .panel .panel-heading {
  background-color: #EEE;
  padding-top: 4px;
  padding-bottom: 4px;
  padding-left: 7px;
  padding-right: 7px;
  line-height: 22px;
}
#running .panel-group .panel .panel-heading a:focus,
#running .panel-group .panel .panel-heading a:hover {
  text-decoration: none;
}
#running .panel-group .panel .panel-body {
  padding: 0px;
}
#running .panel-group .panel .panel-body .list_container {
  margin-top: 0px;
  margin-bottom: 0px;
  border: 0px;
  border-radius: 0px;
}
#running .panel-group .panel .panel-body .list_container .list_item {
  border-bottom: 1px solid #ddd;
}
#running .panel-group .panel .panel-body .list_container .list_item:last-child {
  border-bottom: 0px;
}
.delete-button {
  display: none;
}
.duplicate-button {
  display: none;
}
.rename-button {
  display: none;
}
.move-button {
  display: none;
}
.download-button {
  display: none;
}
.shutdown-button {
  display: none;
}
.dynamic-instructions {
  display: inline-block;
  padding-top: 4px;
}
/*!
*
* IPython text editor webapp
*
*/
.selected-keymap i.fa {
  padding: 0px 5px;
}
.selected-keymap i.fa:before {
  content: "\f00c";
}
#mode-menu {
  overflow: auto;
  max-height: 20em;
}
.edit_app #header {
  -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
  box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
}
.edit_app #menubar .navbar {
  /* Use a negative 1 bottom margin, so the border overlaps the border of the
    header */
  margin-bottom: -1px;
}
.dirty-indicator {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  width: 20px;
}
.dirty-indicator.fa-pull-left {
  margin-right: .3em;
}
.dirty-indicator.fa-pull-right {
  margin-left: .3em;
}
.dirty-indicator.pull-left {
  margin-right: .3em;
}
.dirty-indicator.pull-right {
  margin-left: .3em;
}
.dirty-indicator-dirty {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  width: 20px;
}
.dirty-indicator-dirty.fa-pull-left {
  margin-right: .3em;
}
.dirty-indicator-dirty.fa-pull-right {
  margin-left: .3em;
}
.dirty-indicator-dirty.pull-left {
  margin-right: .3em;
}
.dirty-indicator-dirty.pull-right {
  margin-left: .3em;
}
.dirty-indicator-clean {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  width: 20px;
}
.dirty-indicator-clean.fa-pull-left {
  margin-right: .3em;
}
.dirty-indicator-clean.fa-pull-right {
  margin-left: .3em;
}
.dirty-indicator-clean.pull-left {
  margin-right: .3em;
}
.dirty-indicator-clean.pull-right {
  margin-left: .3em;
}
.dirty-indicator-clean:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f00c";
}
.dirty-indicator-clean:before.fa-pull-left {
  margin-right: .3em;
}
.dirty-indicator-clean:before.fa-pull-right {
  margin-left: .3em;
}
.dirty-indicator-clean:before.pull-left {
  margin-right: .3em;
}
.dirty-indicator-clean:before.pull-right {
  margin-left: .3em;
}
#filename {
  font-size: 16pt;
  display: table;
  padding: 0px 5px;
}
#current-mode {
  padding-left: 5px;
  padding-right: 5px;
}
#texteditor-backdrop {
  padding-top: 20px;
  padding-bottom: 20px;
}
@media not print {
  #texteditor-backdrop {
    background-color: #EEE;
  }
}
@media print {
  #texteditor-backdrop #texteditor-container .CodeMirror-gutter,
  #texteditor-backdrop #texteditor-container .CodeMirror-gutters {
    background-color: #fff;
  }
}
@media not print {
  #texteditor-backdrop #texteditor-container .CodeMirror-gutter,
  #texteditor-backdrop #texteditor-container .CodeMirror-gutters {
    background-color: #fff;
  }
}
@media not print {
  #texteditor-backdrop #texteditor-container {
    padding: 0px;
    background-color: #fff;
    -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
    box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
  }
}
.CodeMirror-dialog {
  background-color: #fff;
}
/*!
*
* IPython notebook
*
*/
/* CSS font colors for translated ANSI escape sequences */
/* The color values are a mix of
   http://www.xcolors.net/dl/baskerville-ivorylight and
   http://www.xcolors.net/dl/euphrasia */
.ansi-black-fg {
  color: #3E424D;
}
.ansi-black-bg {
  background-color: #3E424D;
}
.ansi-black-intense-fg {
  color: #282C36;
}
.ansi-black-intense-bg {
  background-color: #282C36;
}
.ansi-red-fg {
  color: #E75C58;
}
.ansi-red-bg {
  background-color: #E75C58;
}
.ansi-red-intense-fg {
  color: #B22B31;
}
.ansi-red-intense-bg {
  background-color: #B22B31;
}
.ansi-green-fg {
  color: #00A250;
}
.ansi-green-bg {
  background-color: #00A250;
}
.ansi-green-intense-fg {
  color: #007427;
}
.ansi-green-intense-bg {
  background-color: #007427;
}
.ansi-yellow-fg {
  color: #DDB62B;
}
.ansi-yellow-bg {
  background-color: #DDB62B;
}
.ansi-yellow-intense-fg {
  color: #B27D12;
}
.ansi-yellow-intense-bg {
  background-color: #B27D12;
}
.ansi-blue-fg {
  color: #208FFB;
}
.ansi-blue-bg {
  background-color: #208FFB;
}
.ansi-blue-intense-fg {
  color: #0065CA;
}
.ansi-blue-intense-bg {
  background-color: #0065CA;
}
.ansi-magenta-fg {
  color: #D160C4;
}
.ansi-magenta-bg {
  background-color: #D160C4;
}
.ansi-magenta-intense-fg {
  color: #A03196;
}
.ansi-magenta-intense-bg {
  background-color: #A03196;
}
.ansi-cyan-fg {
  color: #60C6C8;
}
.ansi-cyan-bg {
  background-color: #60C6C8;
}
.ansi-cyan-intense-fg {
  color: #258F8F;
}
.ansi-cyan-intense-bg {
  background-color: #258F8F;
}
.ansi-white-fg {
  color: #C5C1B4;
}
.ansi-white-bg {
  background-color: #C5C1B4;
}
.ansi-white-intense-fg {
  color: #A1A6B2;
}
.ansi-white-intense-bg {
  background-color: #A1A6B2;
}
.ansi-default-inverse-fg {
  color: #FFFFFF;
}
.ansi-default-inverse-bg {
  background-color: #000000;
}
.ansi-bold {
  font-weight: bold;
}
.ansi-underline {
  text-decoration: underline;
}
/* The following styles are deprecated an will be removed in a future version */
.ansibold {
  font-weight: bold;
}
.ansi-inverse {
  outline: 0.5px dotted;
}
/* use dark versions for foreground, to improve visibility */
.ansiblack {
  color: black;
}
.ansired {
  color: darkred;
}
.ansigreen {
  color: darkgreen;
}
.ansiyellow {
  color: #c4a000;
}
.ansiblue {
  color: darkblue;
}
.ansipurple {
  color: darkviolet;
}
.ansicyan {
  color: steelblue;
}
.ansigray {
  color: gray;
}
/* and light for background, for the same reason */
.ansibgblack {
  background-color: black;
}
.ansibgred {
  background-color: red;
}
.ansibggreen {
  background-color: green;
}
.ansibgyellow {
  background-color: yellow;
}
.ansibgblue {
  background-color: blue;
}
.ansibgpurple {
  background-color: magenta;
}
.ansibgcyan {
  background-color: cyan;
}
.ansibggray {
  background-color: gray;
}
div.cell {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  border-radius: 2px;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
  border-width: 1px;
  border-style: solid;
  border-color: transparent;
  width: 100%;
  padding: 5px;
  /* This acts as a spacer between cells, that is outside the border */
  margin: 0px;
  outline: none;
  position: relative;
  overflow: visible;
}
div.cell:before {
  position: absolute;
  display: block;
  top: -1px;
  left: -1px;
  width: 5px;
  height: calc(100% +  2px);
  content: '';
  background: transparent;
}
div.cell.jupyter-soft-selected {
  border-left-color: #E3F2FD;
  border-left-width: 1px;
  padding-left: 5px;
  border-right-color: #E3F2FD;
  border-right-width: 1px;
  background: #E3F2FD;
}
@media print {
  div.cell.jupyter-soft-selected {
    border-color: transparent;
  }
}
div.cell.selected,
div.cell.selected.jupyter-soft-selected {
  border-color: #ababab;
}
div.cell.selected:before,
div.cell.selected.jupyter-soft-selected:before {
  position: absolute;
  display: block;
  top: -1px;
  left: -1px;
  width: 5px;
  height: calc(100% +  2px);
  content: '';
  background: #42A5F5;
}
@media print {
  div.cell.selected,
  div.cell.selected.jupyter-soft-selected {
    border-color: transparent;
  }
}
.edit_mode div.cell.selected {
  border-color: #66BB6A;
}
.edit_mode div.cell.selected:before {
  position: absolute;
  display: block;
  top: -1px;
  left: -1px;
  width: 5px;
  height: calc(100% +  2px);
  content: '';
  background: #66BB6A;
}
@media print {
  .edit_mode div.cell.selected {
    border-color: transparent;
  }
}
.prompt {
  /* This needs to be wide enough for 3 digit prompt numbers: In[100]: */
  min-width: 14ex;
  /* This padding is tuned to match the padding on the CodeMirror editor. */
  padding: 0.4em;
  margin: 0px;
  font-family: monospace;
  text-align: right;
  /* This has to match that of the the CodeMirror class line-height below */
  line-height: 1.21429em;
  /* Don't highlight prompt number selection */
  -webkit-touch-callout: none;
  -webkit-user-select: none;
  -khtml-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
  /* Use default cursor */
  cursor: default;
}
@media (max-width: 540px) {
  .prompt {
    text-align: left;
  }
}
div.inner_cell {
  min-width: 0;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
}
/* input_area and input_prompt must match in top border and margin for alignment */
div.input_area {
  border: 1px solid #cfcfcf;
  border-radius: 2px;
  background: #f7f7f7;
  line-height: 1.21429em;
}
/* This is needed so that empty prompt areas can collapse to zero height when there
   is no content in the output_subarea and the prompt. The main purpose of this is
   to make sure that empty JavaScript output_subareas have no height. */
div.prompt:empty {
  padding-top: 0;
  padding-bottom: 0;
}
div.unrecognized_cell {
  padding: 5px 5px 5px 0px;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
div.unrecognized_cell .inner_cell {
  border-radius: 2px;
  padding: 5px;
  font-weight: bold;
  color: red;
  border: 1px solid #cfcfcf;
  background: #eaeaea;
}
div.unrecognized_cell .inner_cell a {
  color: inherit;
  text-decoration: none;
}
div.unrecognized_cell .inner_cell a:hover {
  color: inherit;
  text-decoration: none;
}
@media (max-width: 540px) {
  div.unrecognized_cell > div.prompt {
    display: none;
  }
}
div.code_cell {
  /* avoid page breaking on code cells when printing */
}
@media print {
  div.code_cell {
    page-break-inside: avoid;
  }
}
/* any special styling for code cells that are currently running goes here */
div.input {
  page-break-inside: avoid;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
@media (max-width: 540px) {
  div.input {
    /* Old browsers */
    display: -webkit-box;
    -webkit-box-orient: vertical;
    -webkit-box-align: stretch;
    display: -moz-box;
    -moz-box-orient: vertical;
    -moz-box-align: stretch;
    display: box;
    box-orient: vertical;
    box-align: stretch;
    /* Modern browsers */
    display: flex;
    flex-direction: column;
    align-items: stretch;
  }
}
/* input_area and input_prompt must match in top border and margin for alignment */
div.input_prompt {
  color: #303F9F;
  border-top: 1px solid transparent;
}
div.input_area > div.highlight {
  margin: 0.4em;
  border: none;
  padding: 0px;
  background-color: transparent;
}
div.input_area > div.highlight > pre {
  margin: 0px;
  border: none;
  padding: 0px;
  background-color: transparent;
}
/* The following gets added to the <head> if it is detected that the user has a
 * monospace font with inconsistent normal/bold/italic height.  See
 * notebookmain.js.  Such fonts will have keywords vertically offset with
 * respect to the rest of the text.  The user should select a better font.
 * See: https://github.com/ipython/ipython/issues/1503
 *
 * .CodeMirror span {
 *      vertical-align: bottom;
 * }
 */
.CodeMirror {
  line-height: 1.21429em;
  /* Changed from 1em to our global default */
  font-size: 14px;
  height: auto;
  /* Changed to auto to autogrow */
  background: none;
  /* Changed from white to allow our bg to show through */
}
.CodeMirror-scroll {
  /*  The CodeMirror docs are a bit fuzzy on if overflow-y should be hidden or visible.*/
  /*  We have found that if it is visible, vertical scrollbars appear with font size changes.*/
  overflow-y: hidden;
  overflow-x: auto;
}
.CodeMirror-lines {
  /* In CM2, this used to be 0.4em, but in CM3 it went to 4px. We need the em value because */
  /* we have set a different line-height and want this to scale with that. */
  /* Note that this should set vertical padding only, since CodeMirror assumes
       that horizontal padding will be set on CodeMirror pre */
  padding: 0.4em 0;
}
.CodeMirror-linenumber {
  padding: 0 8px 0 4px;
}
.CodeMirror-gutters {
  border-bottom-left-radius: 2px;
  border-top-left-radius: 2px;
}
.CodeMirror pre {
  /* In CM3 this went to 4px from 0 in CM2. This sets horizontal padding only,
    use .CodeMirror-lines for vertical */
  padding: 0 0.4em;
  border: 0;
  border-radius: 0;
}
.CodeMirror-cursor {
  border-left: 1.4px solid black;
}
@media screen and (min-width: 2138px) and (max-width: 4319px) {
  .CodeMirror-cursor {
    border-left: 2px solid black;
  }
}
@media screen and (min-width: 4320px) {
  .CodeMirror-cursor {
    border-left: 4px solid black;
  }
}
/*

Original style from softwaremaniacs.org (c) Ivan Sagalaev <Maniac@SoftwareManiacs.Org>
Adapted from GitHub theme

*/
.highlight-base {
  color: #000;
}
.highlight-variable {
  color: #000;
}
.highlight-variable-2 {
  color: #1a1a1a;
}
.highlight-variable-3 {
  color: #333333;
}
.highlight-string {
  color: #BA2121;
}
.highlight-comment {
  color: #408080;
  font-style: italic;
}
.highlight-number {
  color: #080;
}
.highlight-atom {
  color: #88F;
}
.highlight-keyword {
  color: #008000;
  font-weight: bold;
}
.highlight-builtin {
  color: #008000;
}
.highlight-error {
  color: #f00;
}
.highlight-operator {
  color: #AA22FF;
  font-weight: bold;
}
.highlight-meta {
  color: #AA22FF;
}
/* previously not defined, copying from default codemirror */
.highlight-def {
  color: #00f;
}
.highlight-string-2 {
  color: #f50;
}
.highlight-qualifier {
  color: #555;
}
.highlight-bracket {
  color: #997;
}
.highlight-tag {
  color: #170;
}
.highlight-attribute {
  color: #00c;
}
.highlight-header {
  color: blue;
}
.highlight-quote {
  color: #090;
}
.highlight-link {
  color: #00c;
}
/* apply the same style to codemirror */
.cm-s-ipython span.cm-keyword {
  color: #008000;
  font-weight: bold;
}
.cm-s-ipython span.cm-atom {
  color: #88F;
}
.cm-s-ipython span.cm-number {
  color: #080;
}
.cm-s-ipython span.cm-def {
  color: #00f;
}
.cm-s-ipython span.cm-variable {
  color: #000;
}
.cm-s-ipython span.cm-operator {
  color: #AA22FF;
  font-weight: bold;
}
.cm-s-ipython span.cm-variable-2 {
  color: #1a1a1a;
}
.cm-s-ipython span.cm-variable-3 {
  color: #333333;
}
.cm-s-ipython span.cm-comment {
  color: #408080;
  font-style: italic;
}
.cm-s-ipython span.cm-string {
  color: #BA2121;
}
.cm-s-ipython span.cm-string-2 {
  color: #f50;
}
.cm-s-ipython span.cm-meta {
  color: #AA22FF;
}
.cm-s-ipython span.cm-qualifier {
  color: #555;
}
.cm-s-ipython span.cm-builtin {
  color: #008000;
}
.cm-s-ipython span.cm-bracket {
  color: #997;
}
.cm-s-ipython span.cm-tag {
  color: #170;
}
.cm-s-ipython span.cm-attribute {
  color: #00c;
}
.cm-s-ipython span.cm-header {
  color: blue;
}
.cm-s-ipython span.cm-quote {
  color: #090;
}
.cm-s-ipython span.cm-link {
  color: #00c;
}
.cm-s-ipython span.cm-error {
  color: #f00;
}
.cm-s-ipython span.cm-tab {
  background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=);
  background-position: right;
  background-repeat: no-repeat;
}
div.output_wrapper {
  /* this position must be relative to enable descendents to be absolute within it */
  position: relative;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  z-index: 1;
}
/* class for the output area when it should be height-limited */
div.output_scroll {
  /* ideally, this would be max-height, but FF barfs all over that */
  height: 24em;
  /* FF needs this *and the wrapper* to specify full width, or it will shrinkwrap */
  width: 100%;
  overflow: auto;
  border-radius: 2px;
  -webkit-box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8);
  box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8);
  display: block;
}
/* output div while it is collapsed */
div.output_collapsed {
  margin: 0px;
  padding: 0px;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
}
div.out_prompt_overlay {
  height: 100%;
  padding: 0px 0.4em;
  position: absolute;
  border-radius: 2px;
}
div.out_prompt_overlay:hover {
  /* use inner shadow to get border that is computed the same on WebKit/FF */
  -webkit-box-shadow: inset 0 0 1px #000;
  box-shadow: inset 0 0 1px #000;
  background: rgba(240, 240, 240, 0.5);
}
div.output_prompt {
  color: #D84315;
}
/* This class is the outer container of all output sections. */
div.output_area {
  padding: 0px;
  page-break-inside: avoid;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
div.output_area .MathJax_Display {
  text-align: left !important;
}
div.output_area .rendered_html table {
  margin-left: 0;
  margin-right: 0;
}
div.output_area .rendered_html img {
  margin-left: 0;
  margin-right: 0;
}
div.output_area img,
div.output_area svg {
  max-width: 100%;
  height: auto;
}
div.output_area img.unconfined,
div.output_area svg.unconfined {
  max-width: none;
}
div.output_area .mglyph > img {
  max-width: none;
}
/* This is needed to protect the pre formating from global settings such
   as that of bootstrap */
.output {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
}
@media (max-width: 540px) {
  div.output_area {
    /* Old browsers */
    display: -webkit-box;
    -webkit-box-orient: vertical;
    -webkit-box-align: stretch;
    display: -moz-box;
    -moz-box-orient: vertical;
    -moz-box-align: stretch;
    display: box;
    box-orient: vertical;
    box-align: stretch;
    /* Modern browsers */
    display: flex;
    flex-direction: column;
    align-items: stretch;
  }
}
div.output_area pre {
  margin: 0;
  padding: 1px 0 1px 0;
  border: 0;
  vertical-align: baseline;
  color: black;
  background-color: transparent;
  border-radius: 0;
}
/* This class is for the output subarea inside the output_area and after
   the prompt div. */
div.output_subarea {
  overflow-x: auto;
  padding: 0.4em;
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
  max-width: calc(100% - 14ex);
}
div.output_scroll div.output_subarea {
  overflow-x: visible;
}
/* The rest of the output_* classes are for special styling of the different
   output types */
/* all text output has this class: */
div.output_text {
  text-align: left;
  color: #000;
  /* This has to match that of the the CodeMirror class line-height below */
  line-height: 1.21429em;
}
/* stdout/stderr are 'text' as well as 'stream', but execute_result/error are *not* streams */
div.output_stderr {
  background: #fdd;
  /* very light red background for stderr */
}
div.output_latex {
  text-align: left;
}
/* Empty output_javascript divs should have no height */
div.output_javascript:empty {
  padding: 0;
}
.js-error {
  color: darkred;
}
/* raw_input styles */
div.raw_input_container {
  line-height: 1.21429em;
  padding-top: 5px;
}
pre.raw_input_prompt {
  /* nothing needed here. */
}
input.raw_input {
  font-family: monospace;
  font-size: inherit;
  color: inherit;
  width: auto;
  /* make sure input baseline aligns with prompt */
  vertical-align: baseline;
  /* padding + margin = 0.5em between prompt and cursor */
  padding: 0em 0.25em;
  margin: 0em 0.25em;
}
input.raw_input:focus {
  box-shadow: none;
}
p.p-space {
  margin-bottom: 10px;
}
div.output_unrecognized {
  padding: 5px;
  font-weight: bold;
  color: red;
}
div.output_unrecognized a {
  color: inherit;
  text-decoration: none;
}
div.output_unrecognized a:hover {
  color: inherit;
  text-decoration: none;
}
.rendered_html {
  color: #000;
  /* any extras will just be numbers: */
}
.rendered_html em {
  font-style: italic;
}
.rendered_html strong {
  font-weight: bold;
}
.rendered_html u {
  text-decoration: underline;
}
.rendered_html :link {
  text-decoration: underline;
}
.rendered_html :visited {
  text-decoration: underline;
}
.rendered_html h1 {
  font-size: 185.7%;
  margin: 1.08em 0 0 0;
  font-weight: bold;
  line-height: 1.0;
}
.rendered_html h2 {
  font-size: 157.1%;
  margin: 1.27em 0 0 0;
  font-weight: bold;
  line-height: 1.0;
}
.rendered_html h3 {
  font-size: 128.6%;
  margin: 1.55em 0 0 0;
  font-weight: bold;
  line-height: 1.0;
}
.rendered_html h4 {
  font-size: 100%;
  margin: 2em 0 0 0;
  font-weight: bold;
  line-height: 1.0;
}
.rendered_html h5 {
  font-size: 100%;
  margin: 2em 0 0 0;
  font-weight: bold;
  line-height: 1.0;
  font-style: italic;
}
.rendered_html h6 {
  font-size: 100%;
  margin: 2em 0 0 0;
  font-weight: bold;
  line-height: 1.0;
  font-style: italic;
}
.rendered_html h1:first-child {
  margin-top: 0.538em;
}
.rendered_html h2:first-child {
  margin-top: 0.636em;
}
.rendered_html h3:first-child {
  margin-top: 0.777em;
}
.rendered_html h4:first-child {
  margin-top: 1em;
}
.rendered_html h5:first-child {
  margin-top: 1em;
}
.rendered_html h6:first-child {
  margin-top: 1em;
}
.rendered_html ul:not(.list-inline),
.rendered_html ol:not(.list-inline) {
  padding-left: 2em;
}
.rendered_html ul {
  list-style: disc;
}
.rendered_html ul ul {
  list-style: square;
  margin-top: 0;
}
.rendered_html ul ul ul {
  list-style: circle;
}
.rendered_html ol {
  list-style: decimal;
}
.rendered_html ol ol {
  list-style: upper-alpha;
  margin-top: 0;
}
.rendered_html ol ol ol {
  list-style: lower-alpha;
}
.rendered_html ol ol ol ol {
  list-style: lower-roman;
}
.rendered_html ol ol ol ol ol {
  list-style: decimal;
}
.rendered_html * + ul {
  margin-top: 1em;
}
.rendered_html * + ol {
  margin-top: 1em;
}
.rendered_html hr {
  color: black;
  background-color: black;
}
.rendered_html pre {
  margin: 1em 2em;
  padding: 0px;
  background-color: #fff;
}
.rendered_html code {
  background-color: #eff0f1;
}
.rendered_html p code {
  padding: 1px 5px;
}
.rendered_html pre code {
  background-color: #fff;
}
.rendered_html pre,
.rendered_html code {
  border: 0;
  color: #000;
  font-size: 100%;
}
.rendered_html blockquote {
  margin: 1em 2em;
}
.rendered_html table {
  margin-left: auto;
  margin-right: auto;
  border: none;
  border-collapse: collapse;
  border-spacing: 0;
  color: black;
  font-size: 12px;
  table-layout: fixed;
}
.rendered_html thead {
  border-bottom: 1px solid black;
  vertical-align: bottom;
}
.rendered_html tr,
.rendered_html th,
.rendered_html td {
  text-align: right;
  vertical-align: middle;
  padding: 0.5em 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}
.rendered_html th {
  font-weight: bold;
}
.rendered_html tbody tr:nth-child(odd) {
  background: #f5f5f5;
}
.rendered_html tbody tr:hover {
  background: rgba(66, 165, 245, 0.2);
}
.rendered_html * + table {
  margin-top: 1em;
}
.rendered_html p {
  text-align: left;
}
.rendered_html * + p {
  margin-top: 1em;
}
.rendered_html img {
  display: block;
  margin-left: auto;
  margin-right: auto;
}
.rendered_html * + img {
  margin-top: 1em;
}
.rendered_html img,
.rendered_html svg {
  max-width: 100%;
  height: auto;
}
.rendered_html img.unconfined,
.rendered_html svg.unconfined {
  max-width: none;
}
.rendered_html .alert {
  margin-bottom: initial;
}
.rendered_html * + .alert {
  margin-top: 1em;
}
[dir="rtl"] .rendered_html p {
  text-align: right;
}
div.text_cell {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
@media (max-width: 540px) {
  div.text_cell > div.prompt {
    display: none;
  }
}
div.text_cell_render {
  /*font-family: "Helvetica Neue", Arial, Helvetica, Geneva, sans-serif;*/
  outline: none;
  resize: none;
  width: inherit;
  border-style: none;
  padding: 0.5em 0.5em 0.5em 0.4em;
  color: #000;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
}
a.anchor-link:link {
  text-decoration: none;
  padding: 0px 20px;
  visibility: hidden;
}
h1:hover .anchor-link,
h2:hover .anchor-link,
h3:hover .anchor-link,
h4:hover .anchor-link,
h5:hover .anchor-link,
h6:hover .anchor-link {
  visibility: visible;
}
.text_cell.rendered .input_area {
  display: none;
}
.text_cell.rendered .rendered_html {
  overflow-x: auto;
  overflow-y: hidden;
}
.text_cell.rendered .rendered_html tr,
.text_cell.rendered .rendered_html th,
.text_cell.rendered .rendered_html td {
  max-width: none;
}
.text_cell.unrendered .text_cell_render {
  display: none;
}
.text_cell .dropzone .input_area {
  border: 2px dashed #bababa;
  margin: -1px;
}
.cm-header-1,
.cm-header-2,
.cm-header-3,
.cm-header-4,
.cm-header-5,
.cm-header-6 {
  font-weight: bold;
  font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;
}
.cm-header-1 {
  font-size: 185.7%;
}
.cm-header-2 {
  font-size: 157.1%;
}
.cm-header-3 {
  font-size: 128.6%;
}
.cm-header-4 {
  font-size: 110%;
}
.cm-header-5 {
  font-size: 100%;
  font-style: italic;
}
.cm-header-6 {
  font-size: 100%;
  font-style: italic;
}
/*!
*
* IPython notebook webapp
*
*/
@media (max-width: 767px) {
  .notebook_app {
    padding-left: 0px;
    padding-right: 0px;
  }
}
#ipython-main-app {
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
  height: 100%;
}
div#notebook_panel {
  margin: 0px;
  padding: 0px;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
  height: 100%;
}
div#notebook {
  font-size: 14px;
  line-height: 20px;
  overflow-y: hidden;
  overflow-x: auto;
  width: 100%;
  /* This spaces the page away from the edge of the notebook area */
  padding-top: 20px;
  margin: 0px;
  outline: none;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
  min-height: 100%;
}
@media not print {
  #notebook-container {
    padding: 15px;
    background-color: #fff;
    min-height: 0;
    -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
    box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
  }
}
@media print {
  #notebook-container {
    width: 100%;
  }
}
div.ui-widget-content {
  border: 1px solid #ababab;
  outline: none;
}
pre.dialog {
  background-color: #f7f7f7;
  border: 1px solid #ddd;
  border-radius: 2px;
  padding: 0.4em;
  padding-left: 2em;
}
p.dialog {
  padding: 0.2em;
}
/* Word-wrap output correctly.  This is the CSS3 spelling, though Firefox seems
   to not honor it correctly.  Webkit browsers (Chrome, rekonq, Safari) do.
 */
pre,
code,
kbd,
samp {
  white-space: pre-wrap;
}
#fonttest {
  font-family: monospace;
}
p {
  margin-bottom: 0;
}
.end_space {
  min-height: 100px;
  transition: height .2s ease;
}
.notebook_app > #header {
  -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
  box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
}
@media not print {
  .notebook_app {
    background-color: #EEE;
  }
}
kbd {
  border-style: solid;
  border-width: 1px;
  box-shadow: none;
  margin: 2px;
  padding-left: 2px;
  padding-right: 2px;
  padding-top: 1px;
  padding-bottom: 1px;
}
.jupyter-keybindings {
  padding: 1px;
  line-height: 24px;
  border-bottom: 1px solid gray;
}
.jupyter-keybindings input {
  margin: 0;
  padding: 0;
  border: none;
}
.jupyter-keybindings i {
  padding: 6px;
}
.well code {
  background-color: #ffffff;
  border-color: #ababab;
  border-width: 1px;
  border-style: solid;
  padding: 2px;
  padding-top: 1px;
  padding-bottom: 1px;
}
/* CSS for the cell toolbar */
.celltoolbar {
  border: thin solid #CFCFCF;
  border-bottom: none;
  background: #EEE;
  border-radius: 2px 2px 0px 0px;
  width: 100%;
  height: 29px;
  padding-right: 4px;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
  /* Old browsers */
  -webkit-box-pack: end;
  -moz-box-pack: end;
  box-pack: end;
  /* Modern browsers */
  justify-content: flex-end;
  display: -webkit-flex;
}
@media print {
  .celltoolbar {
    display: none;
  }
}
.ctb_hideshow {
  display: none;
  vertical-align: bottom;
}
/* ctb_show is added to the ctb_hideshow div to show the cell toolbar.
   Cell toolbars are only shown when the ctb_global_show class is also set.
*/
.ctb_global_show .ctb_show.ctb_hideshow {
  display: block;
}
.ctb_global_show .ctb_show + .input_area,
.ctb_global_show .ctb_show + div.text_cell_input,
.ctb_global_show .ctb_show ~ div.text_cell_render {
  border-top-right-radius: 0px;
  border-top-left-radius: 0px;
}
.ctb_global_show .ctb_show ~ div.text_cell_render {
  border: 1px solid #cfcfcf;
}
.celltoolbar {
  font-size: 87%;
  padding-top: 3px;
}
.celltoolbar select {
  display: block;
  width: 100%;
  height: 32px;
  padding: 6px 12px;
  font-size: 13px;
  line-height: 1.42857143;
  color: #555555;
  background-color: #fff;
  background-image: none;
  border: 1px solid #ccc;
  border-radius: 2px;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  -webkit-transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
  -o-transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
  transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
  height: 30px;
  padding: 5px 10px;
  font-size: 12px;
  line-height: 1.5;
  border-radius: 1px;
  width: inherit;
  font-size: inherit;
  height: 22px;
  padding: 0px;
  display: inline-block;
}
.celltoolbar select:focus {
  border-color: #66afe9;
  outline: 0;
  -webkit-box-shadow: inset 0 1px 1px rgba(0,0,0,.075), 0 0 8px rgba(102, 175, 233, 0.6);
  box-shadow: inset 0 1px 1px rgba(0,0,0,.075), 0 0 8px rgba(102, 175, 233, 0.6);
}
.celltoolbar select::-moz-placeholder {
  color: #999;
  opacity: 1;
}
.celltoolbar select:-ms-input-placeholder {
  color: #999;
}
.celltoolbar select::-webkit-input-placeholder {
  color: #999;
}
.celltoolbar select::-ms-expand {
  border: 0;
  background-color: transparent;
}
.celltoolbar select[disabled],
.celltoolbar select[readonly],
fieldset[disabled] .celltoolbar select {
  background-color: #eeeeee;
  opacity: 1;
}
.celltoolbar select[disabled],
fieldset[disabled] .celltoolbar select {
  cursor: not-allowed;
}
textarea.celltoolbar select {
  height: auto;
}
select.celltoolbar select {
  height: 30px;
  line-height: 30px;
}
textarea.celltoolbar select,
select[multiple].celltoolbar select {
  height: auto;
}
.celltoolbar label {
  margin-left: 5px;
  margin-right: 5px;
}
.tags_button_container {
  width: 100%;
  display: flex;
}
.tag-container {
  display: flex;
  flex-direction: row;
  flex-grow: 1;
  overflow: hidden;
  position: relative;
}
.tag-container > * {
  margin: 0 4px;
}
.remove-tag-btn {
  margin-left: 4px;
}
.tags-input {
  display: flex;
}
.cell-tag:last-child:after {
  content: "";
  position: absolute;
  right: 0;
  width: 40px;
  height: 100%;
  /* Fade to background color of cell toolbar */
  background: linear-gradient(to right, rgba(0, 0, 0, 0), #EEE);
}
.tags-input > * {
  margin-left: 4px;
}
.cell-tag,
.tags-input input,
.tags-input button {
  display: block;
  width: 100%;
  height: 32px;
  padding: 6px 12px;
  font-size: 13px;
  line-height: 1.42857143;
  color: #555555;
  background-color: #fff;
  background-image: none;
  border: 1px solid #ccc;
  border-radius: 2px;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  -webkit-transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
  -o-transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
  transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
  height: 30px;
  padding: 5px 10px;
  font-size: 12px;
  line-height: 1.5;
  border-radius: 1px;
  box-shadow: none;
  width: inherit;
  font-size: inherit;
  height: 22px;
  line-height: 22px;
  padding: 0px 4px;
  display: inline-block;
}
.cell-tag:focus,
.tags-input input:focus,
.tags-input button:focus {
  border-color: #66afe9;
  outline: 0;
  -webkit-box-shadow: inset 0 1px 1px rgba(0,0,0,.075), 0 0 8px rgba(102, 175, 233, 0.6);
  box-shadow: inset 0 1px 1px rgba(0,0,0,.075), 0 0 8px rgba(102, 175, 233, 0.6);
}
.cell-tag::-moz-placeholder,
.tags-input input::-moz-placeholder,
.tags-input button::-moz-placeholder {
  color: #999;
  opacity: 1;
}
.cell-tag:-ms-input-placeholder,
.tags-input input:-ms-input-placeholder,
.tags-input button:-ms-input-placeholder {
  color: #999;
}
.cell-tag::-webkit-input-placeholder,
.tags-input input::-webkit-input-placeholder,
.tags-input button::-webkit-input-placeholder {
  color: #999;
}
.cell-tag::-ms-expand,
.tags-input input::-ms-expand,
.tags-input button::-ms-expand {
  border: 0;
  background-color: transparent;
}
.cell-tag[disabled],
.tags-input input[disabled],
.tags-input button[disabled],
.cell-tag[readonly],
.tags-input input[readonly],
.tags-input button[readonly],
fieldset[disabled] .cell-tag,
fieldset[disabled] .tags-input input,
fieldset[disabled] .tags-input button {
  background-color: #eeeeee;
  opacity: 1;
}
.cell-tag[disabled],
.tags-input input[disabled],
.tags-input button[disabled],
fieldset[disabled] .cell-tag,
fieldset[disabled] .tags-input input,
fieldset[disabled] .tags-input button {
  cursor: not-allowed;
}
textarea.cell-tag,
textarea.tags-input input,
textarea.tags-input button {
  height: auto;
}
select.cell-tag,
select.tags-input input,
select.tags-input button {
  height: 30px;
  line-height: 30px;
}
textarea.cell-tag,
textarea.tags-input input,
textarea.tags-input button,
select[multiple].cell-tag,
select[multiple].tags-input input,
select[multiple].tags-input button {
  height: auto;
}
.cell-tag,
.tags-input button {
  padding: 0px 4px;
}
.cell-tag {
  background-color: #fff;
  white-space: nowrap;
}
.tags-input input[type=text]:focus {
  outline: none;
  box-shadow: none;
  border-color: #ccc;
}
.completions {
  position: absolute;
  z-index: 110;
  overflow: hidden;
  border: 1px solid #ababab;
  border-radius: 2px;
  -webkit-box-shadow: 0px 6px 10px -1px #adadad;
  box-shadow: 0px 6px 10px -1px #adadad;
  line-height: 1;
}
.completions select {
  background: white;
  outline: none;
  border: none;
  padding: 0px;
  margin: 0px;
  overflow: auto;
  font-family: monospace;
  font-size: 110%;
  color: #000;
  width: auto;
}
.completions select option.context {
  color: #286090;
}
#kernel_logo_widget .current_kernel_logo {
  display: none;
  margin-top: -1px;
  margin-bottom: -1px;
  width: 32px;
  height: 32px;
}
[dir="rtl"] #kernel_logo_widget {
  float: left !important;
  float: left;
}
.modal .modal-body .move-path {
  display: flex;
  flex-direction: row;
  justify-content: space;
  align-items: center;
}
.modal .modal-body .move-path .server-root {
  padding-right: 20px;
}
.modal .modal-body .move-path .path-input {
  flex: 1;
}
#menubar {
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
  margin-top: 1px;
}
#menubar .navbar {
  border-top: 1px;
  border-radius: 0px 0px 2px 2px;
  margin-bottom: 0px;
}
#menubar .navbar-toggle {
  float: left;
  padding-top: 7px;
  padding-bottom: 7px;
  border: none;
}
#menubar .navbar-collapse {
  clear: left;
}
[dir="rtl"] #menubar .navbar-toggle {
  float: right;
}
[dir="rtl"] #menubar .navbar-collapse {
  clear: right;
}
[dir="rtl"] #menubar .navbar-nav {
  float: right;
}
[dir="rtl"] #menubar .nav {
  padding-right: 0px;
}
[dir="rtl"] #menubar .navbar-nav > li {
  float: right;
}
[dir="rtl"] #menubar .navbar-right {
  float: left !important;
}
[dir="rtl"] ul.dropdown-menu {
  text-align: right;
  left: auto;
}
[dir="rtl"] ul#new-menu.dropdown-menu {
  right: auto;
  left: 0;
}
.nav-wrapper {
  border-bottom: 1px solid #e7e7e7;
}
i.menu-icon {
  padding-top: 4px;
}
[dir="rtl"] i.menu-icon.pull-right {
  float: left !important;
  float: left;
}
ul#help_menu li a {
  overflow: hidden;
  padding-right: 2.2em;
}
ul#help_menu li a i {
  margin-right: -1.2em;
}
[dir="rtl"] ul#help_menu li a {
  padding-left: 2.2em;
}
[dir="rtl"] ul#help_menu li a i {
  margin-right: 0;
  margin-left: -1.2em;
}
[dir="rtl"] ul#help_menu li a i.pull-right {
  float: left !important;
  float: left;
}
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu > .dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
}
[dir="rtl"] .dropdown-submenu > .dropdown-menu {
  right: 100%;
  margin-right: -1px;
}
.dropdown-submenu:hover > .dropdown-menu {
  display: block;
}
.dropdown-submenu > a:after {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  display: block;
  content: "\f0da";
  float: right;
  color: #333333;
  margin-top: 2px;
  margin-right: -10px;
}
.dropdown-submenu > a:after.fa-pull-left {
  margin-right: .3em;
}
.dropdown-submenu > a:after.fa-pull-right {
  margin-left: .3em;
}
.dropdown-submenu > a:after.pull-left {
  margin-right: .3em;
}
.dropdown-submenu > a:after.pull-right {
  margin-left: .3em;
}
[dir="rtl"] .dropdown-submenu > a:after {
  float: left;
  content: "\f0d9";
  margin-right: 0;
  margin-left: -10px;
}
.dropdown-submenu:hover > a:after {
  color: #262626;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left > .dropdown-menu {
  left: -100%;
  margin-left: 10px;
}
#notification_area {
  float: right !important;
  float: right;
  z-index: 10;
}
[dir="rtl"] #notification_area {
  float: left !important;
  float: left;
}
.indicator_area {
  float: right !important;
  float: right;
  color: #777;
  margin-left: 5px;
  margin-right: 5px;
  width: 11px;
  z-index: 10;
  text-align: center;
  width: auto;
}
[dir="rtl"] .indicator_area {
  float: left !important;
  float: left;
}
#kernel_indicator {
  float: right !important;
  float: right;
  color: #777;
  margin-left: 5px;
  margin-right: 5px;
  width: 11px;
  z-index: 10;
  text-align: center;
  width: auto;
  border-left: 1px solid;
}
#kernel_indicator .kernel_indicator_name {
  padding-left: 5px;
  padding-right: 5px;
}
[dir="rtl"] #kernel_indicator {
  float: left !important;
  float: left;
  border-left: 0;
  border-right: 1px solid;
}
#modal_indicator {
  float: right !important;
  float: right;
  color: #777;
  margin-left: 5px;
  margin-right: 5px;
  width: 11px;
  z-index: 10;
  text-align: center;
  width: auto;
}
[dir="rtl"] #modal_indicator {
  float: left !important;
  float: left;
}
#readonly-indicator {
  float: right !important;
  float: right;
  color: #777;
  margin-left: 5px;
  margin-right: 5px;
  width: 11px;
  z-index: 10;
  text-align: center;
  width: auto;
  margin-top: 2px;
  margin-bottom: 0px;
  margin-left: 0px;
  margin-right: 0px;
  display: none;
}
.modal_indicator:before {
  width: 1.28571429em;
  text-align: center;
}
.edit_mode .modal_indicator:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f040";
}
.edit_mode .modal_indicator:before.fa-pull-left {
  margin-right: .3em;
}
.edit_mode .modal_indicator:before.fa-pull-right {
  margin-left: .3em;
}
.edit_mode .modal_indicator:before.pull-left {
  margin-right: .3em;
}
.edit_mode .modal_indicator:before.pull-right {
  margin-left: .3em;
}
.command_mode .modal_indicator:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: ' ';
}
.command_mode .modal_indicator:before.fa-pull-left {
  margin-right: .3em;
}
.command_mode .modal_indicator:before.fa-pull-right {
  margin-left: .3em;
}
.command_mode .modal_indicator:before.pull-left {
  margin-right: .3em;
}
.command_mode .modal_indicator:before.pull-right {
  margin-left: .3em;
}
.kernel_idle_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f10c";
}
.kernel_idle_icon:before.fa-pull-left {
  margin-right: .3em;
}
.kernel_idle_icon:before.fa-pull-right {
  margin-left: .3em;
}
.kernel_idle_icon:before.pull-left {
  margin-right: .3em;
}
.kernel_idle_icon:before.pull-right {
  margin-left: .3em;
}
.kernel_busy_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f111";
}
.kernel_busy_icon:before.fa-pull-left {
  margin-right: .3em;
}
.kernel_busy_icon:before.fa-pull-right {
  margin-left: .3em;
}
.kernel_busy_icon:before.pull-left {
  margin-right: .3em;
}
.kernel_busy_icon:before.pull-right {
  margin-left: .3em;
}
.kernel_dead_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f1e2";
}
.kernel_dead_icon:before.fa-pull-left {
  margin-right: .3em;
}
.kernel_dead_icon:before.fa-pull-right {
  margin-left: .3em;
}
.kernel_dead_icon:before.pull-left {
  margin-right: .3em;
}
.kernel_dead_icon:before.pull-right {
  margin-left: .3em;
}
.kernel_disconnected_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f127";
}
.kernel_disconnected_icon:before.fa-pull-left {
  margin-right: .3em;
}
.kernel_disconnected_icon:before.fa-pull-right {
  margin-left: .3em;
}
.kernel_disconnected_icon:before.pull-left {
  margin-right: .3em;
}
.kernel_disconnected_icon:before.pull-right {
  margin-left: .3em;
}
.notification_widget {
  color: #777;
  z-index: 10;
  background: rgba(240, 240, 240, 0.5);
  margin-right: 4px;
  color: #333;
  background-color: #fff;
  border-color: #ccc;
}
.notification_widget:focus,
.notification_widget.focus {
  color: #333;
  background-color: #e6e6e6;
  border-color: #8c8c8c;
}
.notification_widget:hover {
  color: #333;
  background-color: #e6e6e6;
  border-color: #adadad;
}
.notification_widget:active,
.notification_widget.active,
.open > .dropdown-toggle.notification_widget {
  color: #333;
  background-color: #e6e6e6;
  border-color: #adadad;
}
.notification_widget:active:hover,
.notification_widget.active:hover,
.open > .dropdown-toggle.notification_widget:hover,
.notification_widget:active:focus,
.notification_widget.active:focus,
.open > .dropdown-toggle.notification_widget:focus,
.notification_widget:active.focus,
.notification_widget.active.focus,
.open > .dropdown-toggle.notification_widget.focus {
  color: #333;
  background-color: #d4d4d4;
  border-color: #8c8c8c;
}
.notification_widget:active,
.notification_widget.active,
.open > .dropdown-toggle.notification_widget {
  background-image: none;
}
.notification_widget.disabled:hover,
.notification_widget[disabled]:hover,
fieldset[disabled] .notification_widget:hover,
.notification_widget.disabled:focus,
.notification_widget[disabled]:focus,
fieldset[disabled] .notification_widget:focus,
.notification_widget.disabled.focus,
.notification_widget[disabled].focus,
fieldset[disabled] .notification_widget.focus {
  background-color: #fff;
  border-color: #ccc;
}
.notification_widget .badge {
  color: #fff;
  background-color: #333;
}
.notification_widget.warning {
  color: #fff;
  background-color: #f0ad4e;
  border-color: #eea236;
}
.notification_widget.warning:focus,
.notification_widget.warning.focus {
  color: #fff;
  background-color: #ec971f;
  border-color: #985f0d;
}
.notification_widget.warning:hover {
  color: #fff;
  background-color: #ec971f;
  border-color: #d58512;
}
.notification_widget.warning:active,
.notification_widget.warning.active,
.open > .dropdown-toggle.notification_widget.warning {
  color: #fff;
  background-color: #ec971f;
  border-color: #d58512;
}
.notification_widget.warning:active:hover,
.notification_widget.warning.active:hover,
.open > .dropdown-toggle.notification_widget.warning:hover,
.notification_widget.warning:active:focus,
.notification_widget.warning.active:focus,
.open > .dropdown-toggle.notification_widget.warning:focus,
.notification_widget.warning:active.focus,
.notification_widget.warning.active.focus,
.open > .dropdown-toggle.notification_widget.warning.focus {
  color: #fff;
  background-color: #d58512;
  border-color: #985f0d;
}
.notification_widget.warning:active,
.notification_widget.warning.active,
.open > .dropdown-toggle.notification_widget.warning {
  background-image: none;
}
.notification_widget.warning.disabled:hover,
.notification_widget.warning[disabled]:hover,
fieldset[disabled] .notification_widget.warning:hover,
.notification_widget.warning.disabled:focus,
.notification_widget.warning[disabled]:focus,
fieldset[disabled] .notification_widget.warning:focus,
.notification_widget.warning.disabled.focus,
.notification_widget.warning[disabled].focus,
fieldset[disabled] .notification_widget.warning.focus {
  background-color: #f0ad4e;
  border-color: #eea236;
}
.notification_widget.warning .badge {
  color: #f0ad4e;
  background-color: #fff;
}
.notification_widget.success {
  color: #fff;
  background-color: #5cb85c;
  border-color: #4cae4c;
}
.notification_widget.success:focus,
.notification_widget.success.focus {
  color: #fff;
  background-color: #449d44;
  border-color: #255625;
}
.notification_widget.success:hover {
  color: #fff;
  background-color: #449d44;
  border-color: #398439;
}
.notification_widget.success:active,
.notification_widget.success.active,
.open > .dropdown-toggle.notification_widget.success {
  color: #fff;
  background-color: #449d44;
  border-color: #398439;
}
.notification_widget.success:active:hover,
.notification_widget.success.active:hover,
.open > .dropdown-toggle.notification_widget.success:hover,
.notification_widget.success:active:focus,
.notification_widget.success.active:focus,
.open > .dropdown-toggle.notification_widget.success:focus,
.notification_widget.success:active.focus,
.notification_widget.success.active.focus,
.open > .dropdown-toggle.notification_widget.success.focus {
  color: #fff;
  background-color: #398439;
  border-color: #255625;
}
.notification_widget.success:active,
.notification_widget.success.active,
.open > .dropdown-toggle.notification_widget.success {
  background-image: none;
}
.notification_widget.success.disabled:hover,
.notification_widget.success[disabled]:hover,
fieldset[disabled] .notification_widget.success:hover,
.notification_widget.success.disabled:focus,
.notification_widget.success[disabled]:focus,
fieldset[disabled] .notification_widget.success:focus,
.notification_widget.success.disabled.focus,
.notification_widget.success[disabled].focus,
fieldset[disabled] .notification_widget.success.focus {
  background-color: #5cb85c;
  border-color: #4cae4c;
}
.notification_widget.success .badge {
  color: #5cb85c;
  background-color: #fff;
}
.notification_widget.info {
  color: #fff;
  background-color: #5bc0de;
  border-color: #46b8da;
}
.notification_widget.info:focus,
.notification_widget.info.focus {
  color: #fff;
  background-color: #31b0d5;
  border-color: #1b6d85;
}
.notification_widget.info:hover {
  color: #fff;
  background-color: #31b0d5;
  border-color: #269abc;
}
.notification_widget.info:active,
.notification_widget.info.active,
.open > .dropdown-toggle.notification_widget.info {
  color: #fff;
  background-color: #31b0d5;
  border-color: #269abc;
}
.notification_widget.info:active:hover,
.notification_widget.info.active:hover,
.open > .dropdown-toggle.notification_widget.info:hover,
.notification_widget.info:active:focus,
.notification_widget.info.active:focus,
.open > .dropdown-toggle.notification_widget.info:focus,
.notification_widget.info:active.focus,
.notification_widget.info.active.focus,
.open > .dropdown-toggle.notification_widget.info.focus {
  color: #fff;
  background-color: #269abc;
  border-color: #1b6d85;
}
.notification_widget.info:active,
.notification_widget.info.active,
.open > .dropdown-toggle.notification_widget.info {
  background-image: none;
}
.notification_widget.info.disabled:hover,
.notification_widget.info[disabled]:hover,
fieldset[disabled] .notification_widget.info:hover,
.notification_widget.info.disabled:focus,
.notification_widget.info[disabled]:focus,
fieldset[disabled] .notification_widget.info:focus,
.notification_widget.info.disabled.focus,
.notification_widget.info[disabled].focus,
fieldset[disabled] .notification_widget.info.focus {
  background-color: #5bc0de;
  border-color: #46b8da;
}
.notification_widget.info .badge {
  color: #5bc0de;
  background-color: #fff;
}
.notification_widget.danger {
  color: #fff;
  background-color: #d9534f;
  border-color: #d43f3a;
}
.notification_widget.danger:focus,
.notification_widget.danger.focus {
  color: #fff;
  background-color: #c9302c;
  border-color: #761c19;
}
.notification_widget.danger:hover {
  color: #fff;
  background-color: #c9302c;
  border-color: #ac2925;
}
.notification_widget.danger:active,
.notification_widget.danger.active,
.open > .dropdown-toggle.notification_widget.danger {
  color: #fff;
  background-color: #c9302c;
  border-color: #ac2925;
}
.notification_widget.danger:active:hover,
.notification_widget.danger.active:hover,
.open > .dropdown-toggle.notification_widget.danger:hover,
.notification_widget.danger:active:focus,
.notification_widget.danger.active:focus,
.open > .dropdown-toggle.notification_widget.danger:focus,
.notification_widget.danger:active.focus,
.notification_widget.danger.active.focus,
.open > .dropdown-toggle.notification_widget.danger.focus {
  color: #fff;
  background-color: #ac2925;
  border-color: #761c19;
}
.notification_widget.danger:active,
.notification_widget.danger.active,
.open > .dropdown-toggle.notification_widget.danger {
  background-image: none;
}
.notification_widget.danger.disabled:hover,
.notification_widget.danger[disabled]:hover,
fieldset[disabled] .notification_widget.danger:hover,
.notification_widget.danger.disabled:focus,
.notification_widget.danger[disabled]:focus,
fieldset[disabled] .notification_widget.danger:focus,
.notification_widget.danger.disabled.focus,
.notification_widget.danger[disabled].focus,
fieldset[disabled] .notification_widget.danger.focus {
  background-color: #d9534f;
  border-color: #d43f3a;
}
.notification_widget.danger .badge {
  color: #d9534f;
  background-color: #fff;
}
div#pager {
  background-color: #fff;
  font-size: 14px;
  line-height: 20px;
  overflow: hidden;
  display: none;
  position: fixed;
  bottom: 0px;
  width: 100%;
  max-height: 50%;
  padding-top: 8px;
  -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
  box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
  /* Display over codemirror */
  z-index: 100;
  /* Hack which prevents jquery ui resizable from changing top. */
  top: auto !important;
}
div#pager pre {
  line-height: 1.21429em;
  color: #000;
  background-color: #f7f7f7;
  padding: 0.4em;
}
div#pager #pager-button-area {
  position: absolute;
  top: 8px;
  right: 20px;
}
div#pager #pager-contents {
  position: relative;
  overflow: auto;
  width: 100%;
  height: 100%;
}
div#pager #pager-contents #pager-container {
  position: relative;
  padding: 15px 0px;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
}
div#pager .ui-resizable-handle {
  top: 0px;
  height: 8px;
  background: #f7f7f7;
  border-top: 1px solid #cfcfcf;
  border-bottom: 1px solid #cfcfcf;
  /* This injects handle bars (a short, wide = symbol) for 
        the resize handle. */
}
div#pager .ui-resizable-handle::after {
  content: '';
  top: 2px;
  left: 50%;
  height: 3px;
  width: 30px;
  margin-left: -15px;
  position: absolute;
  border-top: 1px solid #cfcfcf;
}
.quickhelp {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
  line-height: 1.8em;
}
.shortcut_key {
  display: inline-block;
  width: 21ex;
  text-align: right;
  font-family: monospace;
}
.shortcut_descr {
  display: inline-block;
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
}
span.save_widget {
  height: 30px;
  margin-top: 4px;
  display: flex;
  justify-content: flex-start;
  align-items: baseline;
  width: 50%;
  flex: 1;
}
span.save_widget span.filename {
  height: 100%;
  line-height: 1em;
  margin-left: 16px;
  border: none;
  font-size: 146.5%;
  text-overflow: ellipsis;
  overflow: hidden;
  white-space: nowrap;
  border-radius: 2px;
}
span.save_widget span.filename:hover {
  background-color: #e6e6e6;
}
[dir="rtl"] span.save_widget.pull-left {
  float: right !important;
  float: right;
}
[dir="rtl"] span.save_widget span.filename {
  margin-left: 0;
  margin-right: 16px;
}
span.checkpoint_status,
span.autosave_status {
  font-size: small;
  white-space: nowrap;
  padding: 0 5px;
}
@media (max-width: 767px) {
  span.save_widget {
    font-size: small;
    padding: 0 0 0 5px;
  }
  span.checkpoint_status,
  span.autosave_status {
    display: none;
  }
}
@media (min-width: 768px) and (max-width: 991px) {
  span.checkpoint_status {
    display: none;
  }
  span.autosave_status {
    font-size: x-small;
  }
}
.toolbar {
  padding: 0px;
  margin-left: -5px;
  margin-top: 2px;
  margin-bottom: 5px;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
}
.toolbar select,
.toolbar label {
  width: auto;
  vertical-align: middle;
  margin-right: 2px;
  margin-bottom: 0px;
  display: inline;
  font-size: 92%;
  margin-left: 0.3em;
  margin-right: 0.3em;
  padding: 0px;
  padding-top: 3px;
}
.toolbar .btn {
  padding: 2px 8px;
}
.toolbar .btn-group {
  margin-top: 0px;
  margin-left: 5px;
}
.toolbar-btn-label {
  margin-left: 6px;
}
#maintoolbar {
  margin-bottom: -3px;
  margin-top: -8px;
  border: 0px;
  min-height: 27px;
  margin-left: 0px;
  padding-top: 11px;
  padding-bottom: 3px;
}
#maintoolbar .navbar-text {
  float: none;
  vertical-align: middle;
  text-align: right;
  margin-left: 5px;
  margin-right: 0px;
  margin-top: 0px;
}
.select-xs {
  height: 24px;
}
[dir="rtl"] .btn-group > .btn,
.btn-group-vertical > .btn {
  float: right;
}
.pulse,
.dropdown-menu > li > a.pulse,
li.pulse > a.dropdown-toggle,
li.pulse.open > a.dropdown-toggle {
  background-color: #F37626;
  color: white;
}
/**
 * Primary styles
 *
 * Author: Jupyter Development Team
 */
/** WARNING IF YOU ARE EDITTING THIS FILE, if this is a .css file, It has a lot
 * of chance of beeing generated from the ../less/[samename].less file, you can
 * try to get back the less file by reverting somme commit in history
 **/
/*
 * We'll try to get something pretty, so we
 * have some strange css to have the scroll bar on
 * the left with fix button on the top right of the tooltip
 */
@-moz-keyframes fadeOut {
  from {
    opacity: 1;
  }
  to {
    opacity: 0;
  }
}
@-webkit-keyframes fadeOut {
  from {
    opacity: 1;
  }
  to {
    opacity: 0;
  }
}
@-moz-keyframes fadeIn {
  from {
    opacity: 0;
  }
  to {
    opacity: 1;
  }
}
@-webkit-keyframes fadeIn {
  from {
    opacity: 0;
  }
  to {
    opacity: 1;
  }
}
/*properties of tooltip after "expand"*/
.bigtooltip {
  overflow: auto;
  height: 200px;
  -webkit-transition-property: height;
  -webkit-transition-duration: 500ms;
  -moz-transition-property: height;
  -moz-transition-duration: 500ms;
  transition-property: height;
  transition-duration: 500ms;
}
/*properties of tooltip before "expand"*/
.smalltooltip {
  -webkit-transition-property: height;
  -webkit-transition-duration: 500ms;
  -moz-transition-property: height;
  -moz-transition-duration: 500ms;
  transition-property: height;
  transition-duration: 500ms;
  text-overflow: ellipsis;
  overflow: hidden;
  height: 80px;
}
.tooltipbuttons {
  position: absolute;
  padding-right: 15px;
  top: 0px;
  right: 0px;
}
.tooltiptext {
  /*avoid the button to overlap on some docstring*/
  padding-right: 30px;
}
.ipython_tooltip {
  max-width: 700px;
  /*fade-in animation when inserted*/
  -webkit-animation: fadeOut 400ms;
  -moz-animation: fadeOut 400ms;
  animation: fadeOut 400ms;
  -webkit-animation: fadeIn 400ms;
  -moz-animation: fadeIn 400ms;
  animation: fadeIn 400ms;
  vertical-align: middle;
  background-color: #f7f7f7;
  overflow: visible;
  border: #ababab 1px solid;
  outline: none;
  padding: 3px;
  margin: 0px;
  padding-left: 7px;
  font-family: monospace;
  min-height: 50px;
  -moz-box-shadow: 0px 6px 10px -1px #adadad;
  -webkit-box-shadow: 0px 6px 10px -1px #adadad;
  box-shadow: 0px 6px 10px -1px #adadad;
  border-radius: 2px;
  position: absolute;
  z-index: 1000;
}
.ipython_tooltip a {
  float: right;
}
.ipython_tooltip .tooltiptext pre {
  border: 0;
  border-radius: 0;
  font-size: 100%;
  background-color: #f7f7f7;
}
.pretooltiparrow {
  left: 0px;
  margin: 0px;
  top: -16px;
  width: 40px;
  height: 16px;
  overflow: hidden;
  position: absolute;
}
.pretooltiparrow:before {
  background-color: #f7f7f7;
  border: 1px #ababab solid;
  z-index: 11;
  content: "";
  position: absolute;
  left: 15px;
  top: 10px;
  width: 25px;
  height: 25px;
  -webkit-transform: rotate(45deg);
  -moz-transform: rotate(45deg);
  -ms-transform: rotate(45deg);
  -o-transform: rotate(45deg);
}
ul.typeahead-list i {
  margin-left: -10px;
  width: 18px;
}
[dir="rtl"] ul.typeahead-list i {
  margin-left: 0;
  margin-right: -10px;
}
ul.typeahead-list {
  max-height: 80vh;
  overflow: auto;
}
ul.typeahead-list > li > a {
  /** Firefox bug **/
  /* see https://github.com/jupyter/notebook/issues/559 */
  white-space: normal;
}
ul.typeahead-list  > li > a.pull-right {
  float: left !important;
  float: left;
}
[dir="rtl"] .typeahead-list {
  text-align: right;
}
.cmd-palette .modal-body {
  padding: 7px;
}
.cmd-palette form {
  background: white;
}
.cmd-palette input {
  outline: none;
}
.no-shortcut {
  min-width: 20px;
  color: transparent;
}
[dir="rtl"] .no-shortcut.pull-right {
  float: left !important;
  float: left;
}
[dir="rtl"] .command-shortcut.pull-right {
  float: left !important;
  float: left;
}
.command-shortcut:before {
  content: "(command mode)";
  padding-right: 3px;
  color: #777777;
}
.edit-shortcut:before {
  content: "(edit)";
  padding-right: 3px;
  color: #777777;
}
[dir="rtl"] .edit-shortcut.pull-right {
  float: left !important;
  float: left;
}
#find-and-replace #replace-preview .match,
#find-and-replace #replace-preview .insert {
  background-color: #BBDEFB;
  border-color: #90CAF9;
  border-style: solid;
  border-width: 1px;
  border-radius: 0px;
}
[dir="ltr"] #find-and-replace .input-group-btn + .form-control {
  border-left: none;
}
[dir="rtl"] #find-and-replace .input-group-btn + .form-control {
  border-right: none;
}
#find-and-replace #replace-preview .replace .match {
  background-color: #FFCDD2;
  border-color: #EF9A9A;
  border-radius: 0px;
}
#find-and-replace #replace-preview .replace .insert {
  background-color: #C8E6C9;
  border-color: #A5D6A7;
  border-radius: 0px;
}
#find-and-replace #replace-preview {
  max-height: 60vh;
  overflow: auto;
}
#find-and-replace #replace-preview pre {
  padding: 5px 10px;
}
.terminal-app {
  background: #EEE;
}
.terminal-app #header {
  background: #fff;
  -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
  box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
}
.terminal-app .terminal {
  width: 100%;
  float: left;
  font-family: monospace;
  color: white;
  background: black;
  padding: 0.4em;
  border-radius: 2px;
  -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.4);
  box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.4);
}
.terminal-app .terminal,
.terminal-app .terminal dummy-screen {
  line-height: 1em;
  font-size: 14px;
}
.terminal-app .terminal .xterm-rows {
  padding: 10px;
}
.terminal-app .terminal-cursor {
  color: black;
  background: white;
}
.terminal-app #terminado-container {
  margin-top: 20px;
}
/*# sourceMappingURL=style.min.css.map */
    </style>
<style type="text/css">
    .highlight .hll { background-color: #ffffcc }
.highlight  { background: #f8f8f8; }
.highlight .c { color: #408080; font-style: italic } /* Comment */
.highlight .err { border: 1px solid #FF0000 } /* Error */
.highlight .k { color: #008000; font-weight: bold } /* Keyword */
.highlight .o { color: #666666 } /* Operator */
.highlight .ch { color: #408080; font-style: italic } /* Comment.Hashbang */
.highlight .cm { color: #408080; font-style: italic } /* Comment.Multiline */
.highlight .cp { color: #BC7A00 } /* Comment.Preproc */
.highlight .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */
.highlight .c1 { color: #408080; font-style: italic } /* Comment.Single */
.highlight .cs { color: #408080; font-style: italic } /* Comment.Special */
.highlight .gd { color: #A00000 } /* Generic.Deleted */
.highlight .ge { font-style: italic } /* Generic.Emph */
.highlight .gr { color: #FF0000 } /* Generic.Error */
.highlight .gh { color: #000080; font-weight: bold } /* Generic.Heading */
.highlight .gi { color: #00A000 } /* Generic.Inserted */
.highlight .go { color: #888888 } /* Generic.Output */
.highlight .gp { color: #000080; font-weight: bold } /* Generic.Prompt */
.highlight .gs { font-weight: bold } /* Generic.Strong */
.highlight .gu { color: #800080; font-weight: bold } /* Generic.Subheading */
.highlight .gt { color: #0044DD } /* Generic.Traceback */
.highlight .kc { color: #008000; font-weight: bold } /* Keyword.Constant */
.highlight .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */
.highlight .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */
.highlight .kp { color: #008000 } /* Keyword.Pseudo */
.highlight .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */
.highlight .kt { color: #B00040 } /* Keyword.Type */
.highlight .m { color: #666666 } /* Literal.Number */
.highlight .s { color: #BA2121 } /* Literal.String */
.highlight .na { color: #7D9029 } /* Name.Attribute */
.highlight .nb { color: #008000 } /* Name.Builtin */
.highlight .nc { color: #0000FF; font-weight: bold } /* Name.Class */
.highlight .no { color: #880000 } /* Name.Constant */
.highlight .nd { color: #AA22FF } /* Name.Decorator */
.highlight .ni { color: #999999; font-weight: bold } /* Name.Entity */
.highlight .ne { color: #D2413A; font-weight: bold } /* Name.Exception */
.highlight .nf { color: #0000FF } /* Name.Function */
.highlight .nl { color: #A0A000 } /* Name.Label */
.highlight .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */
.highlight .nt { color: #008000; font-weight: bold } /* Name.Tag */
.highlight .nv { color: #19177C } /* Name.Variable */
.highlight .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */
.highlight .w { color: #bbbbbb } /* Text.Whitespace */
.highlight .mb { color: #666666 } /* Literal.Number.Bin */
.highlight .mf { color: #666666 } /* Literal.Number.Float */
.highlight .mh { color: #666666 } /* Literal.Number.Hex */
.highlight .mi { color: #666666 } /* Literal.Number.Integer */
.highlight .mo { color: #666666 } /* Literal.Number.Oct */
.highlight .sa { color: #BA2121 } /* Literal.String.Affix */
.highlight .sb { color: #BA2121 } /* Literal.String.Backtick */
.highlight .sc { color: #BA2121 } /* Literal.String.Char */
.highlight .dl { color: #BA2121 } /* Literal.String.Delimiter */
.highlight .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */
.highlight .s2 { color: #BA2121 } /* Literal.String.Double */
.highlight .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */
.highlight .sh { color: #BA2121 } /* Literal.String.Heredoc */
.highlight .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */
.highlight .sx { color: #008000 } /* Literal.String.Other */
.highlight .sr { color: #BB6688 } /* Literal.String.Regex */
.highlight .s1 { color: #BA2121 } /* Literal.String.Single */
.highlight .ss { color: #19177C } /* Literal.String.Symbol */
.highlight .bp { color: #008000 } /* Name.Builtin.Pseudo */
.highlight .fm { color: #0000FF } /* Name.Function.Magic */
.highlight .vc { color: #19177C } /* Name.Variable.Class */
.highlight .vg { color: #19177C } /* Name.Variable.Global */
.highlight .vi { color: #19177C } /* Name.Variable.Instance */
.highlight .vm { color: #19177C } /* Name.Variable.Magic */
.highlight .il { color: #666666 } /* Literal.Number.Integer.Long */
    </style>
<style type="text/css">
    
/* Temporary definitions which will become obsolete with Notebook release 5.0 */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-bold { font-weight: bold; }

    </style>
<style type="text/css">
    body[data-notebook-name] > #header #header-container:not(.show-panel), /* body has data-notebook-name attribute when 
a notebook is open. this rule won't be applied when the notebook is not open (in 404 view for example) */
body[data-notebook-name] #menubar-container #maintoolbar:not(.show-panel),
body[data-notebook-name] #menubar-container ul.navbar-nav:not(.show-panel) {
  display: none;
}

#menubar.only-panel #menus.navbar-default {
    background: #fff;
    border-color: #fff;
}

.toolbar.with-statusbar{
    margin-top: 3px;
    margin-bottom: 0;
}

    </style>


<style type="text/css">
/* Overrides of notebook CSS for static HTML export */
body {
  overflow: visible;
  padding: 8px;
}

div#notebook {
  overflow: visible;
  border-top: none;
}@media print {
  div.cell {
    display: block;
    page-break-inside: avoid;
  } 
  div.output_wrapper { 
    display: block;
    page-break-inside: avoid; 
  }
  div.output { 
    display: block;
    page-break-inside: avoid; 
  }
}
</style>

<!-- Custom stylesheet, it must be in the same directory as the html file -->
<link rel="stylesheet" href="custom.css">

<!-- Loading mathjax macro -->
<!-- Load mathjax -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_HTML"></script>
    <!-- MathJax configuration -->
    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ['$','$'], ["\\(","\\)"] ],
            displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
            processEscapes: true,
            processEnvironments: true
        },
        // Center justify equations in code and markdown cells. Elsewhere
        // we use CSS to left justify single line equations in code cells.
        displayAlign: 'center',
        "HTML-CSS": {
            styles: {'.MathJax_Display': {"margin": 0}},
            linebreaks: { automatic: true }
        }
    });
    </script>
    <!-- End of mathjax configuration --></head>
<body>
  <div tabindex="-1" id="notebook" class="border-box-sizing">
    <div class="container" id="notebook-container">

<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="TV-Script-Generation">TV Script Generation<a class="anchor-link" href="#TV-Script-Generation">&#182;</a></h1><p>In this project, you'll generate your own <a href="https://en.wikipedia.org/wiki/Seinfeld">Seinfeld</a> TV scripts using RNNs.  You'll be using part of the <a href="https://www.kaggle.com/thec03u5/seinfeld-chronicles#scripts.csv">Seinfeld dataset</a> of scripts from 9 seasons.  The Neural Network you'll build will generate a new ,"fake" TV script, based on patterns it recognizes in this training data.</p>
<h2 id="Get-the-Data">Get the Data<a class="anchor-link" href="#Get-the-Data">&#182;</a></h2><p>The data is already provided for you in <code>./data/Seinfeld_Scripts.txt</code> and you're encouraged to open that file and look at the text.</p>
<blockquote><ul>
<li>As a first step, we'll load in this data and look at some samples. </li>
<li>Then, you'll be tasked with defining and training an RNN to generate a new script!</li>
</ul>
</blockquote>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[1]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">DON&#39;T MODIFY ANYTHING IN THIS CELL</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="c1"># load in data</span>
<span class="kn">import</span> <span class="nn">helper</span>
<span class="n">data_dir</span> <span class="o">=</span> <span class="s1">&#39;./data/Seinfeld_Scripts.txt&#39;</span>
<span class="n">text</span> <span class="o">=</span> <span class="n">helper</span><span class="o">.</span><span class="n">load_data</span><span class="p">(</span><span class="n">data_dir</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Explore-the-Data">Explore the Data<a class="anchor-link" href="#Explore-the-Data">&#182;</a></h2><p>Play around with <code>view_line_range</code> to view different parts of the data. This will give you a sense of the data you'll be working with. You can see, for example, that it is all lowercase text, and each new line of dialogue is separated by a newline character <code>\n</code>.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[2]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">view_line_range</span> <span class="o">=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>

<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">DON&#39;T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Dataset Stats&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Roughly the number of unique words: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">len</span><span class="p">({</span><span class="n">word</span><span class="p">:</span> <span class="kc">None</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">text</span><span class="o">.</span><span class="n">split</span><span class="p">()})))</span>

<span class="n">lines</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Number of lines: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">lines</span><span class="p">)))</span>
<span class="n">word_count_line</span> <span class="o">=</span> <span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">line</span><span class="o">.</span><span class="n">split</span><span class="p">())</span> <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">lines</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Average number of words in each line: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">average</span><span class="p">(</span><span class="n">word_count_line</span><span class="p">)))</span>

<span class="nb">print</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;The lines </span><span class="si">{}</span><span class="s1"> to </span><span class="si">{}</span><span class="s1">:&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="o">*</span><span class="n">view_line_range</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">text</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)[</span><span class="n">view_line_range</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span><span class="n">view_line_range</span><span class="p">[</span><span class="mi">1</span><span class="p">]]))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Dataset Stats
Roughly the number of unique words: 46367
Number of lines: 109233
Average number of words in each line: 5.544240293684143

The lines 0 to 10:
jerry: do you know what this is all about? do you know, why were here? to be out, this is out...and out is one of the single most enjoyable experiences of life. people...did you ever hear people talking about we should go out? this is what theyre talking about...this whole thing, were all out now, no one is home. not one person here is home, were all out! there are people trying to find us, they dont know where we are. (on an imaginary phone) did you ring?, i cant find him. where did he go? he didnt tell me where he was going. he must have gone out. you wanna go out you get ready, you pick out the clothes, right? you take the shower, you get all ready, get the cash, get your friends, the car, the spot, the reservation...then youre standing around, what do you do? you go we gotta be getting back. once youre out, you wanna get back! you wanna go to sleep, you wanna get up, you wanna go out again tomorrow, right? where ever you are in life, its my feeling, youve gotta go. 

jerry: (pointing at georges shirt) see, to me, that button is in the worst possible spot. the second button literally makes or breaks the shirt, look at it. its too high! its in no-mans-land. you look like you live with your mother. 

george: are you through? 

jerry: you do of course try on, when you buy? 

george: yes, it was purple, i liked it, i dont actually recall considering the buttons. 

</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<h2 id="Implement-Pre-processing-Functions">Implement Pre-processing Functions<a class="anchor-link" href="#Implement-Pre-processing-Functions">&#182;</a></h2><p>The first thing to do to any dataset is pre-processing.  Implement the following pre-processing functions below:</p>
<ul>
<li>Lookup Table</li>
<li>Tokenize Punctuation</li>
</ul>
<h3 id="Lookup-Table">Lookup Table<a class="anchor-link" href="#Lookup-Table">&#182;</a></h3><p>To create a word embedding, you first need to transform the words to ids.  In this function, create two dictionaries:</p>
<ul>
<li>Dictionary to go from the words to an id, we'll call <code>vocab_to_int</code></li>
<li>Dictionary to go from the id to word, we'll call <code>int_to_vocab</code></li>
</ul>
<p>Return these dictionaries in the following <strong>tuple</strong> <code>(vocab_to_int, int_to_vocab)</code></p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[3]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">problem_unittests</span> <span class="k">as</span> <span class="nn">tests</span>
<span class="kn">import</span> <span class="nn">re</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="k">import</span> <span class="n">Counter</span>
<span class="k">def</span> <span class="nf">create_lookup_tables</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Create lookup tables for vocabulary</span>
<span class="sd">    :param text: The text of tv scripts split into words</span>
<span class="sd">    :return: A tuple of dicts (vocab_to_int, int_to_vocab)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># TODO: Implement Function</span>
    <span class="n">word_counts</span> <span class="o">=</span> <span class="n">Counter</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
    <span class="c1"># sorting the words from most to least frequent in text occurrence</span>
    <span class="n">sorted_vocab</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">word_counts</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="n">word_counts</span><span class="o">.</span><span class="n">get</span><span class="p">,</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="c1"># create int_to_vocab dictionaries</span>
    <span class="n">int_to_vocab</span> <span class="o">=</span> <span class="p">{</span><span class="n">ii</span><span class="p">:</span> <span class="n">word</span> <span class="k">for</span> <span class="n">ii</span><span class="p">,</span> <span class="n">word</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">sorted_vocab</span><span class="p">)}</span>
    <span class="n">vocab_to_int</span> <span class="o">=</span> <span class="p">{</span><span class="n">word</span><span class="p">:</span> <span class="n">ii</span> <span class="k">for</span> <span class="n">ii</span><span class="p">,</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">int_to_vocab</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
<span class="c1">#     print(&quot;int_to_vocab&quot;)</span>
<span class="c1">#     print(int_to_vocab)</span>
    <span class="k">return</span> <span class="n">vocab_to_int</span><span class="p">,</span> <span class="n">int_to_vocab</span>


<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">DON&#39;T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="n">tests</span><span class="o">.</span><span class="n">test_create_lookup_tables</span><span class="p">(</span><span class="n">create_lookup_tables</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Tests Passed
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Tokenize-Punctuation">Tokenize Punctuation<a class="anchor-link" href="#Tokenize-Punctuation">&#182;</a></h3><p>We'll be splitting the script into a word array using spaces as delimiters.  However, punctuations like periods and exclamation marks can create multiple ids for the same word. For example, "bye" and "bye!" would generate two different word ids.</p>
<p>Implement the function <code>token_lookup</code> to return a dict that will be used to tokenize symbols like "!" into "||Exclamation_Mark||".  Create a dictionary for the following symbols where the symbol is the key and value is the token:</p>
<ul>
<li>Period ( <strong>.</strong> )</li>
<li>Comma ( <strong>,</strong> )</li>
<li>Quotation Mark ( <strong>"</strong> )</li>
<li>Semicolon ( <strong>;</strong> )</li>
<li>Exclamation mark ( <strong>!</strong> )</li>
<li>Question mark ( <strong>?</strong> )</li>
<li>Left Parentheses ( <strong>(</strong> )</li>
<li>Right Parentheses ( <strong>)</strong> )</li>
<li>Dash ( <strong>-</strong> )</li>
<li>Return ( <strong>\n</strong> )</li>
</ul>
<p>This dictionary will be used to tokenize the symbols and add the delimiter (space) around it.  This separates each symbols as its own word, making it easier for the neural network to predict the next word. Make sure you don't use a value that could be confused as a word; for example, instead of using the value "dash", try using something like "||dash||".</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[4]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">token_lookup</span><span class="p">():</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Generate a dict to turn punctuation into a token.</span>
<span class="sd">    :return: Tokenized dictionary where the key is the punctuation and the value is the token</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># TODO: Implement Function</span>
    <span class="n">punctuations_dict</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;.&quot;</span><span class="p">:</span> <span class="s2">&quot;||Period||&quot;</span><span class="p">,</span>
        <span class="s2">&quot;,&quot;</span><span class="p">:</span> <span class="s2">&quot;||Comma||&quot;</span><span class="p">,</span>
        <span class="s2">&quot;</span><span class="se">\&quot;</span><span class="s2">&quot;</span><span class="p">:</span> <span class="s2">&quot;||Quotation_Mark||&quot;</span><span class="p">,</span>
        <span class="s2">&quot;;&quot;</span><span class="p">:</span> <span class="s2">&quot;||Semicolon||&quot;</span><span class="p">,</span>
        <span class="s2">&quot;!&quot;</span><span class="p">:</span> <span class="s2">&quot;||Exclamation_Mark||&quot;</span><span class="p">,</span>
        <span class="s2">&quot;?&quot;</span><span class="p">:</span> <span class="s2">&quot;||Question_Mark||&quot;</span><span class="p">,</span>
        <span class="s2">&quot;(&quot;</span><span class="p">:</span> <span class="s2">&quot;||Left_Parentheses||&quot;</span><span class="p">,</span>
        <span class="s2">&quot;)&quot;</span><span class="p">:</span> <span class="s2">&quot;||Right_Parentheses||&quot;</span><span class="p">,</span>
        <span class="s2">&quot;-&quot;</span><span class="p">:</span> <span class="s2">&quot;||Dash||&quot;</span><span class="p">,</span>
        <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">:</span> <span class="s2">&quot;||Return||&quot;</span>
    <span class="p">}</span>
    <span class="k">return</span> <span class="n">punctuations_dict</span>

<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">DON&#39;T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="n">tests</span><span class="o">.</span><span class="n">test_tokenize</span><span class="p">(</span><span class="n">token_lookup</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Tests Passed
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Pre-process-all-the-data-and-save-it">Pre-process all the data and save it<a class="anchor-link" href="#Pre-process-all-the-data-and-save-it">&#182;</a></h2><p>Running the code cell below will pre-process all the data and save it to file. You're encouraged to lok at the code for <code>preprocess_and_save_data</code> in the <code>helpers.py</code> file to see what it's doing in detail, but you do not need to change this code.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[5]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">DON&#39;T MODIFY ANYTHING IN THIS CELL</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="c1"># pre-process training data</span>
<span class="n">helper</span><span class="o">.</span><span class="n">preprocess_and_save_data</span><span class="p">(</span><span class="n">data_dir</span><span class="p">,</span> <span class="n">token_lookup</span><span class="p">,</span> <span class="n">create_lookup_tables</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Check-Point">Check Point<a class="anchor-link" href="#Check-Point">&#182;</a></h1><p>This is your first checkpoint. If you ever decide to come back to this notebook or have to restart the notebook, you can start from here. The preprocessed data has been saved to disk.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[6]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">DON&#39;T MODIFY ANYTHING IN THIS CELL</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="kn">import</span> <span class="nn">helper</span>
<span class="kn">import</span> <span class="nn">problem_unittests</span> <span class="k">as</span> <span class="nn">tests</span>

<span class="n">int_text</span><span class="p">,</span> <span class="n">vocab_to_int</span><span class="p">,</span> <span class="n">int_to_vocab</span><span class="p">,</span> <span class="n">token_dict</span> <span class="o">=</span> <span class="n">helper</span><span class="o">.</span><span class="n">load_preprocess</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Build-the-Neural-Network">Build the Neural Network<a class="anchor-link" href="#Build-the-Neural-Network">&#182;</a></h2><p>In this section, you'll build the components necessary to build an RNN by implementing the RNN Module and forward and backpropagation functions.</p>
<h3 id="Check-Access-to-GPU">Check Access to GPU<a class="anchor-link" href="#Check-Access-to-GPU">&#182;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[7]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">DON&#39;T MODIFY ANYTHING IN THIS CELL</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="kn">import</span> <span class="nn">torch</span>

<span class="c1"># Check for a GPU</span>
<span class="n">train_on_gpu</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span>
<span class="k">if</span> <span class="ow">not</span> <span class="n">train_on_gpu</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;No GPU found. Please use a GPU to train your neural network.&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Input">Input<a class="anchor-link" href="#Input">&#182;</a></h2><p>Let's start with the preprocessed input data. We'll use <a href="http://pytorch.org/docs/master/data.html#torch.utils.data.TensorDataset">TensorDataset</a> to provide a known format to our dataset; in combination with <a href="http://pytorch.org/docs/master/data.html#torch.utils.data.DataLoader">DataLoader</a>, it will handle batching, shuffling, and other dataset iteration functions.</p>
<p>You can create data with TensorDataset by passing in feature and target tensors. Then create a DataLoader as usual.</p>

<pre><code>data = TensorDataset(feature_tensors, target_tensors)
data_loader = torch.utils.data.DataLoader(data, 
                                          batch_size=batch_size)</code></pre>
<h3 id="Batching">Batching<a class="anchor-link" href="#Batching">&#182;</a></h3><p>Implement the <code>batch_data</code> function to batch <code>words</code> data into chunks of size <code>batch_size</code> using the <code>TensorDataset</code> and <code>DataLoader</code> classes.</p>
<blockquote><p>You can batch words using the DataLoader, but it will be up to you to create <code>feature_tensors</code> and <code>target_tensors</code> of the correct size and content for a given <code>sequence_length</code>.</p>
</blockquote>
<p>For example, say we have these as input:</p>

<pre><code>words = [1, 2, 3, 4, 5, 6, 7]
sequence_length = 4</code></pre>
<p>Your first <code>feature_tensor</code> should contain the values:</p>

<pre><code>[1, 2, 3, 4]</code></pre>
<p>And the corresponding <code>target_tensor</code> should just be the next "word"/tokenized word value:</p>

<pre><code>5</code></pre>
<p>This should continue with the second <code>feature_tensor</code>, <code>target_tensor</code> being:</p>

<pre><code>[2, 3, 4, 5]  # features
6             # target</code></pre>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[8]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="k">import</span> <span class="n">TensorDataset</span><span class="p">,</span> <span class="n">DataLoader</span>


<span class="k">def</span> <span class="nf">batch_data</span><span class="p">(</span><span class="n">words</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Batch the neural network data using DataLoader</span>
<span class="sd">    :param words: The word ids of the TV scripts</span>
<span class="sd">    :param sequence_length: The sequence length of each batch</span>
<span class="sd">    :param batch_size: The size of each batch; the number of sequences in a batch</span>
<span class="sd">    :return: DataLoader with batched data</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># TODO: Implement function</span>
    <span class="n">n_batches</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">words</span><span class="p">)</span><span class="o">//</span><span class="n">batch_size</span>
    
    <span class="c1"># only full batches</span>
    <span class="n">words</span> <span class="o">=</span> <span class="n">words</span><span class="p">[:</span><span class="n">n_batches</span><span class="o">*</span><span class="n">batch_size</span><span class="p">]</span>
    
    <span class="n">feature</span><span class="p">,</span> <span class="n">target</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">words</span><span class="p">),</span> <span class="n">sequence_length</span><span class="p">):</span>
        <span class="n">batch</span> <span class="o">=</span> <span class="n">words</span><span class="p">[</span><span class="n">idx</span><span class="p">:</span><span class="n">idx</span><span class="o">+</span><span class="n">sequence_length</span><span class="p">]</span>
<span class="c1">#         print(batch)</span>
        
        <span class="n">feature</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
        <span class="n">batch_y</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">+</span><span class="mi">1</span>
        <span class="n">target</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">batch_y</span><span class="p">)</span>
<span class="c1">#         print(feature)</span>
<span class="c1">#         for ii in range(len(batch)):</span>
<span class="c1">#             batch_x = batch[ii]</span>
<span class="c1">#             batch_y = batch_x+1</span>
<span class="c1">#             target.append(batch_y)</span>
<span class="c1">#             feature.extend([batch_x])</span>
    <span class="n">feature_tensors</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="n">feature</span><span class="p">)</span>
    <span class="n">target_tensors</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="n">target</span><span class="p">)</span>
    
    <span class="c1"># return a dataloader</span>
<span class="c1">#     print(feature_tensors)</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">TensorDataset</span><span class="p">(</span><span class="n">feature_tensors</span><span class="p">,</span> <span class="n">target_tensors</span><span class="p">)</span>
    <span class="n">data_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> 
                                              <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">data_loader</span>

<span class="c1"># there is no test for this function, but you are encouraged to create</span>
<span class="c1"># print statements and tests of your own</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Test-your-dataloader">Test your dataloader<a class="anchor-link" href="#Test-your-dataloader">&#182;</a></h3><p>You'll have to modify this code to test a batching function, but it should look fairly similar.</p>
<p>Below, we're generating some test text data and defining a dataloader using the function you defined, above. Then, we are getting some sample batch of inputs <code>sample_x</code> and targets <code>sample_y</code> from our dataloader.</p>
<p>Your code should return something like the following (likely in a different order, if you shuffled your data):</p>

<pre><code>torch.Size([10, 5])
tensor([[ 28,  29,  30,  31,  32],
        [ 21,  22,  23,  24,  25],
        [ 17,  18,  19,  20,  21],
        [ 34,  35,  36,  37,  38],
        [ 11,  12,  13,  14,  15],
        [ 23,  24,  25,  26,  27],
        [  6,   7,   8,   9,  10],
        [ 38,  39,  40,  41,  42],
        [ 25,  26,  27,  28,  29],
        [  7,   8,   9,  10,  11]])

torch.Size([10])
tensor([ 33,  26,  22,  39,  16,  28,  11,  43,  30,  12])</code></pre>
<h3 id="Sizes">Sizes<a class="anchor-link" href="#Sizes">&#182;</a></h3><p>Your sample_x should be of size <code>(batch_size, sequence_length)</code> or (10, 5) in this case and sample_y should just have one dimension: batch_size (10).</p>
<h3 id="Values">Values<a class="anchor-link" href="#Values">&#182;</a></h3><p>You should also notice that the targets, sample_y, are the <em>next</em> value in the ordered test_text data. So, for an input sequence <code>[ 28,  29,  30,  31,  32]</code> that ends with the value <code>32</code>, the corresponding output should be <code>33</code>.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[9]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># test dataloader</span>

<span class="n">test_text</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">50</span><span class="p">)</span>
<span class="n">t_loader</span> <span class="o">=</span> <span class="n">batch_data</span><span class="p">(</span><span class="n">test_text</span><span class="p">,</span> <span class="n">sequence_length</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

<span class="n">data_iter</span> <span class="o">=</span> <span class="nb">iter</span><span class="p">(</span><span class="n">t_loader</span><span class="p">)</span>
<span class="n">sample_x</span><span class="p">,</span> <span class="n">sample_y</span> <span class="o">=</span> <span class="n">data_iter</span><span class="o">.</span><span class="n">next</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="n">sample_x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">sample_x</span><span class="p">)</span>
<span class="nb">print</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">sample_y</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">sample_y</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>torch.Size([10, 5])
tensor([[  0.,   1.,   2.,   3.,   4.],
        [  5.,   6.,   7.,   8.,   9.],
        [ 10.,  11.,  12.,  13.,  14.],
        [ 15.,  16.,  17.,  18.,  19.],
        [ 20.,  21.,  22.,  23.,  24.],
        [ 25.,  26.,  27.,  28.,  29.],
        [ 30.,  31.,  32.,  33.,  34.],
        [ 35.,  36.,  37.,  38.,  39.],
        [ 40.,  41.,  42.,  43.,  44.],
        [ 45.,  46.,  47.,  48.,  49.]])

torch.Size([10])
tensor([  5.,  10.,  15.,  20.,  25.,  30.,  35.,  40.,  45.,  50.])
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<h2 id="Build-the-Neural-Network">Build the Neural Network<a class="anchor-link" href="#Build-the-Neural-Network">&#182;</a></h2><p>Implement an RNN using PyTorch's <a href="http://pytorch.org/docs/master/nn.html#torch.nn.Module">Module class</a>. You may choose to use a GRU or an LSTM. To complete the RNN, you'll have to implement the following functions for the class:</p>
<ul>
<li><code>__init__</code> - The initialize function. </li>
<li><code>init_hidden</code> - The initialization function for an LSTM/GRU hidden state</li>
<li><code>forward</code> - Forward propagation function.</li>
</ul>
<p>The initialize function should create the layers of the neural network and save them to the class. The forward propagation function will use these layers to run forward propagation and generate an output and a hidden state.</p>
<p><strong>The output of this model should be the <em>last</em> batch of word scores</strong> after a complete sequence has been processed. That is, for each input sequence of words, we only want to output the word scores for a single, most likely, next word.</p>
<h3 id="Hints">Hints<a class="anchor-link" href="#Hints">&#182;</a></h3><ol>
<li>Make sure to stack the outputs of the lstm to pass to your fully-connected layer, you can do this with <code>lstm_output = lstm_output.contiguous().view(-1, self.hidden_dim)</code></li>
<li>You can get the last batch of word scores by shaping the output of the final, fully-connected layer like so:</li>
</ol>

<pre><code># reshape into (batch_size, seq_length, output_size)
output = output.view(batch_size, -1, self.output_size)
# get last batch
out = output[:, -1]</code></pre>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[10]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">if</span> <span class="n">train_on_gpu</span><span class="p">:</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[11]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>

<span class="k">class</span> <span class="nc">RNN</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">,</span> <span class="n">n_layers</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.5</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initialize the PyTorch RNN Module</span>
<span class="sd">        :param vocab_size: The number of input dimensions of the neural network (the size of the vocabulary)</span>
<span class="sd">        :param output_size: The number of output dimensions of the neural network</span>
<span class="sd">        :param embedding_dim: The size of embeddings, should you choose to use them        </span>
<span class="sd">        :param hidden_dim: The size of the hidden layer outputs</span>
<span class="sd">        :param dropout: dropout to add in between LSTM/GRU layers</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">RNN</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="c1"># TODO: Implement function</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">output_size</span> <span class="o">=</span> <span class="n">output_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_layers</span> <span class="o">=</span> <span class="n">n_layers</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_dim</span> <span class="o">=</span> <span class="n">hidden_dim</span>
        
        <span class="c1"># embedding and LSTM layers</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lstm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LSTM</span><span class="p">(</span><span class="n">embedding_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">,</span> <span class="n">n_layers</span><span class="p">,</span> 
                            <span class="n">dropout</span><span class="o">=</span><span class="n">dropout</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        
        <span class="c1"># dropout layer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>
        
        <span class="c1"># linear and sigmoid layers</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">output_size</span><span class="p">)</span>
    
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">nn_input</span><span class="p">,</span> <span class="n">hidden</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Forward propagation of the neural network</span>
<span class="sd">        :param nn_input: The input to the neural network</span>
<span class="sd">        :param hidden: The hidden state        </span>
<span class="sd">        :return: Two Tensors, the output of the neural network and the latest hidden state</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">nn_input</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">nn_input</span> <span class="o">=</span> <span class="n">nn_input</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">nn_input</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">embeds</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span><span class="p">(</span><span class="n">nn_input</span><span class="o">.</span><span class="n">long</span><span class="p">())</span>
        <span class="n">lstm_out</span><span class="p">,</span> <span class="n">hidden</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lstm</span><span class="p">(</span><span class="n">embeds</span><span class="p">,</span> <span class="n">hidden</span><span class="p">)</span>
    
        <span class="c1"># stack up lstm outputs</span>
        <span class="n">lstm_out</span> <span class="o">=</span> <span class="n">lstm_out</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">)</span>
        
        <span class="c1"># dropout and fully-connected layer</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">lstm_out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="c1"># sigmoid function</span>
        
        <span class="c1"># reshape to be batch_size first</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">out</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_size</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">out</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="c1"># get last batch of labels</span>
        
        <span class="c1"># return last sigmoid output and hidden state</span>
        <span class="k">return</span> <span class="n">out</span><span class="p">,</span> <span class="n">hidden</span>
    
    
    <span class="k">def</span> <span class="nf">init_hidden</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
        <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">        Initialize the hidden state of an LSTM/GRU</span>
<span class="sd">        :param batch_size: The batch_size of the hidden state</span>
<span class="sd">        :return: hidden state of dims (n_layers, batch_size, hidden_dim)</span>
<span class="sd">        &#39;&#39;&#39;</span>
        <span class="c1"># Implement function</span>
        <span class="n">weight</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span><span class="o">.</span><span class="n">data</span>
        
        <span class="k">if</span> <span class="p">(</span><span class="n">train_on_gpu</span><span class="p">):</span>
            <span class="n">hidden</span> <span class="o">=</span> <span class="p">(</span><span class="n">weight</span><span class="o">.</span><span class="n">new</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_layers</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">)</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span><span class="o">.</span><span class="n">cuda</span><span class="p">(),</span>
                  <span class="n">weight</span><span class="o">.</span><span class="n">new</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_layers</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">)</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span><span class="o">.</span><span class="n">cuda</span><span class="p">())</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">hidden</span> <span class="o">=</span> <span class="p">(</span><span class="n">weight</span><span class="o">.</span><span class="n">new</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_layers</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">)</span><span class="o">.</span><span class="n">zero_</span><span class="p">(),</span>
                      <span class="n">weight</span><span class="o">.</span><span class="n">new</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_layers</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">)</span><span class="o">.</span><span class="n">zero_</span><span class="p">())</span>
        
        <span class="k">return</span> <span class="n">hidden</span>

<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">DON&#39;T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="n">tests</span><span class="o">.</span><span class="n">test_rnn</span><span class="p">(</span><span class="n">RNN</span><span class="p">,</span> <span class="n">train_on_gpu</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Tests Passed
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Define-forward-and-backpropagation">Define forward and backpropagation<a class="anchor-link" href="#Define-forward-and-backpropagation">&#182;</a></h3><p>Use the RNN class you implemented to apply forward and back propagation. This function will be called, iteratively, in the training loop as follows:</p>

<pre><code>loss = forward_back_prop(decoder, decoder_optimizer, criterion, inp, target)</code></pre>
<p>And it should return the average loss over a batch and the hidden state returned by a call to <code>RNN(inp, hidden)</code>. Recall that you can get this loss by computing it, as usual, and calling <code>loss.item()</code>.</p>
<p><strong>If a GPU is available, you should move your data to that GPU device, here.</strong></p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[12]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">forward_back_prop</span><span class="p">(</span><span class="n">rnn</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">inp</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">hidden</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Forward and backward propagation on the neural network</span>
<span class="sd">    :param decoder: The PyTorch Module that holds the neural network</span>
<span class="sd">    :param decoder_optimizer: The PyTorch optimizer for the neural network</span>
<span class="sd">    :param criterion: The PyTorch loss function</span>
<span class="sd">    :param inp: A batch of input to the neural network</span>
<span class="sd">    :param target: The target output for the batch of input</span>
<span class="sd">    :return: The loss and the latest hidden state Tensor</span>
<span class="sd">    &quot;&quot;&quot;</span>
    
    <span class="c1"># TODO: Implement Function</span>
    <span class="k">if</span><span class="p">(</span><span class="n">train_on_gpu</span><span class="p">):</span>
        <span class="n">inp</span><span class="p">,</span> <span class="n">target</span> <span class="o">=</span> <span class="n">inp</span><span class="o">.</span><span class="n">cuda</span><span class="p">(),</span> <span class="n">target</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>

    <span class="c1"># Creating new variables for the hidden state, otherwise</span>
    <span class="c1"># we&#39;d backprop through the entire training history</span>
    <span class="n">hidden</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">([</span><span class="n">each</span><span class="o">.</span><span class="n">data</span> <span class="k">for</span> <span class="n">each</span> <span class="ow">in</span> <span class="n">hidden</span><span class="p">])</span>

    <span class="c1"># zero accumulated gradients</span>
    <span class="n">rnn</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

    <span class="c1"># get the output from the model</span>
    <span class="n">output</span><span class="p">,</span> <span class="n">hidden</span> <span class="o">=</span> <span class="n">rnn</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">hidden</span><span class="p">)</span>

    <span class="c1"># calculate the loss and perform backprop</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(),</span> <span class="n">target</span><span class="o">.</span><span class="n">long</span><span class="p">())</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">loss</span><span class="o">=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">loss</span><span class="p">,</span> <span class="n">hidden</span>
<span class="c1"># Note that these tests aren&#39;t completely extensive.</span>
<span class="c1"># they are here to act as general checks on the expected outputs of your functions</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">DON&#39;T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="n">tests</span><span class="o">.</span><span class="n">test_forward_back_prop</span><span class="p">(</span><span class="n">RNN</span><span class="p">,</span> <span class="n">forward_back_prop</span><span class="p">,</span> <span class="n">train_on_gpu</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Tests Passed
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Neural-Network-Training">Neural Network Training<a class="anchor-link" href="#Neural-Network-Training">&#182;</a></h2><p>With the structure of the network complete and data ready to be fed in the neural network, it's time to train it.</p>
<h3 id="Train-Loop">Train Loop<a class="anchor-link" href="#Train-Loop">&#182;</a></h3><p>The training loop is implemented for you in the <code>train_decoder</code> function. This function will train the network over all the batches for the number of epochs given. The model progress will be shown every number of batches. This number is set with the <code>show_every_n_batches</code> parameter. You'll set this parameter along with other parameters in the next section.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[13]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">DON&#39;T MODIFY ANYTHING IN THIS CELL</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="k">def</span> <span class="nf">train_rnn</span><span class="p">(</span><span class="n">rnn</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">n_epochs</span><span class="p">,</span> <span class="n">show_every_n_batches</span><span class="o">=</span><span class="mi">100</span><span class="p">):</span>
    <span class="n">batch_losses</span> <span class="o">=</span> <span class="p">[]</span>
    
    <span class="n">rnn</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Training for </span><span class="si">%d</span><span class="s2"> epoch(s)...&quot;</span> <span class="o">%</span> <span class="n">n_epochs</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">epoch_i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_epochs</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
        
        <span class="c1"># initialize hidden state</span>
        <span class="n">hidden</span> <span class="o">=</span> <span class="n">rnn</span><span class="o">.</span><span class="n">init_hidden</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span>
        
        <span class="k">for</span> <span class="n">batch_i</span><span class="p">,</span> <span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_loader</span><span class="p">,</span> <span class="mi">1</span><span class="p">):</span>
            
            <span class="c1"># make sure you iterate over completely full batches, only</span>
            <span class="n">n_batches</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_loader</span><span class="o">.</span><span class="n">dataset</span><span class="p">)</span><span class="o">//</span><span class="n">batch_size</span>
            <span class="k">if</span><span class="p">(</span><span class="n">batch_i</span> <span class="o">&gt;</span> <span class="n">n_batches</span><span class="p">):</span>
                <span class="k">break</span>
            
            <span class="c1"># forward, back prop</span>
            <span class="n">loss</span><span class="p">,</span> <span class="n">hidden</span> <span class="o">=</span> <span class="n">forward_back_prop</span><span class="p">(</span><span class="n">rnn</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">hidden</span><span class="p">)</span>          
            <span class="c1"># record loss</span>
            <span class="n">batch_losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>

            <span class="c1"># printing loss stats</span>
            <span class="k">if</span> <span class="n">batch_i</span> <span class="o">%</span> <span class="n">show_every_n_batches</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Epoch: </span><span class="si">{:&gt;4}</span><span class="s1">/</span><span class="si">{:&lt;4}</span><span class="s1">  Loss: </span><span class="si">{}</span><span class="se">\n</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                    <span class="n">epoch_i</span><span class="p">,</span> <span class="n">n_epochs</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">average</span><span class="p">(</span><span class="n">batch_losses</span><span class="p">)))</span>
                <span class="n">batch_losses</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="c1"># returns a trained rnn</span>
    <span class="k">return</span> <span class="n">rnn</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Hyperparameters">Hyperparameters<a class="anchor-link" href="#Hyperparameters">&#182;</a></h3><p>Set and train the neural network with the following parameters:</p>
<ul>
<li>Set <code>sequence_length</code> to the length of a sequence.</li>
<li>Set <code>batch_size</code> to the batch size.</li>
<li>Set <code>num_epochs</code> to the number of epochs to train for.</li>
<li>Set <code>learning_rate</code> to the learning rate for an Adam optimizer.</li>
<li>Set <code>vocab_size</code> to the number of uniqe tokens in our vocabulary.</li>
<li>Set <code>output_size</code> to the desired size of the output.</li>
<li>Set <code>embedding_dim</code> to the embedding dimension; smaller than the vocab_size.</li>
<li>Set <code>hidden_dim</code> to the hidden dimension of your RNN.</li>
<li>Set <code>n_layers</code> to the number of layers/cells in your RNN.</li>
<li>Set <code>show_every_n_batches</code> to the number of batches at which the neural network should print progress.</li>
</ul>
<p>If the network isn't getting the desired results, tweak these parameters and/or the layers in the <code>RNN</code> class.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[14]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Data params</span>
<span class="c1"># Sequence Length</span>
<span class="n">sequence_length</span> <span class="o">=</span> <span class="mi">16</span>  <span class="c1"># of words in a sequence</span>
<span class="c1"># Batch Size</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">64</span>

<span class="c1"># data loader - do not change</span>
<span class="n">train_loader</span> <span class="o">=</span> <span class="n">batch_data</span><span class="p">(</span><span class="n">int_text</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[15]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Training parameters</span>

<span class="c1"># Number of Epochs</span>
<span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">3000</span>
<span class="c1"># Learning Rate</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.001</span>



<span class="c1"># Model parameters</span>
<span class="c1"># Vocab size</span>
<span class="n">vocab_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">vocab_to_int</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">)</span>
<span class="c1"># Output size</span>
<span class="n">output_size</span> <span class="o">=</span> <span class="n">vocab_size</span>

<span class="c1"># Embedding Dimension</span>
<span class="n">embedding_dim</span> <span class="o">=</span> <span class="mi">300</span>
<span class="c1"># Hidden Dimension</span>
<span class="n">hidden_dim</span> <span class="o">=</span> <span class="mi">5</span><span class="o">*</span><span class="nb">int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">train_loader</span><span class="o">.</span><span class="n">dataset</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">embedding_dim</span> <span class="o">+</span> <span class="n">output_size</span><span class="p">))</span>
<span class="c1"># Number of RNN Layers</span>
<span class="n">n_layers</span> <span class="o">=</span> <span class="mi">2</span>

<span class="c1"># Show stats for every n number of batches</span>
<span class="n">show_every_n_batches</span> <span class="o">=</span> <span class="mi">500</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>21388
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Train">Train<a class="anchor-link" href="#Train">&#182;</a></h3><p>In the next cell, you'll train the neural network on the pre-processed data.  If you have a hard time getting a good loss, you may consider changing your hyperparameters. In general, you may get better results with larger hidden and n_layer dimensions, but larger models take a longer time to train.</p>
<blockquote><p><strong>You should aim for a loss less than 3.5.</strong></p>
</blockquote>
<p>You should also experiment with different sequence lengths, which determine the size of the long range dependencies that a model can learn.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[16]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">workspace_utils</span> <span class="k">import</span> <span class="n">active_session</span>

<span class="k">with</span> <span class="n">active_session</span><span class="p">():</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    DON&#39;T MODIFY ANYTHING IN THIS CELL</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;sequence_length: </span><span class="si">{}</span><span class="se">\n</span><span class="s1">batch_size: </span><span class="si">{}</span><span class="se">\n</span><span class="s1">learning_rate: </span><span class="si">{}</span><span class="se">\n</span><span class="s1">embedding_dim: </span><span class="si">{}</span><span class="se">\n</span><span class="s1">n_layers: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">sequence_length</span><span class="p">,</span><span class="n">batch_size</span><span class="p">,</span><span class="n">learning_rate</span><span class="p">,</span><span class="n">embedding_dim</span><span class="p">,</span><span class="n">n_layers</span><span class="p">))</span>

    <span class="c1"># create model and move to gpu if available</span>
    <span class="n">rnn</span> <span class="o">=</span> <span class="n">RNN</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">,</span> <span class="n">n_layers</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">train_on_gpu</span><span class="p">:</span>
        <span class="n">rnn</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>

        
    <span class="c1"># defining loss and optimization functions for training</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">rnn</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>
    <span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>

    <span class="c1"># training the model</span>
    <span class="n">trained_rnn</span> <span class="o">=</span> <span class="n">train_rnn</span><span class="p">(</span><span class="n">rnn</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">num_epochs</span><span class="p">,</span> <span class="n">show_every_n_batches</span><span class="p">)</span>

    <span class="c1"># saving the trained model</span>
    <span class="n">helper</span><span class="o">.</span><span class="n">save_model</span><span class="p">(</span><span class="s1">&#39;/trained_rnn&#39;</span><span class="p">,</span> <span class="n">trained_rnn</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Model Trained and Saved&#39;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;sequence_length: </span><span class="si">{}</span><span class="se">\n</span><span class="s1">batch_size: </span><span class="si">{}</span><span class="se">\n</span><span class="s1">learning_rate: </span><span class="si">{}</span><span class="se">\n</span><span class="s1">embedding_dim: </span><span class="si">{}</span><span class="se">\n</span><span class="s1">n_layers: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">sequence_length</span><span class="p">,</span><span class="n">batch_size</span><span class="p">,</span><span class="n">learning_rate</span><span class="p">,</span><span class="n">embedding_dim</span><span class="p">,</span><span class="n">n_layers</span><span class="p">))</span>
    
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>sequence_length: 16
batch_size: 64
learning_rate: 0.001
embedding_dim: 300
n_layers: 2
Training for 3000 epoch(s)...
Epoch:    1/3000  Loss: 7.6355538091659545

Epoch:    2/3000  Loss: 5.872751377753905

Epoch:    3/3000  Loss: 5.481700065078472

Epoch:    4/3000  Loss: 5.317049304051186

Epoch:    5/3000  Loss: 5.214368463246886

Epoch:    6/3000  Loss: 5.136035258405928

Epoch:    7/3000  Loss: 5.074308214724132

Epoch:    8/3000  Loss: 5.017919189210053

Epoch:    9/3000  Loss: 4.969350996849594

Epoch:   10/3000  Loss: 4.921557827192935

Epoch:   11/3000  Loss: 4.872283209615679

Epoch:   12/3000  Loss: 4.833799463188881

Epoch:   13/3000  Loss: 4.794799154579434

Epoch:   14/3000  Loss: 4.755663197021397

Epoch:   15/3000  Loss: 4.725045164198059

Epoch:   16/3000  Loss: 4.682623392405111

Epoch:   17/3000  Loss: 4.658289793301395

Epoch:   18/3000  Loss: 4.627390000083827

Epoch:   19/3000  Loss: 4.602226859803315

Epoch:   20/3000  Loss: 4.580942588339717

Epoch:   21/3000  Loss: 4.55227924844017

Epoch:   22/3000  Loss: 4.53264666315335

Epoch:   23/3000  Loss: 4.505622236654761

Epoch:   24/3000  Loss: 4.479394188704639

Epoch:   25/3000  Loss: 4.4576100495597935

Epoch:   26/3000  Loss: 4.439693826758629

Epoch:   27/3000  Loss: 4.427071519067723

Epoch:   28/3000  Loss: 4.409994388420464

Epoch:   29/3000  Loss: 4.391752436021439

Epoch:   30/3000  Loss: 4.382842402781192

Epoch:   31/3000  Loss: 4.357310280323576

Epoch:   32/3000  Loss: 4.3498811256461245

Epoch:   33/3000  Loss: 4.332105528199714

Epoch:   34/3000  Loss: 4.321317151821851

Epoch:   35/3000  Loss: 4.31847866148133

Epoch:   36/3000  Loss: 4.302644068009544

Epoch:   37/3000  Loss: 4.284996544863135

Epoch:   38/3000  Loss: 4.266129225458261

Epoch:   39/3000  Loss: 4.2658959423222855

Epoch:   40/3000  Loss: 4.251116350241835

Epoch:   41/3000  Loss: 4.239287965743879

Epoch:   42/3000  Loss: 4.22665862812651

Epoch:   43/3000  Loss: 4.22709003628053

Epoch:   44/3000  Loss: 4.2101149444054515

Epoch:   45/3000  Loss: 4.199838653087069

Epoch:   46/3000  Loss: 4.192711698201439

Epoch:   47/3000  Loss: 4.179716717909453

Epoch:   48/3000  Loss: 4.181807770110436

Epoch:   49/3000  Loss: 4.171543099986973

Epoch:   50/3000  Loss: 4.162529261596715

Epoch:   51/3000  Loss: 4.163083538532804

Epoch:   52/3000  Loss: 4.147664453077262

Epoch:   53/3000  Loss: 4.135192471174641

Epoch:   54/3000  Loss: 4.139867649560133

Epoch:   55/3000  Loss: 4.127074156234514

Epoch:   56/3000  Loss: 4.119268458691585

Epoch:   57/3000  Loss: 4.113812204069713

Epoch:   58/3000  Loss: 4.106934010640352

Epoch:   59/3000  Loss: 4.101315905114403

Epoch:   60/3000  Loss: 4.09491218600837

Epoch:   61/3000  Loss: 4.081751660282385

Epoch:   62/3000  Loss: 4.087286676796651

Epoch:   63/3000  Loss: 4.078391433446471

Epoch:   64/3000  Loss: 4.06911633209027

Epoch:   65/3000  Loss: 4.076401785512195

Epoch:   66/3000  Loss: 4.069477585508958

Epoch:   67/3000  Loss: 4.06018718238813

Epoch:   68/3000  Loss: 4.05990914663409

Epoch:   69/3000  Loss: 4.057631148536498

Epoch:   70/3000  Loss: 4.0495500022590365

Epoch:   71/3000  Loss: 4.041576228924652

Epoch:   72/3000  Loss: 4.027919913542668

Epoch:   73/3000  Loss: 4.027322305466907

Epoch:   74/3000  Loss: 4.01657907888892

Epoch:   75/3000  Loss: 4.013147330037762

Epoch:   76/3000  Loss: 4.005029890485253

Epoch:   77/3000  Loss: 4.007621892694765

Epoch:   78/3000  Loss: 3.9957108081550468

Epoch:   79/3000  Loss: 3.985672130924142

Epoch:   80/3000  Loss: 3.987260294562777

Epoch:   81/3000  Loss: 3.9825166933298384

Epoch:   82/3000  Loss: 3.9753097020662747

Epoch:   83/3000  Loss: 3.9595497568767737

Epoch:   84/3000  Loss: 3.9675202925361255

Epoch:   85/3000  Loss: 3.9552082769085155

Epoch:   86/3000  Loss: 3.936468757800202

Epoch:   87/3000  Loss: 3.941148822534782

Epoch:   88/3000  Loss: 3.937548165480113

Epoch:   89/3000  Loss: 3.9449563166030375

Epoch:   90/3000  Loss: 3.937874370820213

Epoch:   91/3000  Loss: 3.9224700005265247

Epoch:   92/3000  Loss: 3.923319116662482

Epoch:   93/3000  Loss: 3.9194009019367733

Epoch:   94/3000  Loss: 3.9172534219980513

Epoch:   95/3000  Loss: 3.9126559839456694

Epoch:   96/3000  Loss: 3.903285681859223

Epoch:   97/3000  Loss: 3.8939581631792946

Epoch:   98/3000  Loss: 3.887510957728845

Epoch:   99/3000  Loss: 3.880120292733649

Epoch:  100/3000  Loss: 3.8915654216924027

Epoch:  101/3000  Loss: 3.8737555694361094

Epoch:  102/3000  Loss: 3.8726520190693345

Epoch:  103/3000  Loss: 3.865498155458102

Epoch:  104/3000  Loss: 3.87077936622497

Epoch:  105/3000  Loss: 3.8582198729334416

Epoch:  106/3000  Loss: 3.856822466330194

Epoch:  107/3000  Loss: 3.8648980235943973

Epoch:  108/3000  Loss: 3.8530231847007568

Epoch:  109/3000  Loss: 3.8520502116184696

Epoch:  110/3000  Loss: 3.8336855940922803

Epoch:  111/3000  Loss: 3.8498043592433295

Epoch:  112/3000  Loss: 3.8332481718227593

Epoch:  113/3000  Loss: 3.848203454034063

Epoch:  114/3000  Loss: 3.8411508805442485

Epoch:  115/3000  Loss: 3.838260666237359

Epoch:  116/3000  Loss: 3.840270163681696

Epoch:  117/3000  Loss: 3.8315340769140374

Epoch:  118/3000  Loss: 3.8280205102519793

Epoch:  119/3000  Loss: 3.821556271422744

Epoch:  120/3000  Loss: 3.828320666925231

Epoch:  121/3000  Loss: 3.8094010016400834

Epoch:  122/3000  Loss: 3.814698585823401

Epoch:  123/3000  Loss: 3.8214998286298716

Epoch:  124/3000  Loss: 3.8088876628985227

Epoch:  125/3000  Loss: 3.8034538185281677

Epoch:  126/3000  Loss: 3.8058403386861666

Epoch:  127/3000  Loss: 3.809148554139515

Epoch:  128/3000  Loss: 3.7996848573914623

Epoch:  129/3000  Loss: 3.8004077072121656

Epoch:  130/3000  Loss: 3.8123187794340465

Epoch:  131/3000  Loss: 3.793921311605127

Epoch:  132/3000  Loss: 3.799229182824749

Epoch:  133/3000  Loss: 3.795306928669681

Epoch:  134/3000  Loss: 3.7957734911094168

Epoch:  135/3000  Loss: 3.799686881075223

Epoch:  136/3000  Loss: 3.79640315840904

Epoch:  137/3000  Loss: 3.7951233783901492

Epoch:  138/3000  Loss: 3.7836799846040394

Epoch:  139/3000  Loss: 3.7909407662195673

Epoch:  140/3000  Loss: 3.7845409201152287

Epoch:  141/3000  Loss: 3.792973705602705

Epoch:  142/3000  Loss: 3.794135714234222

Epoch:  143/3000  Loss: 3.7836579567529305

Epoch:  144/3000  Loss: 3.8015310534379787

Epoch:  145/3000  Loss: 3.779812809794149

Epoch:  146/3000  Loss: 3.7868884743012736

Epoch:  147/3000  Loss: 3.7887283723745773

Epoch:  148/3000  Loss: 3.771538744016575

Epoch:  149/3000  Loss: 3.778890068030111

Epoch:  150/3000  Loss: 3.7712255753277364

Epoch:  151/3000  Loss: 3.7660685844727966

Epoch:  152/3000  Loss: 3.7680318257017604

Epoch:  153/3000  Loss: 3.7669607040939592

Epoch:  154/3000  Loss: 3.777252448002041

Epoch:  155/3000  Loss: 3.7634924260127423

Epoch:  156/3000  Loss: 3.7588567840519236

Epoch:  157/3000  Loss: 3.7571267788774247

Epoch:  158/3000  Loss: 3.76150694670825

Epoch:  159/3000  Loss: 3.751473484575817

Epoch:  160/3000  Loss: 3.763322501724541

Epoch:  161/3000  Loss: 3.754546419761211

Epoch:  162/3000  Loss: 3.74982549429761

Epoch:  163/3000  Loss: 3.7450540840420468

Epoch:  164/3000  Loss: 3.7373570845677304

Epoch:  165/3000  Loss: 3.7429710121570854

Epoch:  166/3000  Loss: 3.736639631602028

Epoch:  167/3000  Loss: 3.740530849187437

Epoch:  168/3000  Loss: 3.747180369362902

Epoch:  169/3000  Loss: 3.756392929228247

Epoch:  170/3000  Loss: 3.7435525640691325

Epoch:  171/3000  Loss: 3.739046570901619

Epoch:  172/3000  Loss: 3.73847646171272

Epoch:  173/3000  Loss: 3.7396285167107215

Epoch:  174/3000  Loss: 3.746525567927399

Epoch:  175/3000  Loss: 3.7479242027558635

Epoch:  176/3000  Loss: 3.737463758952582

Epoch:  177/3000  Loss: 3.733184804598855

Epoch:  178/3000  Loss: 3.7206158851509663

Epoch:  179/3000  Loss: 3.728516883061756

Epoch:  180/3000  Loss: 3.72805857904743

Epoch:  181/3000  Loss: 3.7239306736758877

Epoch:  182/3000  Loss: 3.7288434297428066

Epoch:  183/3000  Loss: 3.7175850493215394

Epoch:  184/3000  Loss: 3.7241887931298168

Epoch:  185/3000  Loss: 3.713827834200503

Epoch:  186/3000  Loss: 3.7234892763034897

Epoch:  187/3000  Loss: 3.712220338674692

Epoch:  188/3000  Loss: 3.7079964677720887

Epoch:  189/3000  Loss: 3.7096273816959457

Epoch:  190/3000  Loss: 3.699774859009052

Epoch:  191/3000  Loss: 3.712533006164428

Epoch:  192/3000  Loss: 3.7019353545218467

Epoch:  193/3000  Loss: 3.7120041220387963

Epoch:  194/3000  Loss: 3.719202084875271

Epoch:  195/3000  Loss: 3.7130622907566284

Epoch:  196/3000  Loss: 3.699062436921725

Epoch:  197/3000  Loss: 3.6994633097051888

Epoch:  198/3000  Loss: 3.701547770768438

Epoch:  199/3000  Loss: 3.700199755133225

Epoch:  200/3000  Loss: 3.7100902304446794

Epoch:  201/3000  Loss: 3.7019210819809363

Epoch:  202/3000  Loss: 3.7027087364897526

Epoch:  203/3000  Loss: 3.6994166817648675

Epoch:  204/3000  Loss: 3.701427973507466

Epoch:  205/3000  Loss: 3.6947010047947635

Epoch:  206/3000  Loss: 3.7076718155743746

Epoch:  207/3000  Loss: 3.704323507203848

Epoch:  208/3000  Loss: 3.701274918907682

Epoch:  209/3000  Loss: 3.6845223200170647

Epoch:  210/3000  Loss: 3.6944584227867434

Epoch:  211/3000  Loss: 3.701456821608078

Epoch:  212/3000  Loss: 3.691392773191362

Epoch:  213/3000  Loss: 3.689729120099038

Epoch:  214/3000  Loss: 3.702807760676615

Epoch:  215/3000  Loss: 3.695364560653914

Epoch:  216/3000  Loss: 3.6954646720951927

Epoch:  217/3000  Loss: 3.6767994333764307

Epoch:  218/3000  Loss: 3.6827587824053936

Epoch:  219/3000  Loss: 3.6892034810391414

Epoch:  220/3000  Loss: 3.6953349020396256

Epoch:  221/3000  Loss: 3.6861885787425166

Epoch:  222/3000  Loss: 3.684820875918961

Epoch:  223/3000  Loss: 3.6961370300069607

Epoch:  224/3000  Loss: 3.686422803782431

Epoch:  225/3000  Loss: 3.6858464313839936

Epoch:  226/3000  Loss: 3.681641623839719

Epoch:  227/3000  Loss: 3.6919950814800067

Epoch:  228/3000  Loss: 3.677726082358377

Epoch:  229/3000  Loss: 3.679933958020741

Epoch:  230/3000  Loss: 3.68005850969308

Epoch:  231/3000  Loss: 3.677655913936558

Epoch:  232/3000  Loss: 3.6834781531214578

Epoch:  233/3000  Loss: 3.6784220026500187

Epoch:  234/3000  Loss: 3.6737303208264636

Epoch:  235/3000  Loss: 3.6780150100913853

Epoch:  236/3000  Loss: 3.6723266162500043

Epoch:  237/3000  Loss: 3.685039223540664

Epoch:  238/3000  Loss: 3.6788934552710586

Epoch:  239/3000  Loss: 3.6739638070698857

Epoch:  240/3000  Loss: 3.678579415026817

Epoch:  241/3000  Loss: 3.673722964340455

Epoch:  242/3000  Loss: 3.6538370433001512

Epoch:  243/3000  Loss: 3.667137620095955

Epoch:  244/3000  Loss: 3.6564047424171875

Epoch:  245/3000  Loss: 3.6684983214609383

Epoch:  246/3000  Loss: 3.672179375121843

Epoch:  247/3000  Loss: 3.6579684732154645

Epoch:  248/3000  Loss: 3.6805286492326914

Epoch:  249/3000  Loss: 3.6782971200384584

Epoch:  250/3000  Loss: 3.670939639843701

Epoch:  251/3000  Loss: 3.6658761514998743

Epoch:  252/3000  Loss: 3.6501734035023317

Epoch:  253/3000  Loss: 3.6564493770577466

Epoch:  254/3000  Loss: 3.6696493743344645

Epoch:  255/3000  Loss: 3.667765654871576

Epoch:  256/3000  Loss: 3.658406995604698

Epoch:  257/3000  Loss: 3.6535077226422

Epoch:  258/3000  Loss: 3.667726528220281

Epoch:  259/3000  Loss: 3.65720980942044

Epoch:  260/3000  Loss: 3.673902234855941

Epoch:  261/3000  Loss: 3.660055188169162

Epoch:  262/3000  Loss: 3.6750576821862624

Epoch:  263/3000  Loss: 3.641744640199243

Epoch:  264/3000  Loss: 3.6777988503912695

Epoch:  265/3000  Loss: 3.6552371809047344

Epoch:  266/3000  Loss: 3.6662587218936085

Epoch:  267/3000  Loss: 3.6498187877554296

Epoch:  268/3000  Loss: 3.6602806574167808

Epoch:  269/3000  Loss: 3.670715114689859

Epoch:  270/3000  Loss: 3.650249367877071

Epoch:  271/3000  Loss: 3.6475574324243243

Epoch:  272/3000  Loss: 3.660327086629331

Epoch:  273/3000  Loss: 3.65528221792797

Epoch:  274/3000  Loss: 3.642347175683548

Epoch:  275/3000  Loss: 3.65124418595902

Epoch:  276/3000  Loss: 3.6607084309876856

Epoch:  277/3000  Loss: 3.660233753830639

Epoch:  278/3000  Loss: 3.6322350269343904

Epoch:  279/3000  Loss: 3.6596916069530043

Epoch:  280/3000  Loss: 3.6594223226115843

Epoch:  281/3000  Loss: 3.644934628231517

Epoch:  282/3000  Loss: 3.632676902786325

Epoch:  283/3000  Loss: 3.632830217429335

Epoch:  284/3000  Loss: 3.649832654628907

Epoch:  285/3000  Loss: 3.6570107999816965

Epoch:  286/3000  Loss: 3.637637951070214

Epoch:  287/3000  Loss: 3.6395408324341276

Epoch:  288/3000  Loss: 3.6413554627914517

Epoch:  289/3000  Loss: 3.6360474292501106

Epoch:  290/3000  Loss: 3.6400850933263276

Epoch:  291/3000  Loss: 3.6282759328660408

Epoch:  292/3000  Loss: 3.6257467899475797

Epoch:  293/3000  Loss: 3.624702487008306

Epoch:  294/3000  Loss: 3.619450232465286

Epoch:  295/3000  Loss: 3.6289542610965557

Epoch:  296/3000  Loss: 3.634943945125926

Epoch:  297/3000  Loss: 3.6145044626789993

Epoch:  298/3000  Loss: 3.626890371642348

Epoch:  299/3000  Loss: 3.6184832376399076

Epoch:  300/3000  Loss: 3.623962561654442

Epoch:  301/3000  Loss: 3.612314407094612

Epoch:  302/3000  Loss: 3.6252256096709607

Epoch:  303/3000  Loss: 3.626628890787556

Epoch:  304/3000  Loss: 3.6142673678567845

Epoch:  305/3000  Loss: 3.621472502685442

Epoch:  306/3000  Loss: 3.6115721528483586

Epoch:  307/3000  Loss: 3.619524688589313

Epoch:  308/3000  Loss: 3.603705290127567

Epoch:  309/3000  Loss: 3.615219242947794

Epoch:  310/3000  Loss: 3.6041401530242814

Epoch:  311/3000  Loss: 3.6058946252553525

Epoch:  312/3000  Loss: 3.614011541436652

Epoch:  313/3000  Loss: 3.613304207437211

Epoch:  314/3000  Loss: 3.6050071754630753

Epoch:  315/3000  Loss: 3.605152354311587

Epoch:  316/3000  Loss: 3.628984772652627

Epoch:  317/3000  Loss: 3.6112954052663424

Epoch:  318/3000  Loss: 3.6277780768244465

Epoch:  319/3000  Loss: 3.617909319775797

Epoch:  320/3000  Loss: 3.603413421989993

Epoch:  321/3000  Loss: 3.6042482965164973

Epoch:  322/3000  Loss: 3.6198652743198294

Epoch:  323/3000  Loss: 3.604455994136298

Epoch:  324/3000  Loss: 3.6015404631158106

Epoch:  325/3000  Loss: 3.5977183860971516

Epoch:  326/3000  Loss: 3.5890011987237243

Epoch:  327/3000  Loss: 3.599828493444572

Epoch:  328/3000  Loss: 3.6031102723560706

Epoch:  329/3000  Loss: 3.6022490551770074

Epoch:  330/3000  Loss: 3.5981697154784174

Epoch:  331/3000  Loss: 3.607017851587552

Epoch:  332/3000  Loss: 3.601135285385167

Epoch:  333/3000  Loss: 3.5986827301239996

Epoch:  334/3000  Loss: 3.59891590458928

Epoch:  335/3000  Loss: 3.587709214739356

Epoch:  336/3000  Loss: 3.595670572241467

Epoch:  337/3000  Loss: 3.59317884680598

Epoch:  338/3000  Loss: 3.615399383923764

Epoch:  339/3000  Loss: 3.608005574595367

Epoch:  340/3000  Loss: 3.6044800864568947

Epoch:  341/3000  Loss: 3.5869129757930436

Epoch:  342/3000  Loss: 3.5798618579978374

Epoch:  343/3000  Loss: 3.5831943773648494

Epoch:  344/3000  Loss: 3.5952434175734407

Epoch:  345/3000  Loss: 3.5732175934328416

Epoch:  346/3000  Loss: 3.5874727886388276

Epoch:  347/3000  Loss: 3.587001346693247

Epoch:  348/3000  Loss: 3.5875935792649245

Epoch:  349/3000  Loss: 3.5886560876936096

Epoch:  350/3000  Loss: 3.5591725840497372

Epoch:  351/3000  Loss: 3.5898767702888814

Epoch:  352/3000  Loss: 3.583470082584123

Epoch:  353/3000  Loss: 3.5944873856895962

Epoch:  354/3000  Loss: 3.5859591161616224

Epoch:  355/3000  Loss: 3.605267008738731

Epoch:  356/3000  Loss: 3.5941864423171523

Epoch:  357/3000  Loss: 3.5947636030571606

Epoch:  358/3000  Loss: 3.590698202222962

Epoch:  359/3000  Loss: 3.5744207710130342

Epoch:  360/3000  Loss: 3.5960548351060098

Epoch:  361/3000  Loss: 3.5927972199585625

Epoch:  362/3000  Loss: 3.5729627017996752

Epoch:  363/3000  Loss: 3.6017272226024852

Epoch:  364/3000  Loss: 3.5772069204004158

Epoch:  365/3000  Loss: 3.596984598858348

Epoch:  366/3000  Loss: 3.664220422608981

Epoch:  367/3000  Loss: 3.616625527153059

Epoch:  368/3000  Loss: 3.605036434979444

Epoch:  369/3000  Loss: 3.615085325887091

Epoch:  370/3000  Loss: 3.5950753488989564

Epoch:  371/3000  Loss: 3.6074559108261126

Epoch:  372/3000  Loss: 3.6073093433467722

Epoch:  373/3000  Loss: 3.5871566707860727

Epoch:  374/3000  Loss: 3.5985610372847723

Epoch:  375/3000  Loss: 3.604059658970269

Epoch:  376/3000  Loss: 3.59325209249722

Epoch:  377/3000  Loss: 3.5980642739721884

Epoch:  378/3000  Loss: 3.5968604424517134

Epoch:  379/3000  Loss: 3.6006741578488617

Epoch:  380/3000  Loss: 3.593673915457917

Epoch:  381/3000  Loss: 3.588627263954942

Epoch:  382/3000  Loss: 3.5876410560410825

Epoch:  383/3000  Loss: 3.605088555306436

Epoch:  384/3000  Loss: 3.632507322302642

Epoch:  385/3000  Loss: 3.5857353202237325

Epoch:  386/3000  Loss: 3.591164227878733

Epoch:  387/3000  Loss: 3.575618731031188

Epoch:  388/3000  Loss: 3.5972307631671088

Epoch:  389/3000  Loss: 3.591535374710398

Epoch:  390/3000  Loss: 3.5625956488257846

Epoch:  391/3000  Loss: 3.557676534017657

Epoch:  392/3000  Loss: 3.5718678801259545

Epoch:  393/3000  Loss: 3.5650048732210107

Epoch:  394/3000  Loss: 3.554128958628728

Epoch:  395/3000  Loss: 3.577748378026636

Epoch:  396/3000  Loss: 3.5766640859137446

Epoch:  397/3000  Loss: 3.5702704162466268

Epoch:  398/3000  Loss: 3.556004632627923

Epoch:  399/3000  Loss: 3.573356644294292

Epoch:  400/3000  Loss: 3.5557166476200424

Epoch:  401/3000  Loss: 3.569018350266151

Epoch:  402/3000  Loss: 3.5664157823634888

Epoch:  403/3000  Loss: 3.554896818647155

Epoch:  404/3000  Loss: 3.5727198277220524

Epoch:  405/3000  Loss: 3.5521080321477285

Epoch:  406/3000  Loss: 3.560342881764391

Epoch:  407/3000  Loss: 3.552807095677653

Epoch:  408/3000  Loss: 3.5878766212616666

Epoch:  409/3000  Loss: 3.5786485028732247

Epoch:  410/3000  Loss: 3.5650895213971316

Epoch:  411/3000  Loss: 3.5725190215214795

Epoch:  412/3000  Loss: 3.561772659369643

Epoch:  413/3000  Loss: 3.5752642061625504

Epoch:  414/3000  Loss: 3.5569255664072137

Epoch:  415/3000  Loss: 3.561979891648933

Epoch:  416/3000  Loss: 3.5621447103305472

Epoch:  417/3000  Loss: 3.571746490852978

Epoch:  418/3000  Loss: 3.5763789901229734

Epoch:  419/3000  Loss: 3.5706090053926243

Epoch:  420/3000  Loss: 3.559589405147408

Epoch:  421/3000  Loss: 3.5715358607667733

Epoch:  422/3000  Loss: 3.561494393682644

Epoch:  423/3000  Loss: 3.5531741409433146

Epoch:  424/3000  Loss: 3.565502559002724

Epoch:  425/3000  Loss: 3.5532697363918055

Epoch:  426/3000  Loss: 3.5480971292567993

Epoch:  427/3000  Loss: 3.5779647772402496

Epoch:  428/3000  Loss: 3.55871552577933

Epoch:  429/3000  Loss: 3.56351916278134

Epoch:  430/3000  Loss: 3.546486750809662

Epoch:  431/3000  Loss: 3.5413443544017733

Epoch:  432/3000  Loss: 3.558157120177995

Epoch:  433/3000  Loss: 3.5595684741312548

Epoch:  434/3000  Loss: 3.547656150143401

Epoch:  435/3000  Loss: 3.5674082228291595

Epoch:  436/3000  Loss: 3.5350971794018924

Epoch:  437/3000  Loss: 3.558782922961545

Epoch:  438/3000  Loss: 3.5362268205898912

Epoch:  439/3000  Loss: 3.531952107951781

Epoch:  440/3000  Loss: 3.5555610292677766

Epoch:  441/3000  Loss: 3.5667094754570523

Epoch:  442/3000  Loss: 3.5663098495124266

Epoch:  443/3000  Loss: 3.561892540939366

Epoch:  444/3000  Loss: 3.5532313061088976

Epoch:  445/3000  Loss: 3.5403967708451876

Epoch:  446/3000  Loss: 3.5296664889452787

Epoch:  447/3000  Loss: 3.535299422683453

Epoch:  448/3000  Loss: 3.514615666852616

Epoch:  449/3000  Loss: 3.548184687453488

Epoch:  450/3000  Loss: 3.533551260195972

Epoch:  451/3000  Loss: 3.5262054006486756

Epoch:  452/3000  Loss: 3.5510924793138297

Epoch:  453/3000  Loss: 3.553667477981094

Epoch:  454/3000  Loss: 3.5238589517832075

Epoch:  455/3000  Loss: 3.5060375453957735

Epoch:  456/3000  Loss: 3.514926423161503

Epoch:  457/3000  Loss: 3.5414602258312167

Epoch:  458/3000  Loss: 3.511610529042276

Epoch:  459/3000  Loss: 3.5390173690874733

Epoch:  460/3000  Loss: 3.5061571565205414

Epoch:  461/3000  Loss: 3.512908122295901

Epoch:  462/3000  Loss: 3.512383894312669

Epoch:  463/3000  Loss: 3.5119782052549238

Epoch:  464/3000  Loss: 3.5311331215189465

Epoch:  465/3000  Loss: 3.507468158424106

Epoch:  466/3000  Loss: 3.4969628555219017

Epoch:  467/3000  Loss: 3.492689016629032

Epoch:  468/3000  Loss: 3.5232166803252682

Epoch:  469/3000  Loss: 3.515441171337351

Epoch:  470/3000  Loss: 3.5124526574061465

Epoch:  471/3000  Loss: 3.5124057688751398

Epoch:  472/3000  Loss: 3.508360224941705

Epoch:  473/3000  Loss: 3.5147593492207925

Epoch:  474/3000  Loss: 3.5048418006721787

Epoch:  475/3000  Loss: 3.517061570755513

Epoch:  476/3000  Loss: 3.5070923724212824

Epoch:  477/3000  Loss: 3.5245226483394303

Epoch:  478/3000  Loss: 3.5163898199762462

Epoch:  479/3000  Loss: 3.512623309541652

Epoch:  480/3000  Loss: 3.5127357322504547

Epoch:  481/3000  Loss: 3.513789514997112

Epoch:  482/3000  Loss: 3.515455709396082

Epoch:  483/3000  Loss: 3.495941681922098

Epoch:  484/3000  Loss: 3.487647842457593

Epoch:  485/3000  Loss: 3.5657649623266456

Epoch:  486/3000  Loss: 3.5374003135514727

Epoch:  487/3000  Loss: 3.5566238464635767

Epoch:  488/3000  Loss: 3.536592508703641

Epoch:  489/3000  Loss: 3.537467086629944

Epoch:  490/3000  Loss: 3.531610890453089

Epoch:  491/3000  Loss: 3.566243031542282

Epoch:  492/3000  Loss: 3.552729434452429

Epoch:  493/3000  Loss: 3.539461803764755

Epoch:  494/3000  Loss: 3.5369297070289725

Epoch:  495/3000  Loss: 3.546084949808745

Epoch:  496/3000  Loss: 3.525839089252371

Epoch:  497/3000  Loss: 3.5312906784250324

Epoch:  498/3000  Loss: 3.5128510871519314

Epoch:  499/3000  Loss: 3.5185899206198727

Epoch:  500/3000  Loss: 3.533271256051573

Epoch:  501/3000  Loss: 3.5124259411125314

Epoch:  502/3000  Loss: 3.533655936901386

Epoch:  503/3000  Loss: 3.5125528912046065

Epoch:  504/3000  Loss: 3.5176052480559674

Epoch:  505/3000  Loss: 3.504037793550371

Epoch:  506/3000  Loss: 3.524258256095422

Epoch:  507/3000  Loss: 3.5148700983461363

Epoch:  508/3000  Loss: 3.524835741205139

Epoch:  509/3000  Loss: 3.513592180510476

Epoch:  510/3000  Loss: 3.555092797897987

Epoch:  511/3000  Loss: 3.512190171689582

Epoch:  512/3000  Loss: 3.562483935898125

Epoch:  513/3000  Loss: 3.558832346503414

Epoch:  514/3000  Loss: 3.527804341573529

Epoch:  515/3000  Loss: 3.519102119550913

Epoch:  516/3000  Loss: 3.518887594838367

Epoch:  517/3000  Loss: 3.524837609410423

Epoch:  518/3000  Loss: 3.5311608347361725

Epoch:  519/3000  Loss: 3.5032657939175103

Epoch:  520/3000  Loss: 3.5322408435265316

Epoch:  521/3000  Loss: 3.515541050108374

Epoch:  522/3000  Loss: 3.5308469965044584

Epoch:  523/3000  Loss: 3.5174138472082968

Epoch:  524/3000  Loss: 3.51761945189072

Epoch:  525/3000  Loss: 3.526685306864409

Epoch:  526/3000  Loss: 3.532980828279743

Epoch:  527/3000  Loss: 3.532638033276721

Epoch:  528/3000  Loss: 3.51648038536755

Epoch:  529/3000  Loss: 3.5208810213375856

Epoch:  530/3000  Loss: 3.515054831138024

Epoch:  531/3000  Loss: 3.521420456922424

Epoch:  532/3000  Loss: 3.5200569309405427

Epoch:  533/3000  Loss: 3.507476949253805

Epoch:  534/3000  Loss: 3.5007149223345158

Epoch:  535/3000  Loss: 3.5250803156917323

Epoch:  536/3000  Loss: 3.52646726855728

Epoch:  537/3000  Loss: 3.5145001337530966

Epoch:  538/3000  Loss: 3.5095386436694525

Epoch:  539/3000  Loss: 3.5255087653750254

Epoch:  540/3000  Loss: 3.518621316870373

Epoch:  541/3000  Loss: 3.5029325835603524

Epoch:  542/3000  Loss: 3.503966645449913

Epoch:  543/3000  Loss: 3.5322049143120013

Epoch:  544/3000  Loss: 3.566921511145328

Epoch:  545/3000  Loss: 3.5727757769802544

Epoch:  546/3000  Loss: 3.5597625920200455

Epoch:  547/3000  Loss: 3.547175073459419

Epoch:  548/3000  Loss: 3.5589406197435136

Epoch:  549/3000  Loss: 3.551039229030587

Epoch:  550/3000  Loss: 3.558715812100609

Epoch:  551/3000  Loss: 3.548465998109802

Epoch:  552/3000  Loss: 3.5465748260817764

Epoch:  553/3000  Loss: 3.5636503428186534

Epoch:  554/3000  Loss: 3.5590292807972115

Epoch:  555/3000  Loss: 3.537426773086618

Epoch:  556/3000  Loss: 3.530979381226234

Epoch:  557/3000  Loss: 3.549996885996051

Epoch:  558/3000  Loss: 3.5297662678049573

Epoch:  559/3000  Loss: 3.5403178222691287

Epoch:  560/3000  Loss: 3.5278388159146403

Epoch:  561/3000  Loss: 3.525035485608159

Epoch:  562/3000  Loss: 3.5785946585965074

Epoch:  563/3000  Loss: 3.57442675228097

Epoch:  564/3000  Loss: 3.561997447030279

Epoch:  565/3000  Loss: 3.562764003821547

Epoch:  566/3000  Loss: 3.5693232243972584

Epoch:  567/3000  Loss: 3.538353200621227

Epoch:  568/3000  Loss: 3.5389695936721997

Epoch:  569/3000  Loss: 3.5424195575933095

Epoch:  570/3000  Loss: 3.543190291773712

Epoch:  571/3000  Loss: 3.5472687686762496

Epoch:  572/3000  Loss: 3.5400464209021165

Epoch:  573/3000  Loss: 3.539241070041153

Epoch:  574/3000  Loss: 3.5355482230093394

Epoch:  575/3000  Loss: 3.535912066184831

Epoch:  576/3000  Loss: 3.529727464975911

Epoch:  577/3000  Loss: 3.541838124206228

Epoch:  578/3000  Loss: 3.528705860251811

Epoch:  579/3000  Loss: 3.534820934661357

Epoch:  580/3000  Loss: 3.5359268744147876

Epoch:  581/3000  Loss: 3.5505064754617472

Epoch:  582/3000  Loss: 3.5418793550178185

Epoch:  583/3000  Loss: 3.529117230295998

Epoch:  584/3000  Loss: 3.5351531858696044

Epoch:  585/3000  Loss: 3.535193259625703

Epoch:  586/3000  Loss: 3.5272074498484245

Epoch:  587/3000  Loss: 3.525564117354996

Epoch:  588/3000  Loss: 3.532480588599817

Epoch:  589/3000  Loss: 3.5279843106472395

Epoch:  590/3000  Loss: 3.517723146180198

Epoch:  591/3000  Loss: 3.534555645405083

Epoch:  592/3000  Loss: 3.542985925170776

Epoch:  593/3000  Loss: 3.531327761410298

Epoch:  594/3000  Loss: 3.547788476560748

Epoch:  595/3000  Loss: 3.5388448435458195

Epoch:  596/3000  Loss: 3.538488257218721

Epoch:  597/3000  Loss: 3.534137618801761

Epoch:  598/3000  Loss: 3.5391658211958807

Epoch:  599/3000  Loss: 3.5287119252810384

Epoch:  600/3000  Loss: 3.5268349502445226

Epoch:  601/3000  Loss: 3.5691249671677636

Epoch:  602/3000  Loss: 3.524981953657043

Epoch:  603/3000  Loss: 3.528233260155819

Epoch:  604/3000  Loss: 3.5546240527375286

Epoch:  605/3000  Loss: 3.5417417957095685

Epoch:  606/3000  Loss: 3.536781661956647

Epoch:  607/3000  Loss: 3.527903168674999

Epoch:  608/3000  Loss: 3.5181858840683984

Epoch:  609/3000  Loss: 3.519370847126099

Epoch:  610/3000  Loss: 3.5148791590733315

Epoch:  611/3000  Loss: 3.516475579768727

Epoch:  612/3000  Loss: 3.505335843658885

Epoch:  613/3000  Loss: 3.5192832697683305

Epoch:  614/3000  Loss: 3.5046201272344755

Epoch:  615/3000  Loss: 3.5082447854577468

Epoch:  616/3000  Loss: 3.5128439389742336

Epoch:  617/3000  Loss: 3.540617449275388

Epoch:  618/3000  Loss: 3.539174760934543

Epoch:  619/3000  Loss: 3.518266792275465

Epoch:  620/3000  Loss: 3.527051073812316

Epoch:  621/3000  Loss: 3.520528273522238

Epoch:  622/3000  Loss: 3.516716347222487

Epoch:  623/3000  Loss: 3.5161368852367905

Epoch:  624/3000  Loss: 3.5104399200969394

Epoch:  625/3000  Loss: 3.5132368618256735

Epoch:  626/3000  Loss: 3.5145699238804533

Epoch:  627/3000  Loss: 3.5134030217828762

Epoch:  628/3000  Loss: 3.517301392473118

Epoch:  629/3000  Loss: 3.509360033390032

Epoch:  630/3000  Loss: 3.5247506985845027

Epoch:  631/3000  Loss: 3.516911289037711

Epoch:  632/3000  Loss: 3.5106355714195767

Epoch:  633/3000  Loss: 3.509973500270384

Epoch:  634/3000  Loss: 3.5255176221735853

Epoch:  635/3000  Loss: 3.521236111184349

Epoch:  636/3000  Loss: 3.508317787255814

Epoch:  637/3000  Loss: 3.508695434346949

Epoch:  638/3000  Loss: 3.510352648768989

Epoch:  639/3000  Loss: 3.501626403131934

Epoch:  640/3000  Loss: 3.502816548440541

Epoch:  641/3000  Loss: 3.514264365698796

Epoch:  642/3000  Loss: 3.5087486167452226

Epoch:  643/3000  Loss: 3.4949612518949977

Epoch:  644/3000  Loss: 3.5016820507399933

Epoch:  645/3000  Loss: 3.508727488643677

Epoch:  646/3000  Loss: 3.51503620985318

Epoch:  647/3000  Loss: 3.5094039056933433

Epoch:  648/3000  Loss: 3.497468789327295

Epoch:  649/3000  Loss: 3.502103115743118

Epoch:  650/3000  Loss: 3.4919220560864384

Epoch:  651/3000  Loss: 3.4978763668463233

Epoch:  652/3000  Loss: 3.4957967612007046

Epoch:  653/3000  Loss: 3.5042506586396733

Epoch:  654/3000  Loss: 3.5062036525231415

Epoch:  655/3000  Loss: 3.495640889100996

Epoch:  656/3000  Loss: 3.4939520684777663

Epoch:  657/3000  Loss: 3.4897518823122184

Epoch:  658/3000  Loss: 3.494880218867183

Epoch:  659/3000  Loss: 3.4971081616547295

Epoch:  660/3000  Loss: 3.4980806485930485

Epoch:  661/3000  Loss: 3.4765551722556114

Epoch:  662/3000  Loss: 3.4936490028788656

Epoch:  663/3000  Loss: 3.4970564894232767

Epoch:  664/3000  Loss: 3.479523788226595

Epoch:  665/3000  Loss: 3.492066900710972

Epoch:  666/3000  Loss: 3.483790721740022

Epoch:  667/3000  Loss: 3.5269782989361804

Epoch:  668/3000  Loss: 3.5102557492721504

Epoch:  669/3000  Loss: 3.492353683497958

Epoch:  670/3000  Loss: 3.4878391576278633

Epoch:  671/3000  Loss: 3.491497084412865

Epoch:  672/3000  Loss: 3.484765972527242

Epoch:  673/3000  Loss: 3.500488816117857

Epoch:  674/3000  Loss: 3.53621717654468

Epoch:  675/3000  Loss: 3.5228003971064816

Epoch:  676/3000  Loss: 3.511812622319954

Epoch:  677/3000  Loss: 3.5194998770165253

Epoch:  678/3000  Loss: 3.5059608088295167

Epoch:  679/3000  Loss: 3.4992091891138752

Epoch:  680/3000  Loss: 3.509605561550942

Epoch:  681/3000  Loss: 3.502922277089238

Epoch:  682/3000  Loss: 3.5067766676266623

Epoch:  683/3000  Loss: 3.500228673253897

Epoch:  684/3000  Loss: 3.528148029209141

Epoch:  685/3000  Loss: 3.5048050625863496

Epoch:  686/3000  Loss: 3.5071325641548867

Epoch:  687/3000  Loss: 3.489629224028297

Epoch:  688/3000  Loss: 3.504529139204496

Epoch:  689/3000  Loss: 3.485619024348998

Epoch:  690/3000  Loss: 3.4978756606236665

Epoch:  691/3000  Loss: 3.5099839512707858

Epoch:  692/3000  Loss: 3.495168684544437

Epoch:  693/3000  Loss: 3.5027476999433937

Epoch:  694/3000  Loss: 3.4923834466769965

Epoch:  695/3000  Loss: 3.509887703798121

Epoch:  696/3000  Loss: 3.5011829577138314

Epoch:  697/3000  Loss: 3.4991192661662054

Epoch:  698/3000  Loss: 3.4908873183582187

Epoch:  699/3000  Loss: 3.485005081726407

Epoch:  700/3000  Loss: 3.508615854823904

Epoch:  701/3000  Loss: 3.4937274352006877

Epoch:  702/3000  Loss: 3.5035009485435267

Epoch:  703/3000  Loss: 3.5001735435424526

Epoch:  704/3000  Loss: 3.5114373473705025

Epoch:  705/3000  Loss: 3.4917192360563747

Epoch:  706/3000  Loss: 3.4939222965393766

Epoch:  707/3000  Loss: 3.4923563217049214

Epoch:  708/3000  Loss: 3.491575919938553

Epoch:  709/3000  Loss: 3.4815983126275714

Epoch:  710/3000  Loss: 3.488348234945269

Epoch:  711/3000  Loss: 3.5048748324577077

Epoch:  712/3000  Loss: 3.471466239366411

Epoch:  713/3000  Loss: 3.487441384012198

Epoch:  714/3000  Loss: 3.4969628916542237

Epoch:  715/3000  Loss: 3.4808034686078706

Epoch:  716/3000  Loss: 3.504052318470076

Epoch:  717/3000  Loss: 3.481086111506693

Epoch:  718/3000  Loss: 3.4950829784029795

Epoch:  719/3000  Loss: 3.4762680544234583

Epoch:  720/3000  Loss: 3.471063829316885

Epoch:  721/3000  Loss: 3.476624077688539

Epoch:  722/3000  Loss: 3.490969213360897

Epoch:  723/3000  Loss: 3.5096895771376984

Epoch:  724/3000  Loss: 3.478554896180583

Epoch:  725/3000  Loss: 3.4831184798896517

Epoch:  726/3000  Loss: 3.4913744225704573

Epoch:  727/3000  Loss: 3.491761003378201

Epoch:  728/3000  Loss: 3.508381959354563

Epoch:  729/3000  Loss: 3.5010246803511436

Epoch:  730/3000  Loss: 3.5080212213689235

Epoch:  731/3000  Loss: 3.5027339688398533

Epoch:  732/3000  Loss: 3.4979582914118104

Epoch:  733/3000  Loss: 3.4798733635145815

Epoch:  734/3000  Loss: 3.4986114543012583

Epoch:  735/3000  Loss: 3.489438094446771

Epoch:  736/3000  Loss: 3.5048356176654454

Epoch:  737/3000  Loss: 3.503463170558522

Epoch:  738/3000  Loss: 3.4933178074461173

Epoch:  739/3000  Loss: 3.4911257996214244

Epoch:  740/3000  Loss: 3.502706681546059

Epoch:  741/3000  Loss: 3.5137857985688132

Epoch:  742/3000  Loss: 3.481060985856434

Epoch:  743/3000  Loss: 3.507035701611559

Epoch:  744/3000  Loss: 3.4910100833967275

Epoch:  745/3000  Loss: 3.5059069303365855

Epoch:  746/3000  Loss: 3.4852514009661846

Epoch:  747/3000  Loss: 3.503383289659064

Epoch:  748/3000  Loss: 3.4923855262016184

Epoch:  749/3000  Loss: 3.508288155467037

Epoch:  750/3000  Loss: 3.4863834200441906

Epoch:  751/3000  Loss: 3.499616553944917

Epoch:  752/3000  Loss: 3.490513866448649

Epoch:  753/3000  Loss: 3.511389790938998

Epoch:  754/3000  Loss: 3.5019955353129197

Epoch:  755/3000  Loss: 3.469137951098682

Epoch:  756/3000  Loss: 3.486355958932029

Epoch:  757/3000  Loss: 3.5236857293257073

Epoch:  758/3000  Loss: 3.4955399928766333

Epoch:  759/3000  Loss: 3.4894427128144714

Epoch:  760/3000  Loss: 3.4887886071998957

Epoch:  761/3000  Loss: 3.4939807813559005

Epoch:  762/3000  Loss: 3.480536925395786

Epoch:  763/3000  Loss: 3.470374799075822

Epoch:  764/3000  Loss: 3.502625894327525

Epoch:  765/3000  Loss: 3.4780553120833178

Epoch:  766/3000  Loss: 3.5021666886475273

Epoch:  767/3000  Loss: 3.4894681171215773

Epoch:  768/3000  Loss: 3.5090887573912277

Epoch:  769/3000  Loss: 3.506145604579238

Epoch:  770/3000  Loss: 3.5031145864458226

Epoch:  771/3000  Loss: 3.517779954402236

Epoch:  772/3000  Loss: 3.503377311675885

Epoch:  773/3000  Loss: 3.484313772138033

Epoch:  774/3000  Loss: 3.501450935817066

Epoch:  775/3000  Loss: 3.4898040275486135

Epoch:  776/3000  Loss: 3.494713022295561

Epoch:  777/3000  Loss: 3.5089737687401055

Epoch:  778/3000  Loss: 3.501989447016667

Epoch:  779/3000  Loss: 3.485558507910552

Epoch:  780/3000  Loss: 3.4942328946324905

Epoch:  781/3000  Loss: 3.485525173927421

Epoch:  782/3000  Loss: 3.480966665167212

Epoch:  783/3000  Loss: 3.491272497396108

Epoch:  784/3000  Loss: 3.4780728097076943

Epoch:  785/3000  Loss: 3.4877761390261206

Epoch:  786/3000  Loss: 3.5040882216802833

Epoch:  787/3000  Loss: 3.4810433228993114

Epoch:  788/3000  Loss: 3.5101726542384153

Epoch:  789/3000  Loss: 3.485435206361808

Epoch:  790/3000  Loss: 3.5001888926623197

Epoch:  791/3000  Loss: 3.483106928222078

Epoch:  792/3000  Loss: 3.4822842179702427

Epoch:  793/3000  Loss: 3.4898687008464653

Epoch:  794/3000  Loss: 3.4900106091723786

Epoch:  795/3000  Loss: 3.4859930877707446

Epoch:  796/3000  Loss: 3.4780260232778697

Epoch:  797/3000  Loss: 3.482731795338347

Epoch:  798/3000  Loss: 3.481972798687993

Epoch:  799/3000  Loss: 3.4774680394940205

Epoch:  800/3000  Loss: 3.4896402276889877

Epoch:  801/3000  Loss: 3.4882276862962374

Epoch:  802/3000  Loss: 3.488842579718983

Epoch:  810/3000  Loss: 3.4733824825724833

Epoch:  811/3000  Loss: 3.5037946161802274

Epoch:  812/3000  Loss: 3.4841352376817425

Epoch:  813/3000  Loss: 3.486241283701427

Epoch:  814/3000  Loss: 3.47642787699037

Epoch:  815/3000  Loss: 3.476686945191481

Epoch:  816/3000  Loss: 3.485499865699444

Epoch:  817/3000  Loss: 3.5012014549991157

Epoch:  818/3000  Loss: 3.477809165840718

Epoch:  819/3000  Loss: 3.491749431728359

Epoch:  820/3000  Loss: 3.4896507791481937

Epoch:  821/3000  Loss: 3.485738421964591

Epoch:  822/3000  Loss: 3.4706645761921266

Epoch:  823/3000  Loss: 3.492827595580459

Epoch:  824/3000  Loss: 3.488163863750331

Epoch:  825/3000  Loss: 3.478459519715862

Epoch:  826/3000  Loss: 3.4862177254274984

Epoch:  827/3000  Loss: 3.4806658446994083

Epoch:  828/3000  Loss: 3.484254165850879

Epoch:  829/3000  Loss: 3.481445984232713

Epoch:  830/3000  Loss: 3.485753226909791

Epoch:  831/3000  Loss: 3.4828337851129088

Epoch:  832/3000  Loss: 3.4816732305336218

Epoch:  833/3000  Loss: 3.5112338290559437

Epoch:  834/3000  Loss: 3.502547116011347

</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_text output_error">
<pre>
<span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">KeyboardInterrupt</span>                         Traceback (most recent call last)
<span class="ansi-green-fg">&lt;ipython-input-16-62e907872a91&gt;</span> in <span class="ansi-cyan-fg">&lt;module&gt;</span><span class="ansi-blue-fg">()</span>
<span class="ansi-green-intense-fg ansi-bold">     19</span> 
<span class="ansi-green-intense-fg ansi-bold">     20</span>     <span class="ansi-red-fg"># training the model</span>
<span class="ansi-green-fg">---&gt; 21</span><span class="ansi-red-fg">     </span>trained_rnn <span class="ansi-blue-fg">=</span> train_rnn<span class="ansi-blue-fg">(</span>rnn<span class="ansi-blue-fg">,</span> batch_size<span class="ansi-blue-fg">,</span> optimizer<span class="ansi-blue-fg">,</span> criterion<span class="ansi-blue-fg">,</span> num_epochs<span class="ansi-blue-fg">,</span> show_every_n_batches<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">     22</span> 
<span class="ansi-green-intense-fg ansi-bold">     23</span>     <span class="ansi-red-fg"># saving the trained model</span>

<span class="ansi-green-fg">&lt;ipython-input-13-54bd9dc2f859&gt;</span> in <span class="ansi-cyan-fg">train_rnn</span><span class="ansi-blue-fg">(rnn, batch_size, optimizer, criterion, n_epochs, show_every_n_batches)</span>
<span class="ansi-green-intense-fg ansi-bold">     22</span> 
<span class="ansi-green-intense-fg ansi-bold">     23</span>             <span class="ansi-red-fg"># forward, back prop</span>
<span class="ansi-green-fg">---&gt; 24</span><span class="ansi-red-fg">             </span>loss<span class="ansi-blue-fg">,</span> hidden <span class="ansi-blue-fg">=</span> forward_back_prop<span class="ansi-blue-fg">(</span>rnn<span class="ansi-blue-fg">,</span> optimizer<span class="ansi-blue-fg">,</span> criterion<span class="ansi-blue-fg">,</span> inputs<span class="ansi-blue-fg">,</span> labels<span class="ansi-blue-fg">,</span> hidden<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">     25</span>             <span class="ansi-red-fg"># record loss</span>
<span class="ansi-green-intense-fg ansi-bold">     26</span>             batch_losses<span class="ansi-blue-fg">.</span>append<span class="ansi-blue-fg">(</span>loss<span class="ansi-blue-fg">)</span>

<span class="ansi-green-fg">&lt;ipython-input-12-c302a43168d3&gt;</span> in <span class="ansi-cyan-fg">forward_back_prop</span><span class="ansi-blue-fg">(rnn, optimizer, criterion, inp, target, hidden)</span>
<span class="ansi-green-intense-fg ansi-bold">     26</span>     <span class="ansi-red-fg"># calculate the loss and perform backprop</span>
<span class="ansi-green-intense-fg ansi-bold">     27</span>     loss <span class="ansi-blue-fg">=</span> criterion<span class="ansi-blue-fg">(</span>output<span class="ansi-blue-fg">.</span>squeeze<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">,</span> target<span class="ansi-blue-fg">.</span>long<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">)</span>
<span class="ansi-green-fg">---&gt; 28</span><span class="ansi-red-fg">     </span>loss<span class="ansi-blue-fg">.</span>backward<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">     29</span>     loss<span class="ansi-blue-fg">=</span> loss<span class="ansi-blue-fg">.</span>item<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">     30</span>     optimizer<span class="ansi-blue-fg">.</span>step<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span>

<span class="ansi-green-fg">/opt/conda/lib/python3.6/site-packages/torch/tensor.py</span> in <span class="ansi-cyan-fg">backward</span><span class="ansi-blue-fg">(self, gradient, retain_graph, create_graph)</span>
<span class="ansi-green-intense-fg ansi-bold">     91</span>                 products<span class="ansi-blue-fg">.</span> Defaults to<span class="ansi-red-fg"> </span><span class="ansi-red-fg">`</span><span class="ansi-red-fg">`</span><span class="ansi-green-fg">False</span><span class="ansi-red-fg">`</span><span class="ansi-red-fg">`</span><span class="ansi-blue-fg">.</span>
<span class="ansi-green-intense-fg ansi-bold">     92</span>         &#34;&#34;&#34;
<span class="ansi-green-fg">---&gt; 93</span><span class="ansi-red-fg">         </span>torch<span class="ansi-blue-fg">.</span>autograd<span class="ansi-blue-fg">.</span>backward<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">,</span> gradient<span class="ansi-blue-fg">,</span> retain_graph<span class="ansi-blue-fg">,</span> create_graph<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">     94</span> 
<span class="ansi-green-intense-fg ansi-bold">     95</span>     <span class="ansi-green-fg">def</span> register_hook<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">,</span> hook<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">:</span>

<span class="ansi-green-fg">/opt/conda/lib/python3.6/site-packages/torch/autograd/__init__.py</span> in <span class="ansi-cyan-fg">backward</span><span class="ansi-blue-fg">(tensors, grad_tensors, retain_graph, create_graph, grad_variables)</span>
<span class="ansi-green-intense-fg ansi-bold">     87</span>     Variable._execution_engine.run_backward(
<span class="ansi-green-intense-fg ansi-bold">     88</span>         tensors<span class="ansi-blue-fg">,</span> grad_tensors<span class="ansi-blue-fg">,</span> retain_graph<span class="ansi-blue-fg">,</span> create_graph<span class="ansi-blue-fg">,</span>
<span class="ansi-green-fg">---&gt; 89</span><span class="ansi-red-fg">         allow_unreachable=True)  # allow_unreachable flag
</span><span class="ansi-green-intense-fg ansi-bold">     90</span> 
<span class="ansi-green-intense-fg ansi-bold">     91</span> 

<span class="ansi-red-fg">KeyboardInterrupt</span>: </pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">workspace_utils</span> <span class="k">import</span> <span class="n">active_session</span>

<span class="k">with</span> <span class="n">active_session</span><span class="p">():</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    DON&#39;T MODIFY ANYTHING IN THIS CELL</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;sequence_length: </span><span class="si">{}</span><span class="se">\n</span><span class="s1">batch_size: </span><span class="si">{}</span><span class="se">\n</span><span class="s1">learning_rate: </span><span class="si">{}</span><span class="se">\n</span><span class="s1">embedding_dim: </span><span class="si">{}</span><span class="se">\n</span><span class="s1">n_layers: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">sequence_length</span><span class="p">,</span><span class="n">batch_size</span><span class="p">,</span><span class="n">learning_rate</span><span class="p">,</span><span class="n">embedding_dim</span><span class="p">,</span><span class="n">n_layers</span><span class="p">))</span>

    <span class="c1"># create model and move to gpu if available</span>
    <span class="n">rnn</span> <span class="o">=</span> <span class="n">RNN</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">,</span> <span class="n">n_layers</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">train_on_gpu</span><span class="p">:</span>
        <span class="n">rnn</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>

        
    <span class="c1"># defining loss and optimization functions for training</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">rnn</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>
    <span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>

    <span class="c1"># training the model</span>
    <span class="n">trained_rnn</span> <span class="o">=</span> <span class="n">train_rnn</span><span class="p">(</span><span class="n">rnn</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">num_epochs</span><span class="p">,</span> <span class="n">show_every_n_batches</span><span class="p">)</span>

    <span class="c1"># saving the trained model</span>
    <span class="n">helper</span><span class="o">.</span><span class="n">save_model</span><span class="p">(</span><span class="s1">&#39;/trained_rnn&#39;</span><span class="p">,</span> <span class="n">trained_rnn</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Model Trained and Saved&#39;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;sequence_length: </span><span class="si">{}</span><span class="se">\n</span><span class="s1">batch_size: </span><span class="si">{}</span><span class="se">\n</span><span class="s1">learning_rate: </span><span class="si">{}</span><span class="se">\n</span><span class="s1">embedding_dim: </span><span class="si">{}</span><span class="se">\n</span><span class="s1">n_layers: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">sequence_length</span><span class="p">,</span><span class="n">batch_size</span><span class="p">,</span><span class="n">learning_rate</span><span class="p">,</span><span class="n">embedding_dim</span><span class="p">,</span><span class="n">n_layers</span><span class="p">))</span>
    
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>sequence_length: 16
batch_size: 64
learning_rate: 0.0001
embedding_dim: 300
n_layers: 2
Training for 3000 epoch(s)...
Epoch:    1/3000  Loss: 9.927245641708375

Epoch:    2/3000  Loss: 9.02769772193462

Epoch:    3/3000  Loss: 7.782626923408903

Epoch:    4/3000  Loss: 7.083254235483338

Epoch:    5/3000  Loss: 6.671719106001914

Epoch:    6/3000  Loss: 6.416002596013171

Epoch:    7/3000  Loss: 6.263115395634648

Epoch:    8/3000  Loss: 6.173454600004597

Epoch:    9/3000  Loss: 6.079519289916747

Epoch:   10/3000  Loss: 5.9827812755422665

Epoch:   11/3000  Loss: 5.893583059584643

Epoch:   12/3000  Loss: 5.82331772430893

Epoch:   13/3000  Loss: 5.760793973876743

Epoch:   14/3000  Loss: 5.706758129062937

Epoch:   15/3000  Loss: 5.658798049703394

Epoch:   16/3000  Loss: 5.611003027717774

Epoch:   17/3000  Loss: 5.577107125116955

Epoch:   18/3000  Loss: 5.5484686160607675

Epoch:   19/3000  Loss: 5.510022770523614

Epoch:   20/3000  Loss: 5.48522162410066

Epoch:   21/3000  Loss: 5.457748055594934

Epoch:   22/3000  Loss: 5.432739424787624

Epoch:   23/3000  Loss: 5.415745360706211

Epoch:   24/3000  Loss: 5.390912391561865

Epoch:   25/3000  Loss: 5.36943259222773

Epoch:   26/3000  Loss: 5.353854218251944

Epoch:   27/3000  Loss: 5.333834795946369

Epoch:   28/3000  Loss: 5.320067863924222

Epoch:   29/3000  Loss: 5.303584261958826

Epoch:   30/3000  Loss: 5.2930499639631545

Epoch:   31/3000  Loss: 5.276590564083971

Epoch:   32/3000  Loss: 5.258889703060811

Epoch:   33/3000  Loss: 5.249622926920071

Epoch:   34/3000  Loss: 5.240098207059876

Epoch:   35/3000  Loss: 5.226517811434688

Epoch:   36/3000  Loss: 5.21455440565037

Epoch:   37/3000  Loss: 5.206431682840691

Epoch:   38/3000  Loss: 5.193696057618554

Epoch:   39/3000  Loss: 5.1809469052215125

Epoch:   40/3000  Loss: 5.174253241476591

Epoch:   41/3000  Loss: 5.165156772298189

Epoch:   42/3000  Loss: 5.155763916252128

Epoch:   43/3000  Loss: 5.145589143072012

Epoch:   44/3000  Loss: 5.139372355902373

Epoch:   45/3000  Loss: 5.12923780566105

Epoch:   46/3000  Loss: 5.1153891625825905

Epoch:   47/3000  Loss: 5.113196526821938

Epoch:   48/3000  Loss: 5.10695862359486

Epoch:   49/3000  Loss: 5.09343390514054

Epoch:   50/3000  Loss: 5.0881802367835585

Epoch:   51/3000  Loss: 5.0778577335940165

Epoch:   52/3000  Loss: 5.070524514337358

Epoch:   53/3000  Loss: 5.06620485172206

Epoch:   54/3000  Loss: 5.056111201626836

Epoch:   55/3000  Loss: 5.050945982730484

Epoch:   56/3000  Loss: 5.04072183309001

Epoch:   57/3000  Loss: 5.031468915884585

Epoch:   58/3000  Loss: 5.027779014731932

Epoch:   59/3000  Loss: 5.023058970404273

Epoch:   60/3000  Loss: 5.012486657099937

Epoch:   61/3000  Loss: 5.006282911508694

Epoch:   62/3000  Loss: 5.002419387705701

Epoch:   63/3000  Loss: 4.9916701834182655

Epoch:   64/3000  Loss: 4.986045699442569

Epoch:   65/3000  Loss: 4.982703700268173

Epoch:   66/3000  Loss: 4.977261616633489

Epoch:   67/3000  Loss: 4.967521227870551

Epoch:   68/3000  Loss: 4.963961467677224

Epoch:   69/3000  Loss: 4.956325915048645

Epoch:   70/3000  Loss: 4.948177098406716

Epoch:   71/3000  Loss: 4.9488749895522846

Epoch:   72/3000  Loss: 4.9397959375217235

Epoch:   73/3000  Loss: 4.935968170209812

Epoch:   74/3000  Loss: 4.930600010294865

Epoch:   75/3000  Loss: 4.925285402587032

Epoch:   76/3000  Loss: 4.918935491899124

Epoch:   77/3000  Loss: 4.9099611324503005

Epoch:   78/3000  Loss: 4.909762484608507

Epoch:   79/3000  Loss: 4.907291917932294

Epoch:   80/3000  Loss: 4.89944064165503

Epoch:   81/3000  Loss: 4.8962231602104875

Epoch:   82/3000  Loss: 4.891855995362169

Epoch:   83/3000  Loss: 4.884047627038441

Epoch:   84/3000  Loss: 4.883442488658305

Epoch:   85/3000  Loss: 4.878714094205784

Epoch:   86/3000  Loss: 4.874882436099748

Epoch:   87/3000  Loss: 4.866014873392957

Epoch:   88/3000  Loss: 4.8620296203446856

Epoch:   89/3000  Loss: 4.858946833626958

Epoch:   90/3000  Loss: 4.856427125897938

Epoch:   91/3000  Loss: 4.8499273651639845

Epoch:   92/3000  Loss: 4.850236306644882

Epoch:   93/3000  Loss: 4.845710376921258

Epoch:   94/3000  Loss: 4.840903979628833

Epoch:   95/3000  Loss: 4.834823827929666

Epoch:   96/3000  Loss: 4.8294836823346285

Epoch:   97/3000  Loss: 4.8308370502069

Epoch:   98/3000  Loss: 4.82298218697549

Epoch:   99/3000  Loss: 4.819888612022356

Epoch:  100/3000  Loss: 4.8186073976597745

Epoch:  101/3000  Loss: 4.816278209916212

Epoch:  102/3000  Loss: 4.808794168325571

Epoch:  103/3000  Loss: 4.805715752797614

Epoch:  104/3000  Loss: 4.805382983692752

Epoch:  105/3000  Loss: 4.801308486820226

Epoch:  106/3000  Loss: 4.801443552724529

Epoch:  107/3000  Loss: 4.7941984248900384

Epoch:  108/3000  Loss: 4.791721131032425

Epoch:  109/3000  Loss: 4.782828505359479

Epoch:  110/3000  Loss: 4.779481625584319

Epoch:  111/3000  Loss: 4.781630566692243

Epoch:  112/3000  Loss: 4.778124876876376

Epoch:  113/3000  Loss: 4.774265201713133

Epoch:  114/3000  Loss: 4.771300105358785

Epoch:  115/3000  Loss: 4.768417762697222

Epoch:  116/3000  Loss: 4.768620299143304

Epoch:  117/3000  Loss: 4.762888048327476

Epoch:  118/3000  Loss: 4.763351411961797

Epoch:  119/3000  Loss: 4.758790812454048

Epoch:  120/3000  Loss: 4.751240052804607

Epoch:  121/3000  Loss: 4.751143709252814

Epoch:  122/3000  Loss: 4.7501745965806546

Epoch:  123/3000  Loss: 4.744376446705324

Epoch:  124/3000  Loss: 4.7449009681268075

Epoch:  125/3000  Loss: 4.7425405034788986

Epoch:  126/3000  Loss: 4.7346505631535525

Epoch:  127/3000  Loss: 4.735369687786606

Epoch:  128/3000  Loss: 4.732441142286964

Epoch:  129/3000  Loss: 4.7276790424000925

Epoch:  130/3000  Loss: 4.724095497558366

Epoch:  131/3000  Loss: 4.723670401337774

Epoch:  132/3000  Loss: 4.716176002636021

Epoch:  133/3000  Loss: 4.716981484066058

Epoch:  134/3000  Loss: 4.711904488803325

Epoch:  135/3000  Loss: 4.710868456881027

Epoch:  136/3000  Loss: 4.705823663729067

Epoch:  137/3000  Loss: 4.705238108519982

Epoch:  138/3000  Loss: 4.7034318247290345

Epoch:  139/3000  Loss: 4.6970383918381176

Epoch:  140/3000  Loss: 4.7008356462253085

Epoch:  141/3000  Loss: 4.6959141735094425

Epoch:  142/3000  Loss: 4.686930888008442

Epoch:  143/3000  Loss: 4.687897060823495

Epoch:  144/3000  Loss: 4.689644259234931

Epoch:  145/3000  Loss: 4.680706684679717

Epoch:  146/3000  Loss: 4.68045826483813

Epoch:  147/3000  Loss: 4.678587979482044

Epoch:  148/3000  Loss: 4.672806517538603

Epoch:  149/3000  Loss: 4.668487531583153

Epoch:  150/3000  Loss: 4.66768528728069

Epoch:  151/3000  Loss: 4.664346493207491

Epoch:  152/3000  Loss: 4.665693254886895

Epoch:  153/3000  Loss: 4.662019542930595

Epoch:  154/3000  Loss: 4.659331438325166

Epoch:  155/3000  Loss: 4.6612898055228795

Epoch:  156/3000  Loss: 4.651853304916627

Epoch:  157/3000  Loss: 4.652332621244831

Epoch:  158/3000  Loss: 4.6518997842217145

Epoch:  159/3000  Loss: 4.648305986012438

Epoch:  160/3000  Loss: 4.6449996948789645

Epoch:  161/3000  Loss: 4.640549755534403

Epoch:  162/3000  Loss: 4.64212262808387

Epoch:  163/3000  Loss: 4.639869221589869

Epoch:  164/3000  Loss: 4.638542427124304

Epoch:  165/3000  Loss: 4.632489466092343

Epoch:  166/3000  Loss: 4.631872744839446

Epoch:  167/3000  Loss: 4.630588124731789

Epoch:  168/3000  Loss: 4.626916833367331

Epoch:  169/3000  Loss: 4.619282579859965

Epoch:  170/3000  Loss: 4.622615361733771

Epoch:  171/3000  Loss: 4.618980980631131

Epoch:  172/3000  Loss: 4.620641609831327

Epoch:  173/3000  Loss: 4.616636323326627

Epoch:  174/3000  Loss: 4.613581166886024

Epoch:  175/3000  Loss: 4.611694748448176

Epoch:  176/3000  Loss: 4.610059772101664

Epoch:  177/3000  Loss: 4.606561440687913

Epoch:  178/3000  Loss: 4.60336960853857

Epoch:  179/3000  Loss: 4.601890743257805

Epoch:  180/3000  Loss: 4.598820612980769

Epoch:  181/3000  Loss: 4.60447966299703

Epoch:  182/3000  Loss: 4.600133609005285

Epoch:  183/3000  Loss: 4.596463736929384

Epoch:  184/3000  Loss: 4.59230399980337

Epoch:  185/3000  Loss: 4.595705381901475

Epoch:  186/3000  Loss: 4.592081699524627

Epoch:  187/3000  Loss: 4.5924179272044

Epoch:  188/3000  Loss: 4.59015285024413

Epoch:  189/3000  Loss: 4.583297155754711

Epoch:  190/3000  Loss: 4.582935028864513

Epoch:  191/3000  Loss: 4.583048615198321

Epoch:  192/3000  Loss: 4.575864780647199

Epoch:  193/3000  Loss: 4.580725494673824

Epoch:  194/3000  Loss: 4.572587682240045

Epoch:  195/3000  Loss: 4.574253011926855

Epoch:  196/3000  Loss: 4.573353132889547

Epoch:  197/3000  Loss: 4.568586663729014

Epoch:  198/3000  Loss: 4.569317526713305

Epoch:  199/3000  Loss: 4.566060872904606

Epoch:  200/3000  Loss: 4.564940980052839

Epoch:  201/3000  Loss: 4.563058209063402

Epoch:  202/3000  Loss: 4.559796717903233

Epoch:  203/3000  Loss: 4.561212622339225

Epoch:  204/3000  Loss: 4.556698462993214

Epoch:  205/3000  Loss: 4.557166677391762

Epoch:  206/3000  Loss: 4.551921830795936

Epoch:  207/3000  Loss: 4.554923338402837

Epoch:  208/3000  Loss: 4.5511103201952645

Epoch:  209/3000  Loss: 4.552821037826801

Epoch:  210/3000  Loss: 4.547718435149516

Epoch:  211/3000  Loss: 4.5440322964117525

Epoch:  212/3000  Loss: 4.5463335894552905

Epoch:  213/3000  Loss: 4.543147891041332

Epoch:  214/3000  Loss: 4.53803278877048

Epoch:  215/3000  Loss: 4.537059817056842

Epoch:  216/3000  Loss: 4.535259636343552

Epoch:  217/3000  Loss: 4.530384216461883

Epoch:  218/3000  Loss: 4.533524169166381

Epoch:  219/3000  Loss: 4.535691731011689

Epoch:  220/3000  Loss: 4.5286647381109155

Epoch:  221/3000  Loss: 4.5246113367661

Epoch:  222/3000  Loss: 4.517893143554232

Epoch:  223/3000  Loss: 4.524569302010344

Epoch:  224/3000  Loss: 4.524116899334878

Epoch:  225/3000  Loss: 4.518086111778233

Epoch:  226/3000  Loss: 4.513675944813357

Epoch:  227/3000  Loss: 4.517977968012287

Epoch:  228/3000  Loss: 4.517474466842844

Epoch:  229/3000  Loss: 4.5124315588674095

Epoch:  230/3000  Loss: 4.505894318240108

Epoch:  231/3000  Loss: 4.507649129074832

Epoch:  232/3000  Loss: 4.50557065776514

Epoch:  233/3000  Loss: 4.501939162868583

Epoch:  234/3000  Loss: 4.509870197140664

Epoch:  235/3000  Loss: 4.498160319815547

Epoch:  236/3000  Loss: 4.4946118019752195

Epoch:  237/3000  Loss: 4.494325822858668

Epoch:  238/3000  Loss: 4.489998393436524

Epoch:  239/3000  Loss: 4.482935770781524

Epoch:  240/3000  Loss: 4.484981879027374

Epoch:  241/3000  Loss: 4.483069697970227

Epoch:  242/3000  Loss: 4.476126819198906

Epoch:  243/3000  Loss: 4.477605711031453

Epoch:  244/3000  Loss: 4.475569451306909

Epoch:  245/3000  Loss: 4.473569545077949

Epoch:  246/3000  Loss: 4.469967131225989

Epoch:  247/3000  Loss: 4.464183327524862

Epoch:  248/3000  Loss: 4.457301211001268

Epoch:  249/3000  Loss: 4.459562314227309

Epoch:  250/3000  Loss: 4.452749196204745

Epoch:  251/3000  Loss: 4.457293812360884

Epoch:  252/3000  Loss: 4.458867874539679

Epoch:  253/3000  Loss: 4.451689195961411

Epoch:  254/3000  Loss: 4.449254636786425

Epoch:  255/3000  Loss: 4.447489143102232

Epoch:  256/3000  Loss: 4.444577022753955

Epoch:  257/3000  Loss: 4.438226667528996

Epoch:  258/3000  Loss: 4.435732526976261

Epoch:  259/3000  Loss: 4.43461857624908

Epoch:  260/3000  Loss: 4.438006419949362

Epoch:  261/3000  Loss: 4.431122107839749

Epoch:  262/3000  Loss: 4.4316666369870665

Epoch:  263/3000  Loss: 4.4312176756415385

Epoch:  264/3000  Loss: 4.42528231655826

Epoch:  265/3000  Loss: 4.4227310146721575

Epoch:  266/3000  Loss: 4.424688569987546

Epoch:  267/3000  Loss: 4.4226787325709065

Epoch:  268/3000  Loss: 4.417943181455067

Epoch:  269/3000  Loss: 4.420292150823722

Epoch:  270/3000  Loss: 4.422869172627288

Epoch:  271/3000  Loss: 4.414603265363779

Epoch:  272/3000  Loss: 4.410953438788413

Epoch:  273/3000  Loss: 4.412267813863218

Epoch:  274/3000  Loss: 4.415441270810728

Epoch:  275/3000  Loss: 4.4125995808436045

Epoch:  276/3000  Loss: 4.408423583898971

Epoch:  277/3000  Loss: 4.413864613126806

Epoch:  278/3000  Loss: 4.403192777447531

Epoch:  279/3000  Loss: 4.409557151739688

Epoch:  280/3000  Loss: 4.404313408548331

Epoch:  281/3000  Loss: 4.397774492695192

Epoch:  282/3000  Loss: 4.398324384481296

Epoch:  283/3000  Loss: 4.402695003251121

Epoch:  284/3000  Loss: 4.394743144443744

Epoch:  285/3000  Loss: 4.387227539080019

Epoch:  286/3000  Loss: 4.393805471818839

Epoch:  287/3000  Loss: 4.392361347491924

Epoch:  288/3000  Loss: 4.387169190854621

Epoch:  289/3000  Loss: 4.389953406615318

Epoch:  290/3000  Loss: 4.3813262043262045

Epoch:  291/3000  Loss: 4.385580469081104

Epoch:  292/3000  Loss: 4.380763851268694

Epoch:  293/3000  Loss: 4.383485927373752

Epoch:  294/3000  Loss: 4.382820499203372

Epoch:  295/3000  Loss: 4.3777383824577285

Epoch:  296/3000  Loss: 4.3739364264890055

Epoch:  297/3000  Loss: 4.373998869984623

Epoch:  298/3000  Loss: 4.371168442626498

Epoch:  299/3000  Loss: 4.37577132488912

Epoch:  300/3000  Loss: 4.36948224351272

Epoch:  301/3000  Loss: 4.371113660551787

Epoch:  302/3000  Loss: 4.363855147334382

Epoch:  303/3000  Loss: 4.364659885314521

Epoch:  304/3000  Loss: 4.3628111684910875

Epoch:  305/3000  Loss: 4.363428024692732

Epoch:  306/3000  Loss: 4.36362833270523

Epoch:  307/3000  Loss: 4.3641753068063345

Epoch:  308/3000  Loss: 4.358481199130947

Epoch:  309/3000  Loss: 4.359874903539839

Epoch:  310/3000  Loss: 4.3604419598212605

Epoch:  311/3000  Loss: 4.353505842447555

Epoch:  312/3000  Loss: 4.357612310950436

Epoch:  313/3000  Loss: 4.353355972692969

Epoch:  314/3000  Loss: 4.346458323650601

Epoch:  315/3000  Loss: 4.35018234302201

Epoch:  316/3000  Loss: 4.344698333302267

Epoch:  317/3000  Loss: 4.341590655520644

Epoch:  318/3000  Loss: 4.343310407601322

Epoch:  319/3000  Loss: 4.345374246415534

Epoch:  320/3000  Loss: 4.344978555882976

Epoch:  321/3000  Loss: 4.346803596728705

Epoch:  322/3000  Loss: 4.338442879894708

Epoch:  323/3000  Loss: 4.340926231117117

Epoch:  324/3000  Loss: 4.34090089414752

Epoch:  325/3000  Loss: 4.336120911224838

Epoch:  326/3000  Loss: 4.335363071356246

Epoch:  327/3000  Loss: 4.33729859498831

Epoch:  328/3000  Loss: 4.333099259027247

Epoch:  329/3000  Loss: 4.333121057766587

Epoch:  330/3000  Loss: 4.332333619777973

Epoch:  331/3000  Loss: 4.331443797570282

Epoch:  332/3000  Loss: 4.333964534796749

Epoch:  333/3000  Loss: 4.328504053240666

Epoch:  334/3000  Loss: 4.32219849908393

Epoch:  335/3000  Loss: 4.3208896965986

Epoch:  336/3000  Loss: 4.32644852431305

Epoch:  337/3000  Loss: 4.326357725430301

Epoch:  338/3000  Loss: 4.318040393660728

Epoch:  339/3000  Loss: 4.325297020332958

Epoch:  340/3000  Loss: 4.319674461772056

Epoch:  341/3000  Loss: 4.317297760846284

Epoch:  342/3000  Loss: 4.32121572127709

Epoch:  343/3000  Loss: 4.318433488414427

Epoch:  344/3000  Loss: 4.310665252424956

Epoch:  345/3000  Loss: 4.305829211299646

Epoch:  346/3000  Loss: 4.310641413304343

Epoch:  347/3000  Loss: 4.309657640490001

Epoch:  348/3000  Loss: 4.309536528231493

Epoch:  349/3000  Loss: 4.302452363869353

Epoch:  350/3000  Loss: 4.306641438524704

Epoch:  351/3000  Loss: 4.305147957993429

Epoch:  352/3000  Loss: 4.303995710015982

Epoch:  353/3000  Loss: 4.29975230001281

Epoch:  354/3000  Loss: 4.292061819685313

Epoch:  355/3000  Loss: 4.298900562093671

Epoch:  356/3000  Loss: 4.294326230387464

Epoch:  357/3000  Loss: 4.29105584958117

Epoch:  358/3000  Loss: 4.293353676111635

Epoch:  359/3000  Loss: 4.295453798072894

Epoch:  360/3000  Loss: 4.295814774477113

Epoch:  361/3000  Loss: 4.285674985596562

Epoch:  362/3000  Loss: 4.292738162006768

Epoch:  363/3000  Loss: 4.285363858383914

Epoch:  364/3000  Loss: 4.286061703269161

Epoch:  365/3000  Loss: 4.284784124584614

Epoch:  366/3000  Loss: 4.286443892083677

Epoch:  367/3000  Loss: 4.280792432592328

Epoch:  368/3000  Loss: 4.281697259841638

Epoch:  369/3000  Loss: 4.276309436552834

Epoch:  370/3000  Loss: 4.279616320311134

Epoch:  371/3000  Loss: 4.273645979118128

Epoch:  372/3000  Loss: 4.280715557245108

Epoch:  373/3000  Loss: 4.277782618383589

Epoch:  374/3000  Loss: 4.272528434867289

Epoch:  375/3000  Loss: 4.274524146462144

Epoch:  376/3000  Loss: 4.276080241843836

Epoch:  377/3000  Loss: 4.277318165578744

Epoch:  378/3000  Loss: 4.265717709242408

Epoch:  379/3000  Loss: 4.275792445709456

Epoch:  380/3000  Loss: 4.267072633541821

Epoch:  381/3000  Loss: 4.270066064479841

Epoch:  382/3000  Loss: 4.263192937239987

Epoch:  383/3000  Loss: 4.264794107693345

Epoch:  384/3000  Loss: 4.264885130213268

Epoch:  385/3000  Loss: 4.257181235487508

Epoch:  386/3000  Loss: 4.258735695060441

Epoch:  387/3000  Loss: 4.259306808016193

Epoch:  388/3000  Loss: 4.2597551526486805

Epoch:  389/3000  Loss: 4.2637927652637115

Epoch:  390/3000  Loss: 4.251759724283054

Epoch:  391/3000  Loss: 4.25843806545989

Epoch:  392/3000  Loss: 4.260021841758702

Epoch:  393/3000  Loss: 4.251980569688379

Epoch:  394/3000  Loss: 4.259610768984982

Epoch:  395/3000  Loss: 4.250876764205512

Epoch:  396/3000  Loss: 4.248954403961567

Epoch:  397/3000  Loss: 4.250533355774206

Epoch:  398/3000  Loss: 4.24687059629114

Epoch:  399/3000  Loss: 4.251058049919137

Epoch:  400/3000  Loss: 4.2462674725616845

Epoch:  401/3000  Loss: 4.252560740907759

Epoch:  402/3000  Loss: 4.248551780676595

Epoch:  403/3000  Loss: 4.2450157101474595

Epoch:  404/3000  Loss: 4.244663295734901

Epoch:  405/3000  Loss: 4.249045799848817

Epoch:  406/3000  Loss: 4.24466888392697

Epoch:  407/3000  Loss: 4.241096710912395

Epoch:  408/3000  Loss: 4.241183616537451

Epoch:  409/3000  Loss: 4.23890384083911

Epoch:  410/3000  Loss: 4.234016531233673

Epoch:  411/3000  Loss: 4.241292784873709

Epoch:  412/3000  Loss: 4.24286347634483

Epoch:  413/3000  Loss: 4.24119091581395

Epoch:  414/3000  Loss: 4.237679759615735

Epoch:  415/3000  Loss: 4.237171666082914

Epoch:  416/3000  Loss: 4.232448682445609

Epoch:  417/3000  Loss: 4.224660577784997

Epoch:  418/3000  Loss: 4.23591422952549

Epoch:  419/3000  Loss: 4.235766133265709

Epoch:  420/3000  Loss: 4.232937220454079

Epoch:  421/3000  Loss: 4.231707569652255

Epoch:  422/3000  Loss: 4.227444299189834

Epoch:  423/3000  Loss: 4.224296408871148

Epoch:  424/3000  Loss: 4.226660876268634

Epoch:  425/3000  Loss: 4.220538695562037

Epoch:  426/3000  Loss: 4.2264748865099095

Epoch:  427/3000  Loss: 4.22582819705442

Epoch:  428/3000  Loss: 4.2346840517939714

Epoch:  429/3000  Loss: 4.225052983561011

Epoch:  430/3000  Loss: 4.233042995636827

Epoch:  431/3000  Loss: 4.2208081994346855

Epoch:  432/3000  Loss: 4.218110343481999

Epoch:  433/3000  Loss: 4.223398798779423

Epoch:  434/3000  Loss: 4.211823292632602

Epoch:  435/3000  Loss: 4.22519421248978

Epoch:  436/3000  Loss: 4.223021960285857

Epoch:  437/3000  Loss: 4.216913990804714

Epoch:  438/3000  Loss: 4.212636113303674

Epoch:  439/3000  Loss: 4.214560507359379

Epoch:  440/3000  Loss: 4.213924254670346

Epoch:  441/3000  Loss: 4.213085216714923

Epoch:  442/3000  Loss: 4.2177137244582585

Epoch:  443/3000  Loss: 4.220669517287157

Epoch:  444/3000  Loss: 4.212695354435392

Epoch:  445/3000  Loss: 4.218120526501012

Epoch:  446/3000  Loss: 4.208387573605701

Epoch:  447/3000  Loss: 4.212855618254599

Epoch:  448/3000  Loss: 4.202952341699436

Epoch:  449/3000  Loss: 4.209051650919952

Epoch:  450/3000  Loss: 4.20035837630043

Epoch:  451/3000  Loss: 4.201893345770414

Epoch:  452/3000  Loss: 4.199396013529237

Epoch:  453/3000  Loss: 4.199234849139279

Epoch:  454/3000  Loss: 4.201297710738418

Epoch:  455/3000  Loss: 4.201089154154781

Epoch:  456/3000  Loss: 4.199565063524739

Epoch:  457/3000  Loss: 4.21125949537713

Epoch:  458/3000  Loss: 4.195838635331865

Epoch:  459/3000  Loss: 4.202967981520257

Epoch:  460/3000  Loss: 4.195478693899687

Epoch:  461/3000  Loss: 4.204930213781504

Epoch:  462/3000  Loss: 4.199786015958381

Epoch:  463/3000  Loss: 4.19950982377395

Epoch:  464/3000  Loss: 4.1967194460837085

Epoch:  465/3000  Loss: 4.200924255270361

Epoch:  466/3000  Loss: 4.196250105561126

Epoch:  467/3000  Loss: 4.194590488613405

Epoch:  468/3000  Loss: 4.190597543760227

Epoch:  469/3000  Loss: 4.191836994906928

Epoch:  470/3000  Loss: 4.187936046777718

Epoch:  471/3000  Loss: 4.191451806288499

Epoch:  472/3000  Loss: 4.192997700585016

Epoch:  473/3000  Loss: 4.189686248551554

Epoch:  474/3000  Loss: 4.194647511986997

Epoch:  475/3000  Loss: 4.192226001781109

Epoch:  476/3000  Loss: 4.182432725426797

Epoch:  477/3000  Loss: 4.190865243480345

Epoch:  478/3000  Loss: 4.187835957782276

Epoch:  485/3000  Loss: 4.181684051123881

Epoch:  486/3000  Loss: 4.177807380904013

Epoch:  487/3000  Loss: 4.178459464477206

Epoch:  488/3000  Loss: 4.178543411084076

Epoch:  489/3000  Loss: 4.182715371610376

Epoch:  490/3000  Loss: 4.177220301841622

Epoch:  491/3000  Loss: 4.1705489413199

Epoch:  492/3000  Loss: 4.1825795929139025

Epoch:  493/3000  Loss: 4.17771020083422

Epoch:  494/3000  Loss: 4.177087157519896

Epoch:  495/3000  Loss: 4.179763593027704

Epoch:  496/3000  Loss: 4.1795195887201

Epoch:  497/3000  Loss: 4.169747693941049

Epoch:  498/3000  Loss: 4.178530541407939

Epoch:  499/3000  Loss: 4.1766918690962855

Epoch:  500/3000  Loss: 4.165588439674246

Epoch:  501/3000  Loss: 4.173943084909503

Epoch:  502/3000  Loss: 4.162671798406047

Epoch:  503/3000  Loss: 4.169213136219677

Epoch:  504/3000  Loss: 4.170499004261091

Epoch:  505/3000  Loss: 4.173735249877387

Epoch:  506/3000  Loss: 4.170981244022619

Epoch:  507/3000  Loss: 4.17277560880072

Epoch:  508/3000  Loss: 4.168190664593579

Epoch:  509/3000  Loss: 4.172916030500567

Epoch:  510/3000  Loss: 4.163423664671135

Epoch:  511/3000  Loss: 4.161068524613583

Epoch:  512/3000  Loss: 4.164233222757771

Epoch:  513/3000  Loss: 4.162602108737494

Epoch:  514/3000  Loss: 4.161249061402716

Epoch:  515/3000  Loss: 4.157804503369687

Epoch:  516/3000  Loss: 4.158486846941347

Epoch:  517/3000  Loss: 4.163026809418653

Epoch:  518/3000  Loss: 4.16003457878542

Epoch:  519/3000  Loss: 4.159005715570548

Epoch:  520/3000  Loss: 4.159728724154484

Epoch:  521/3000  Loss: 4.1616965582942855

Epoch:  522/3000  Loss: 4.163217226208009

Epoch:  523/3000  Loss: 4.154044693017252

Epoch:  524/3000  Loss: 4.158872688678868

Epoch:  525/3000  Loss: 4.1560005856983695

Epoch:  526/3000  Loss: 4.149771553503796

Epoch:  527/3000  Loss: 4.1607279741942

Epoch:  528/3000  Loss: 4.1522592375938165

Epoch:  529/3000  Loss: 4.156731051227118

Epoch:  530/3000  Loss: 4.153029616199323

Epoch:  531/3000  Loss: 4.153756472875549

Epoch:  532/3000  Loss: 4.154203223306194

Epoch:  533/3000  Loss: 4.150572869862535

Epoch:  534/3000  Loss: 4.149356675065891

Epoch:  535/3000  Loss: 4.154198090052906

Epoch:  536/3000  Loss: 4.151494043976514

Epoch:  537/3000  Loss: 4.148565163979163

Epoch:  538/3000  Loss: 4.144285690633903

Epoch:  539/3000  Loss: 4.146347115973244

Epoch:  540/3000  Loss: 4.153797810989187

Epoch:  541/3000  Loss: 4.153279478049032

Epoch:  542/3000  Loss: 4.145787822666453

Epoch:  543/3000  Loss: 4.148587465286255

Epoch:  544/3000  Loss: 4.143497204807952

Epoch:  545/3000  Loss: 4.1459593641497925

Epoch:  546/3000  Loss: 4.148888659121385

Epoch:  547/3000  Loss: 4.14696513990584

Epoch:  548/3000  Loss: 4.14307217482995

Epoch:  549/3000  Loss: 4.140759123726635

Epoch:  550/3000  Loss: 4.141115379388242

Epoch:  551/3000  Loss: 4.141235233584994

Epoch:  552/3000  Loss: 4.142291369038526

Epoch:  553/3000  Loss: 4.140927690315466

Epoch:  554/3000  Loss: 4.133341080559381

Epoch:  555/3000  Loss: 4.141976866191071

Epoch:  556/3000  Loss: 4.137664890453545

Epoch:  557/3000  Loss: 4.141181124064186

Epoch:  558/3000  Loss: 4.133787172943799

Epoch:  559/3000  Loss: 4.138464515983853

Epoch:  560/3000  Loss: 4.140092312400021

Epoch:  561/3000  Loss: 4.1319387156161325

Epoch:  562/3000  Loss: 4.133447144800705

Epoch:  563/3000  Loss: 4.132064267223108

Epoch:  564/3000  Loss: 4.134617957400947

Epoch:  565/3000  Loss: 4.136176552208639

Epoch:  566/3000  Loss: 4.127210186488592

Epoch:  567/3000  Loss: 4.134790989342568

Epoch:  568/3000  Loss: 4.12750709727492

Epoch:  569/3000  Loss: 4.135401582608403

Epoch:  570/3000  Loss: 4.126790793426549

Epoch:  571/3000  Loss: 4.124037531295408

Epoch:  572/3000  Loss: 4.127944064058201

Epoch:  573/3000  Loss: 4.124163207175674

Epoch:  574/3000  Loss: 4.132883159492922

Epoch:  575/3000  Loss: 4.129646106100247

Epoch:  576/3000  Loss: 4.124846757347904

Epoch:  577/3000  Loss: 4.132792152849322

Epoch:  578/3000  Loss: 4.1267339737078625

Epoch:  579/3000  Loss: 4.128068499942872

Epoch:  580/3000  Loss: 4.1181467403363685

Epoch:  581/3000  Loss: 4.12421193133813

Epoch:  582/3000  Loss: 4.119118914401627

Epoch:  583/3000  Loss: 4.125452821755656

Epoch:  584/3000  Loss: 4.126220307859296

Epoch:  585/3000  Loss: 4.124533335459081

Epoch:  586/3000  Loss: 4.114441144069492

Epoch:  587/3000  Loss: 4.116649704056137

Epoch:  588/3000  Loss: 4.118761983175092

Epoch:  589/3000  Loss: 4.121058555749746

Epoch:  590/3000  Loss: 4.118536063368367

Epoch:  591/3000  Loss: 4.1217215964495795

Epoch:  592/3000  Loss: 4.121323701024193

Epoch:  593/3000  Loss: 4.113071121112898

Epoch:  594/3000  Loss: 4.122647988673056

Epoch:  595/3000  Loss: 4.111166757228864

Epoch:  596/3000  Loss: 4.113297980907453

Epoch:  597/3000  Loss: 4.1104431913312345

Epoch:  598/3000  Loss: 4.115510333145527

Epoch:  599/3000  Loss: 4.108876552428498

Epoch:  600/3000  Loss: 4.1141011457629375

Epoch:  601/3000  Loss: 4.114542801262454

Epoch:  602/3000  Loss: 4.118228482324138

Epoch:  603/3000  Loss: 4.1122037975714205

Epoch:  604/3000  Loss: 4.10746754406514

Epoch:  605/3000  Loss: 4.114504224807879

Epoch:  606/3000  Loss: 4.105825605403405

Epoch:  607/3000  Loss: 4.113167042847206

Epoch:  608/3000  Loss: 4.1141079202995785

Epoch:  609/3000  Loss: 4.1122546201458485

Epoch:  610/3000  Loss: 4.1110890768425605

Epoch:  611/3000  Loss: 4.1062177585268955

Epoch:  612/3000  Loss: 4.114938519989171

Epoch:  613/3000  Loss: 4.102448784524894

Epoch:  614/3000  Loss: 4.109330086702594

Epoch:  615/3000  Loss: 4.105304409797922

Epoch:  616/3000  Loss: 4.096194517461906

Epoch:  617/3000  Loss: 4.10348697453224

Epoch:  618/3000  Loss: 4.0945620052302605

Epoch:  619/3000  Loss: 4.102183619815638

Epoch:  620/3000  Loss: 4.109468069744438

Epoch:  621/3000  Loss: 4.103338933018401

Epoch:  622/3000  Loss: 4.097354830611587

Epoch:  623/3000  Loss: 4.100702872369374

Epoch:  624/3000  Loss: 4.098929058670587

Epoch:  625/3000  Loss: 4.0983616728185925

Epoch:  626/3000  Loss: 4.102619310197272

Epoch:  627/3000  Loss: 4.101881290002203

Epoch:  628/3000  Loss: 4.090593983741086

Epoch:  629/3000  Loss: 4.094801760705276

Epoch:  630/3000  Loss: 4.098726392750351

Epoch:  631/3000  Loss: 4.09802770696743

Epoch:  632/3000  Loss: 4.0955688945187765

Epoch:  633/3000  Loss: 4.093168843079927

Epoch:  634/3000  Loss: 4.090092642846529

Epoch:  635/3000  Loss: 4.102442718674477

Epoch:  636/3000  Loss: 4.097449453772141

Epoch:  637/3000  Loss: 4.096835167071302

Epoch:  638/3000  Loss: 4.088954881740356

Epoch:  639/3000  Loss: 4.094634654464459

Epoch:  640/3000  Loss: 4.092822571435835

Epoch:  641/3000  Loss: 4.09646667760768

Epoch:  642/3000  Loss: 4.088041366309988

Epoch:  643/3000  Loss: 4.090447911712523

Epoch:  644/3000  Loss: 4.087026657932251

Epoch:  645/3000  Loss: 4.086263717384207

Epoch:  646/3000  Loss: 4.0925176351680825

Epoch:  647/3000  Loss: 4.085702255043179

Epoch:  648/3000  Loss: 4.08877800553866

Epoch:  649/3000  Loss: 4.083892758212873

Epoch:  650/3000  Loss: 4.090037943438191

Epoch:  651/3000  Loss: 4.091100197024515

Epoch:  652/3000  Loss: 4.088842779569045

Epoch:  653/3000  Loss: 4.082832949854065

Epoch:  654/3000  Loss: 4.077134458397341

Epoch:  655/3000  Loss: 4.079963211077089

Epoch:  656/3000  Loss: 4.083812708148453

Epoch:  657/3000  Loss: 4.083166986874954

Epoch:  658/3000  Loss: 4.084461276210956

Epoch:  659/3000  Loss: 4.086769274537517

Epoch:  660/3000  Loss: 4.073268485808345

Epoch:  661/3000  Loss: 4.068765500109176

Epoch:  662/3000  Loss: 4.0799090046559625

Epoch:  663/3000  Loss: 4.077560861403578

Epoch:  664/3000  Loss: 4.081877350944055

Epoch:  665/3000  Loss: 4.078321358913943

Epoch:  666/3000  Loss: 4.078188994174436

Epoch:  667/3000  Loss: 4.0794288233418685

Epoch:  668/3000  Loss: 4.078154403498197

Epoch:  669/3000  Loss: 4.081790201425826

Epoch:  670/3000  Loss: 4.07246294607388

Epoch:  671/3000  Loss: 4.081118689612598

Epoch:  672/3000  Loss: 4.074116076448618

Epoch:  673/3000  Loss: 4.074407651695401

Epoch:  674/3000  Loss: 4.07406948327197

Epoch:  675/3000  Loss: 4.071216414086991

Epoch:  676/3000  Loss: 4.072689021906541

Epoch:  677/3000  Loss: 4.070045764903387

Epoch:  678/3000  Loss: 4.0690750435217105

Epoch:  679/3000  Loss: 4.066526878304377

Epoch:  680/3000  Loss: 4.07088221885033

Epoch:  681/3000  Loss: 4.0742293156931515

Epoch:  682/3000  Loss: 4.073067275035259

Epoch:  683/3000  Loss: 4.06355987241156

Epoch:  684/3000  Loss: 4.064554595235569

Epoch:  685/3000  Loss: 4.068560539512766

Epoch:  686/3000  Loss: 4.064925376364339

Epoch:  687/3000  Loss: 4.075537932042276

Epoch:  688/3000  Loss: 4.068908668139224

Epoch:  689/3000  Loss: 4.065798634092241

Epoch:  690/3000  Loss: 4.055662673321712

Epoch:  691/3000  Loss: 4.064292298392505

Epoch:  692/3000  Loss: 4.058131138574926

Epoch:  693/3000  Loss: 4.0634770431693745

Epoch:  694/3000  Loss: 4.061797266849557

Epoch:  695/3000  Loss: 4.0621133346097755

Epoch:  696/3000  Loss: 4.067000083069303

Epoch:  697/3000  Loss: 4.058782769946636

Epoch:  698/3000  Loss: 4.064852085233967

Epoch:  699/3000  Loss: 4.0664079055172975

Epoch:  700/3000  Loss: 4.061195122251281

Epoch:  701/3000  Loss: 4.054621727677355

Epoch:  702/3000  Loss: 4.055232208166549

Epoch:  703/3000  Loss: 4.052309080325366

Epoch:  704/3000  Loss: 4.051658236474038

Epoch:  705/3000  Loss: 4.050339560557999

Epoch:  706/3000  Loss: 4.05831694849323

Epoch:  707/3000  Loss: 4.056969451849551

Epoch:  708/3000  Loss: 4.059736691440972

Epoch:  709/3000  Loss: 4.052657434505108

Epoch:  710/3000  Loss: 4.050570394086783

Epoch:  711/3000  Loss: 4.057679326334449

Epoch:  712/3000  Loss: 4.054314227930851

Epoch:  713/3000  Loss: 4.048054996779537

Epoch:  714/3000  Loss: 4.052487130553796

Epoch:  715/3000  Loss: 4.04637085939795

Epoch:  716/3000  Loss: 4.052146291075837

Epoch:  717/3000  Loss: 4.043203668944734

Epoch:  718/3000  Loss: 4.051723218539005

Epoch:  719/3000  Loss: 4.052967094252769

Epoch:  720/3000  Loss: 4.042379216950741

Epoch:  721/3000  Loss: 4.044503278217688

Epoch:  722/3000  Loss: 4.048965982673637

Epoch:  723/3000  Loss: 4.0539090627917735

Epoch:  724/3000  Loss: 4.044743685169417

Epoch:  725/3000  Loss: 4.047448301698529

Epoch:  726/3000  Loss: 4.04549323704979

Epoch:  727/3000  Loss: 4.045301376336204

Epoch:  728/3000  Loss: 4.048714385377553

Epoch:  729/3000  Loss: 4.043572216165326

Epoch:  730/3000  Loss: 4.043496462015005

Epoch:  731/3000  Loss: 4.043251675387885

Epoch:  732/3000  Loss: 4.047702990497979

Epoch:  733/3000  Loss: 4.039540701153358

Epoch:  734/3000  Loss: 4.043380851449977

Epoch:  735/3000  Loss: 4.0532384868604305

Epoch:  736/3000  Loss: 4.040147473426145

Epoch:  737/3000  Loss: 4.034532715888303

Epoch:  738/3000  Loss: 4.0401650424939755

Epoch:  739/3000  Loss: 4.040473614713491

Epoch:  740/3000  Loss: 4.040428672127007

Epoch:  741/3000  Loss: 4.041830957962369

Epoch:  742/3000  Loss: 4.037716262786725

Epoch:  743/3000  Loss: 4.032378695451844

Epoch:  744/3000  Loss: 4.041883777121315

Epoch:  745/3000  Loss: 4.038576764436321

Epoch:  746/3000  Loss: 4.04285659992599

Epoch:  747/3000  Loss: 4.04247893887464

Epoch:  748/3000  Loss: 4.0318781913490165

Epoch:  749/3000  Loss: 4.038812597638294

Epoch:  750/3000  Loss: 4.031759468203434

Epoch:  751/3000  Loss: 4.038618934688284

Epoch:  752/3000  Loss: 4.035074569875146

Epoch:  753/3000  Loss: 4.034813089841543

Epoch:  754/3000  Loss: 4.0466790379941395

Epoch:  755/3000  Loss: 4.032656445979525

Epoch:  756/3000  Loss: 4.0257875224068576

Epoch:  757/3000  Loss: 4.03281918267295

Epoch:  758/3000  Loss: 4.040340845675201

Epoch:  759/3000  Loss: 4.030361054001117

Epoch:  760/3000  Loss: 4.031850712444424

Epoch:  761/3000  Loss: 4.028513043674071

Epoch:  762/3000  Loss: 4.033940463881816

Epoch:  763/3000  Loss: 4.0293467124211935

Epoch:  764/3000  Loss: 4.036531889069094

Epoch:  765/3000  Loss: 4.037449665376157

Epoch:  766/3000  Loss: 4.032369716022373

Epoch:  767/3000  Loss: 4.0238778320163595

Epoch:  768/3000  Loss: 4.0295636905731484

Epoch:  769/3000  Loss: 4.020561505130457

Epoch:  770/3000  Loss: 4.025555344043999

Epoch:  771/3000  Loss: 4.025170776792016

Epoch:  772/3000  Loss: 4.025985802082189

Epoch:  773/3000  Loss: 4.026047954603123

Epoch:  774/3000  Loss: 4.033113645768193

Epoch:  775/3000  Loss: 4.034214116675709

Epoch:  776/3000  Loss: 4.0227777071032

Epoch:  777/3000  Loss: 4.0269943050073564

Epoch:  778/3000  Loss: 4.031039996754836

Epoch:  779/3000  Loss: 4.022760562590151

Epoch:  780/3000  Loss: 4.024261249329822

Epoch:  781/3000  Loss: 4.026481894756978

Epoch:  782/3000  Loss: 4.024986896394451

Epoch:  783/3000  Loss: 4.021101821030048

Epoch:  784/3000  Loss: 4.030090617531339

Epoch:  785/3000  Loss: 4.018057981396926

Epoch:  786/3000  Loss: 4.030218233607804

Epoch:  787/3000  Loss: 4.021580189979719

Epoch:  788/3000  Loss: 4.024042364102964

Epoch:  789/3000  Loss: 4.016070258329985

Epoch:  790/3000  Loss: 4.014692414094331

Epoch:  791/3000  Loss: 4.019006707775059

Epoch:  792/3000  Loss: 4.019118852921934

Epoch:  793/3000  Loss: 4.018257610286008

Epoch:  794/3000  Loss: 4.0185153593836365

Epoch:  795/3000  Loss: 4.028244517016493

Epoch:  796/3000  Loss: 4.020390348237362

Epoch:  797/3000  Loss: 4.010819532019947

Epoch:  798/3000  Loss: 4.012885817977782

Epoch:  799/3000  Loss: 4.01934264201658

Epoch:  800/3000  Loss: 4.014255765658706

Epoch:  801/3000  Loss: 4.0208542839667825

Epoch:  802/3000  Loss: 4.022751757800237

Epoch:  803/3000  Loss: 4.022547597042045

Epoch:  804/3000  Loss: 4.020423092059235

Epoch:  805/3000  Loss: 4.021020751322451

Epoch:  806/3000  Loss: 4.0152619240888905

Epoch:  807/3000  Loss: 4.021463904670952

Epoch:  808/3000  Loss: 4.012739532440046

Epoch:  809/3000  Loss: 4.0144739161950165

Epoch:  810/3000  Loss: 4.014458083942207

Epoch:  811/3000  Loss: 4.015088163478777

Epoch:  812/3000  Loss: 4.007094704598428

Epoch:  813/3000  Loss: 4.008978568043145

Epoch:  814/3000  Loss: 4.02142211435583

Epoch:  815/3000  Loss: 4.0170146332268875

Epoch:  816/3000  Loss: 4.014818845742332

Epoch:  817/3000  Loss: 4.009395691064688

Epoch:  818/3000  Loss: 4.0125606897641

Epoch:  819/3000  Loss: 4.009683987029521

Epoch:  820/3000  Loss: 4.006101349601789

Epoch:  821/3000  Loss: 4.016601286306721

Epoch:  822/3000  Loss: 4.010503570466858

Epoch:  823/3000  Loss: 4.013353097863093

Epoch:  824/3000  Loss: 4.015827140085459

Epoch:  825/3000  Loss: 4.0075807771233825

Epoch:  826/3000  Loss: 4.010567406699249

Epoch:  827/3000  Loss: 4.009446855253795

Epoch:  828/3000  Loss: 4.011840699323967

Epoch:  829/3000  Loss: 4.004450448755556

Epoch:  830/3000  Loss: 4.003153415279192

Epoch:  831/3000  Loss: 4.00663442562423

Epoch:  832/3000  Loss: 4.007263199743803

Epoch:  833/3000  Loss: 4.005089200049399

Epoch:  834/3000  Loss: 4.010976366827053

Epoch:  835/3000  Loss: 4.01066473211952

Epoch:  836/3000  Loss: 4.007852318092548

Epoch:  837/3000  Loss: 4.006898782009646

Epoch:  838/3000  Loss: 4.007380106145288

Epoch:  839/3000  Loss: 4.008519626512319

Epoch:  840/3000  Loss: 4.0123532168763925

Epoch:  841/3000  Loss: 4.016323961155012

Epoch:  842/3000  Loss: 4.003614347098752

Epoch:  843/3000  Loss: 4.006246541589327

Epoch:  844/3000  Loss: 4.0074516325402065

Epoch:  845/3000  Loss: 4.001422453966809

Epoch:  846/3000  Loss: 4.01188165339482

Epoch:  847/3000  Loss: 4.006731347840634

Epoch:  848/3000  Loss: 4.006408838672835

Epoch:  849/3000  Loss: 4.003046072720113

Epoch:  850/3000  Loss: 4.003143684187932

Epoch:  851/3000  Loss: 3.9981533128823807

Epoch:  852/3000  Loss: 4.006268448725634

Epoch:  853/3000  Loss: 4.003367307676103

Epoch:  854/3000  Loss: 3.9977960575598663

Epoch:  855/3000  Loss: 4.003411948612445

Epoch:  856/3000  Loss: 4.002610166913196

Epoch:  857/3000  Loss: 4.002219660273923

Epoch:  858/3000  Loss: 3.9992108046666353

Epoch:  859/3000  Loss: 3.999652685993164

Epoch:  860/3000  Loss: 4.000579397111755

Epoch:  861/3000  Loss: 3.988041469068122

Epoch:  862/3000  Loss: 4.005057893308515

Epoch:  863/3000  Loss: 4.004604487687383

Epoch:  864/3000  Loss: 4.005630872827173

Epoch:  865/3000  Loss: 4.007391941670572

Epoch:  866/3000  Loss: 3.9906625649137966

Epoch:  867/3000  Loss: 4.004375557127557

Epoch:  868/3000  Loss: 3.9914614185263178

Epoch:  869/3000  Loss: 3.990404063880649

Epoch:  870/3000  Loss: 3.9974557165437123

Epoch:  871/3000  Loss: 3.9962187360813917

Epoch:  872/3000  Loss: 3.994795035549475

Epoch:  873/3000  Loss: 3.9943164440575476

Epoch:  874/3000  Loss: 3.9981327932819846

Epoch:  875/3000  Loss: 3.995991016228081

Epoch:  876/3000  Loss: 3.991871370376867

Epoch:  877/3000  Loss: 3.994794644663446

Epoch:  878/3000  Loss: 3.994138451586087

Epoch:  879/3000  Loss: 3.9999491103617935

Epoch:  880/3000  Loss: 3.993021259351658

Epoch:  881/3000  Loss: 3.9946862455076793

Epoch:  882/3000  Loss: 3.9890578480182914

Epoch:  883/3000  Loss: 3.9963787891287206

Epoch:  884/3000  Loss: 3.992506297940365

Epoch:  885/3000  Loss: 3.9892296139599948

Epoch:  886/3000  Loss: 3.9865788094074937

Epoch:  887/3000  Loss: 3.987249850131888

Epoch:  888/3000  Loss: 3.9938482337649464

Epoch:  889/3000  Loss: 3.991498217106413

Epoch:  890/3000  Loss: 3.997971955725301

Epoch:  891/3000  Loss: 4.001619219369374

Epoch:  892/3000  Loss: 3.9944759444445883

Epoch:  893/3000  Loss: 3.9988661014937916

Epoch:  894/3000  Loss: 4.004983260902554

Epoch:  895/3000  Loss: 3.99481656357013

Epoch:  896/3000  Loss: 3.985413789201686

Epoch:  897/3000  Loss: 3.9882378400261724

Epoch:  898/3000  Loss: 3.9892940663579686

Epoch:  899/3000  Loss: 3.979015365670661

Epoch:  900/3000  Loss: 3.987663031171849

Epoch:  901/3000  Loss: 3.9895601143930044

Epoch:  902/3000  Loss: 3.9841342084580256

Epoch:  903/3000  Loss: 3.9855538818784204

Epoch:  904/3000  Loss: 3.990673219295375

Epoch:  905/3000  Loss: 3.999477272603049

Epoch:  906/3000  Loss: 3.9805362495571273

Epoch:  907/3000  Loss: 3.98510632887226

Epoch:  908/3000  Loss: 3.986471641761701

Epoch:  909/3000  Loss: 3.9850907016430055

Epoch:  910/3000  Loss: 3.9873013146024894

Epoch:  911/3000  Loss: 3.9883374371293216

Epoch:  912/3000  Loss: 3.979299124291789

Epoch:  913/3000  Loss: 3.987148965677903

Epoch:  914/3000  Loss: 3.9823044392052527

Epoch:  915/3000  Loss: 3.9825047362685613

Epoch:  916/3000  Loss: 3.9860251346767703

Epoch:  917/3000  Loss: 3.9832224925267847

Epoch:  918/3000  Loss: 3.980272156773424

Epoch:  919/3000  Loss: 3.978954865108538

Epoch:  920/3000  Loss: 3.976346529720845

Epoch:  921/3000  Loss: 3.980869985475332

Epoch:  922/3000  Loss: 3.9853244416338707

Epoch:  923/3000  Loss: 3.9771751632099175

Epoch:  924/3000  Loss: 3.9839131344883913

Epoch:  925/3000  Loss: 3.9873838796955026

Epoch:  926/3000  Loss: 3.9767160254148886

Epoch:  927/3000  Loss: 3.9832757365142437

Epoch:  928/3000  Loss: 3.982743484691966

Epoch:  929/3000  Loss: 3.9728606738124457

Epoch:  930/3000  Loss: 3.9843543199939924

Epoch:  931/3000  Loss: 3.9795778758490266

Epoch:  932/3000  Loss: 3.979919282494949

Epoch:  933/3000  Loss: 3.973023229844261

Epoch:  934/3000  Loss: 3.9827094217666117

Epoch:  935/3000  Loss: 3.9754325453914814

Epoch:  936/3000  Loss: 3.9794065426739977

Epoch:  937/3000  Loss: 3.976600714583895

Epoch:  938/3000  Loss: 3.9769911558017665

Epoch:  939/3000  Loss: 3.9826684165083033

Epoch:  940/3000  Loss: 3.97724516602526

Epoch:  941/3000  Loss: 3.9765611277929542

Epoch:  942/3000  Loss: 3.9747128590650593

Epoch:  943/3000  Loss: 3.9737694017101513

Epoch:  944/3000  Loss: 3.971509627168132

Epoch:  945/3000  Loss: 3.9755918982382075

Epoch:  946/3000  Loss: 3.9764561037245354

Epoch:  947/3000  Loss: 3.9759677948278345

Epoch:  948/3000  Loss: 3.97068607738727

Epoch:  949/3000  Loss: 3.9689316604496008

Epoch:  950/3000  Loss: 3.978420450821536

Epoch:  951/3000  Loss: 3.9757573317167543

Epoch:  952/3000  Loss: 3.975378408771432

Epoch:  953/3000  Loss: 3.969272247275036

Epoch:  954/3000  Loss: 3.974607320384782

Epoch:  955/3000  Loss: 3.9752825784628483

Epoch:  956/3000  Loss: 3.9683712235000734

Epoch:  957/3000  Loss: 3.9677780183667295

Epoch:  958/3000  Loss: 3.9760615683313625

Epoch:  959/3000  Loss: 3.9699567089672065

Epoch:  960/3000  Loss: 3.9730645219713074

Epoch:  961/3000  Loss: 3.9738267286499935

Epoch:  962/3000  Loss: 3.976181634942371

Epoch:  963/3000  Loss: 3.971691428041075

Epoch:  964/3000  Loss: 3.96122138973766

Epoch:  965/3000  Loss: 3.972839617701814

Epoch:  966/3000  Loss: 3.964089647910625

Epoch:  967/3000  Loss: 3.967149721631774

Epoch:  968/3000  Loss: 3.965285058957701

Epoch:  969/3000  Loss: 3.9718321409893362

Epoch:  970/3000  Loss: 3.9735856453121463

Epoch:  971/3000  Loss: 3.966566255254121

Epoch:  972/3000  Loss: 3.9713402495181658

Epoch:  973/3000  Loss: 3.964504565218149

Epoch:  974/3000  Loss: 3.9717076856698563

Epoch:  975/3000  Loss: 3.969496739307583

Epoch:  976/3000  Loss: 3.9652968252293688

Epoch:  977/3000  Loss: 3.9664661005635486

Epoch:  978/3000  Loss: 3.9582144869181373

Epoch:  979/3000  Loss: 3.967699711686845

Epoch:  980/3000  Loss: 3.961579421905097

Epoch:  981/3000  Loss: 3.963988158240247

Epoch:  982/3000  Loss: 3.968771491067144

Epoch:  983/3000  Loss: 3.964706043425165

Epoch:  984/3000  Loss: 3.9692373029400096

Epoch:  985/3000  Loss: 3.9698640785589556

Epoch:  986/3000  Loss: 3.9739925688908926

Epoch:  987/3000  Loss: 3.960991615542862

Epoch:  988/3000  Loss: 3.9617707127134234

Epoch:  989/3000  Loss: 3.9654374051449905

Epoch:  990/3000  Loss: 3.967208720512697

Epoch:  991/3000  Loss: 3.9627872630183925

Epoch:  992/3000  Loss: 3.963901527166093

Epoch:  993/3000  Loss: 3.9677697930626152

Epoch:  994/3000  Loss: 3.956850218033818

Epoch:  995/3000  Loss: 3.960866080359668

Epoch:  996/3000  Loss: 3.9626795747386874

Epoch:  997/3000  Loss: 3.964542859183661

Epoch:  998/3000  Loss: 3.9627752742044686

Epoch:  999/3000  Loss: 3.9700124458111525

Epoch: 1000/3000  Loss: 3.9651240057020045

Epoch: 1001/3000  Loss: 3.965163746055314

Epoch: 1002/3000  Loss: 3.9705740942883847

Epoch: 1003/3000  Loss: 3.957283722409972

Epoch: 1004/3000  Loss: 3.9605991698023097

Epoch: 1005/3000  Loss: 3.9533909878144993

Epoch: 1006/3000  Loss: 3.9583680063109723

Epoch: 1007/3000  Loss: 3.9618987963202352

Epoch: 1008/3000  Loss: 3.9591701909403576

Epoch: 1009/3000  Loss: 3.9684581143437243

Epoch: 1010/3000  Loss: 3.9584557101318674

Epoch: 1011/3000  Loss: 3.9611144208196385

Epoch: 1012/3000  Loss: 3.9604377713734467

Epoch: 1013/3000  Loss: 3.9609614299988776

Epoch: 1014/3000  Loss: 3.9532719734205033

Epoch: 1015/3000  Loss: 3.958530182679677

Epoch: 1016/3000  Loss: 3.9646132610969236

Epoch: 1017/3000  Loss: 3.9612081009813345

Epoch: 1018/3000  Loss: 3.9601897982587495

Epoch: 1019/3000  Loss: 3.950689881868943

Epoch: 1020/3000  Loss: 3.9583710811168262

Epoch: 1021/3000  Loss: 3.967262731764538

Epoch: 1022/3000  Loss: 3.954271766266237

Epoch: 1023/3000  Loss: 3.963113247184885

Epoch: 1024/3000  Loss: 3.960518313056429

Epoch: 1025/3000  Loss: 3.960610212606349

Epoch: 1026/3000  Loss: 3.9535254613677067

Epoch: 1027/3000  Loss: 3.9512848292919034

Epoch: 1028/3000  Loss: 3.950547540228895

Epoch: 1029/3000  Loss: 3.961390103593075

Epoch: 1030/3000  Loss: 3.9464518150697483

Epoch: 1031/3000  Loss: 3.9521900203280005

Epoch: 1032/3000  Loss: 3.9624841651193856

Epoch: 1033/3000  Loss: 3.9632342417396167

Epoch: 1034/3000  Loss: 3.9443988460623984

Epoch: 1035/3000  Loss: 3.96252272268661

Epoch: 1036/3000  Loss: 3.9550232761352397

Epoch: 1037/3000  Loss: 3.9489618138796154

Epoch: 1038/3000  Loss: 3.958143578878637

Epoch: 1039/3000  Loss: 3.956453823742171

Epoch: 1040/3000  Loss: 3.9541750446937116

Epoch: 1041/3000  Loss: 3.9531160994046863

Epoch: 1042/3000  Loss: 3.9524723177799266

Epoch: 1043/3000  Loss: 3.9528592711338084

Epoch: 1044/3000  Loss: 3.9553100310017952

Epoch: 1045/3000  Loss: 3.953264133254094

Epoch: 1046/3000  Loss: 3.953466625629692

Epoch: 1047/3000  Loss: 3.9500602944983956

Epoch: 1048/3000  Loss: 3.9416446510604

Epoch: 1049/3000  Loss: 3.955942336508382

Epoch: 1050/3000  Loss: 3.939066191902117

Epoch: 1051/3000  Loss: 3.950167532492724

Epoch: 1052/3000  Loss: 3.9473362524665454

Epoch: 1053/3000  Loss: 3.950602991025292

Epoch: 1054/3000  Loss: 3.9456569486042117

Epoch: 1055/3000  Loss: 3.946513554532547

Epoch: 1056/3000  Loss: 3.9446817262848812

Epoch: 1057/3000  Loss: 3.949703799323291

Epoch: 1058/3000  Loss: 3.942544780560394

Epoch: 1059/3000  Loss: 3.9490254147044555

Epoch: 1060/3000  Loss: 3.9593442425251553

Epoch: 1061/3000  Loss: 3.9471142111361095

Epoch: 1062/3000  Loss: 3.9511628983078264

Epoch: 1063/3000  Loss: 3.943615250691481

Epoch: 1064/3000  Loss: 3.951348371538585

Epoch: 1065/3000  Loss: 3.9471938040719103

Epoch: 1066/3000  Loss: 3.945221821558325

Epoch: 1067/3000  Loss: 3.9521266858421704

Epoch: 1068/3000  Loss: 3.950735345910529

Epoch: 1069/3000  Loss: 3.9474171070225887

Epoch: 1070/3000  Loss: 3.9480439562748275

Epoch: 1071/3000  Loss: 3.9477770941676282

Epoch: 1072/3000  Loss: 3.9521961286065226

Epoch: 1073/3000  Loss: 3.944637296120417

Epoch: 1074/3000  Loss: 3.94439407440606

Epoch: 1075/3000  Loss: 3.9527021834004485

Epoch: 1076/3000  Loss: 3.9461725089361144

Epoch: 1077/3000  Loss: 3.9485428404452194

Epoch: 1078/3000  Loss: 3.9444152572535485

Epoch: 1079/3000  Loss: 3.942784538498976

Epoch: 1080/3000  Loss: 3.95541171365162

Epoch: 1081/3000  Loss: 3.9445438691587045

Epoch: 1082/3000  Loss: 3.9368397316346897

Epoch: 1083/3000  Loss: 3.952407241551939

Epoch: 1084/3000  Loss: 3.9402370444668966

Epoch: 1085/3000  Loss: 3.954049619276132

Epoch: 1086/3000  Loss: 3.9474602752930807

Epoch: 1087/3000  Loss: 3.9364576473849238

Epoch: 1088/3000  Loss: 3.939620025669803

Epoch: 1089/3000  Loss: 3.9393288714193178

Epoch: 1090/3000  Loss: 3.9483545620323732

Epoch: 1091/3000  Loss: 3.941799359808833

Epoch: 1092/3000  Loss: 3.948593794136179

Epoch: 1093/3000  Loss: 3.9450982101475467

Epoch: 1094/3000  Loss: 3.9407023012159064

Epoch: 1095/3000  Loss: 3.9379403175086845

Epoch: 1096/3000  Loss: 3.945164594256098

Epoch: 1097/3000  Loss: 3.948471595170714

Epoch: 1098/3000  Loss: 3.9458672001222244

Epoch: 1099/3000  Loss: 3.933752407299528

Epoch: 1100/3000  Loss: 3.938406152922305

Epoch: 1101/3000  Loss: 3.9462600134817793

Epoch: 1102/3000  Loss: 3.9346087017234512

Epoch: 1103/3000  Loss: 3.9440559952185157

Epoch: 1104/3000  Loss: 3.936049360084753

Epoch: 1105/3000  Loss: 3.951491691488623

Epoch: 1106/3000  Loss: 3.943061510539356

Epoch: 1107/3000  Loss: 3.945151938362866

Epoch: 1108/3000  Loss: 3.9452214659834293

Epoch: 1109/3000  Loss: 3.9278668333550684

Epoch: 1110/3000  Loss: 3.9358185843129383

Epoch: 1111/3000  Loss: 3.945193397191307

Epoch: 1112/3000  Loss: 3.937661599346472

Epoch: 1113/3000  Loss: 3.9367891655450027

Epoch: 1114/3000  Loss: 3.9303825514735364

Epoch: 1115/3000  Loss: 3.9341668130062204

Epoch: 1116/3000  Loss: 3.9425675354376177

Epoch: 1117/3000  Loss: 3.9431986102554197

Epoch: 1118/3000  Loss: 3.935785489022116

Epoch: 1119/3000  Loss: 3.932624178009384

Epoch: 1120/3000  Loss: 3.940242641965776

Epoch: 1121/3000  Loss: 3.946144361857295

Epoch: 1122/3000  Loss: 3.941295739566965

Epoch: 1123/3000  Loss: 3.9299498507678168

Epoch: 1124/3000  Loss: 3.9291013445016576

Epoch: 1125/3000  Loss: 3.935211379547207

Epoch: 1126/3000  Loss: 3.9351392282273547

Epoch: 1127/3000  Loss: 3.9291145557924745

Epoch: 1128/3000  Loss: 3.9319387588106807

Epoch: 1129/3000  Loss: 3.9381648886491183

Epoch: 1130/3000  Loss: 3.933614400990658

Epoch: 1131/3000  Loss: 3.933184876370786

Epoch: 1132/3000  Loss: 3.9325127158181403

Epoch: 1133/3000  Loss: 3.9280899389466244

Epoch: 1134/3000  Loss: 3.9390175786001946

Epoch: 1135/3000  Loss: 3.9248288066461083

Epoch: 1136/3000  Loss: 3.934266566682765

Epoch: 1137/3000  Loss: 3.9378127167610844

Epoch: 1138/3000  Loss: 3.933958916658649

Epoch: 1139/3000  Loss: 3.929128433067681

Epoch: 1140/3000  Loss: 3.9316336221727792

Epoch: 1141/3000  Loss: 3.930457199482091

Epoch: 1142/3000  Loss: 3.933306432345157

Epoch: 1143/3000  Loss: 3.924077654815569

Epoch: 1144/3000  Loss: 3.929146376597758

Epoch: 1145/3000  Loss: 3.931696758752028

Epoch: 1146/3000  Loss: 3.929581815969246

Epoch: 1147/3000  Loss: 3.934317624117285

Epoch: 1148/3000  Loss: 3.938762259948678

Epoch: 1149/3000  Loss: 3.927720081381902

Epoch: 1150/3000  Loss: 3.9351173421682364

Epoch: 1151/3000  Loss: 3.9280950757583986

Epoch: 1152/3000  Loss: 3.9340059480765657

Epoch: 1153/3000  Loss: 3.9281100947602337

Epoch: 1154/3000  Loss: 3.921894593299051

Epoch: 1155/3000  Loss: 3.932601385631189

Epoch: 1156/3000  Loss: 3.9364475853544425

Epoch: 1157/3000  Loss: 3.92491243764262

Epoch: 1158/3000  Loss: 3.9294319434773635

Epoch: 1159/3000  Loss: 3.93413939842811

Epoch: 1160/3000  Loss: 3.91767672583649

Epoch: 1161/3000  Loss: 3.9338063596994814

Epoch: 1162/3000  Loss: 3.9270050136969092

Epoch: 1163/3000  Loss: 3.9244644288764894

Epoch: 1164/3000  Loss: 3.92994583734278

Epoch: 1165/3000  Loss: 3.9247164583917873

Epoch: 1166/3000  Loss: 3.925047924543221

Epoch: 1167/3000  Loss: 3.9217932686876895

Epoch: 1168/3000  Loss: 3.9270835713869396

Epoch: 1169/3000  Loss: 3.9242693288454915

Epoch: 1170/3000  Loss: 3.9211993636274722

Epoch: 1171/3000  Loss: 3.932732234181821

Epoch: 1172/3000  Loss: 3.9273366520243314

Epoch: 1173/3000  Loss: 3.9243889019218297

Epoch: 1174/3000  Loss: 3.92989513137721

Epoch: 1175/3000  Loss: 3.927375203569502

Epoch: 1176/3000  Loss: 3.9223900216318297

Epoch: 1177/3000  Loss: 3.9259292485382744

Epoch: 1178/3000  Loss: 3.9200048296103933

Epoch: 1179/3000  Loss: 3.923542927108046

Epoch: 1180/3000  Loss: 3.923080025529478

Epoch: 1181/3000  Loss: 3.924259524668399

Epoch: 1182/3000  Loss: 3.9269564534985513

Epoch: 1183/3000  Loss: 3.922981065121639

Epoch: 1184/3000  Loss: 3.9211754292489194

Epoch: 1185/3000  Loss: 3.9127684564732794

Epoch: 1186/3000  Loss: 3.9214442041793456

Epoch: 1187/3000  Loss: 3.9256162320431556

Epoch: 1188/3000  Loss: 3.92197780850013

Epoch: 1189/3000  Loss: 3.9160705228623787

Epoch: 1190/3000  Loss: 3.9293599640597705

Epoch: 1191/3000  Loss: 3.921926341839691

Epoch: 1192/3000  Loss: 3.9227412529845735

Epoch: 1193/3000  Loss: 3.9253056788417147

Epoch: 1194/3000  Loss: 3.925730062270137

Epoch: 1195/3000  Loss: 3.9122988780795773

Epoch: 1196/3000  Loss: 3.91986358590022

Epoch: 1197/3000  Loss: 3.9216472062396677

Epoch: 1198/3000  Loss: 3.918491703497693

Epoch: 1199/3000  Loss: 3.9241361664576044

Epoch: 1200/3000  Loss: 3.9098960069509654

Epoch: 1201/3000  Loss: 3.91365140833893

Epoch: 1202/3000  Loss: 3.923562360275216

Epoch: 1203/3000  Loss: 3.9169025558007435

Epoch: 1204/3000  Loss: 3.910462324709624

Epoch: 1205/3000  Loss: 3.913846941410332

Epoch: 1206/3000  Loss: 3.9110639716672844

Epoch: 1207/3000  Loss: 3.9136225246534555

Epoch: 1208/3000  Loss: 3.91649602858262

Epoch: 1209/3000  Loss: 3.9047035180057916

Epoch: 1210/3000  Loss: 3.928349898137948

Epoch: 1211/3000  Loss: 3.9144881215079095

Epoch: 1212/3000  Loss: 3.9075594709332857

Epoch: 1213/3000  Loss: 3.9189127496134675

Epoch: 1214/3000  Loss: 3.9198775740359597

Epoch: 1215/3000  Loss: 3.9179727179859043

Epoch: 1216/3000  Loss: 3.9130161928117753

Epoch: 1217/3000  Loss: 3.919691774793115

Epoch: 1218/3000  Loss: 3.9115127003425574

Epoch: 1219/3000  Loss: 3.918952482849804

Epoch: 1220/3000  Loss: 3.907158355351567

Epoch: 1221/3000  Loss: 3.917057240187232

Epoch: 1222/3000  Loss: 3.92438553723621

Epoch: 1223/3000  Loss: 3.912995695383486

Epoch: 1224/3000  Loss: 3.915103296461664

Epoch: 1225/3000  Loss: 3.9122923611225957

Epoch: 1226/3000  Loss: 3.9207891073894827

Epoch: 1227/3000  Loss: 3.91204608759568

Epoch: 1228/3000  Loss: 3.9145709690899855

Epoch: 1229/3000  Loss: 3.908064247409457

Epoch: 1230/3000  Loss: 3.9191573566738964

Epoch: 1231/3000  Loss: 3.918724009418597

Epoch: 1232/3000  Loss: 3.908110406177052

Epoch: 1233/3000  Loss: 3.915428476957722

Epoch: 1234/3000  Loss: 3.90944044817192

Epoch: 1235/3000  Loss: 3.9140272759132078

Epoch: 1236/3000  Loss: 3.9099406147660125

Epoch: 1237/3000  Loss: 3.907988591528103

Epoch: 1238/3000  Loss: 3.9094956244174157

Epoch: 1239/3000  Loss: 3.9150338805228233

Epoch: 1240/3000  Loss: 3.9222489615942937

Epoch: 1241/3000  Loss: 3.9122779602024504

Epoch: 1242/3000  Loss: 3.9173969811878577

Epoch: 1243/3000  Loss: 3.9064477001895312

Epoch: 1244/3000  Loss: 3.9110114516949133

Epoch: 1245/3000  Loss: 3.9081642323876085

Epoch: 1246/3000  Loss: 3.905946482199615

Epoch: 1247/3000  Loss: 3.9205522049992556

Epoch: 1248/3000  Loss: 3.9125480777771138

Epoch: 1249/3000  Loss: 3.9144747714361285

Epoch: 1250/3000  Loss: 3.9023266544845705

Epoch: 1251/3000  Loss: 3.9070373413620256

Epoch: 1252/3000  Loss: 3.9089206529676437

Epoch: 1253/3000  Loss: 3.908880655308405

Epoch: 1254/3000  Loss: 3.912157632179567

Epoch: 1255/3000  Loss: 3.9114189985562136

Epoch: 1256/3000  Loss: 3.904248695833401

Epoch: 1257/3000  Loss: 3.896016074650323

Epoch: 1258/3000  Loss: 3.9081826533023034

Epoch: 1259/3000  Loss: 3.9108289810053654

Epoch: 1260/3000  Loss: 3.9109497907925417

Epoch: 1261/3000  Loss: 3.9128513084350307

Epoch: 1262/3000  Loss: 3.9119147567332955

Epoch: 1263/3000  Loss: 3.90818654300151

Epoch: 1264/3000  Loss: 3.903237825419955

Epoch: 1265/3000  Loss: 3.9043015043168885

Epoch: 1266/3000  Loss: 3.8979776995053386

Epoch: 1267/3000  Loss: 3.9048801921949594

Epoch: 1268/3000  Loss: 3.909579302769167

Epoch: 1269/3000  Loss: 3.9047846788653824

Epoch: 1270/3000  Loss: 3.904804438592098

Epoch: 1271/3000  Loss: 3.903769437262166

Epoch: 1272/3000  Loss: 3.905649076783698

Epoch: 1273/3000  Loss: 3.9047376077566573

Epoch: 1274/3000  Loss: 3.9052348314826166

Epoch: 1275/3000  Loss: 3.900938028583023

Epoch: 1276/3000  Loss: 3.8988934411794527

Epoch: 1277/3000  Loss: 3.897923444907783

Epoch: 1278/3000  Loss: 3.8965805069587263

Epoch: 1279/3000  Loss: 3.91169931913216

Epoch: 1280/3000  Loss: 3.8995218558919142

Epoch: 1281/3000  Loss: 3.908934116910985

Epoch: 1282/3000  Loss: 3.903656511711883

Epoch: 1283/3000  Loss: 3.89793575371174

Epoch: 1284/3000  Loss: 3.9009251370085094

Epoch: 1285/3000  Loss: 3.894393330463449

Epoch: 1286/3000  Loss: 3.899701727791577

Epoch: 1287/3000  Loss: 3.909464655185266

Epoch: 1288/3000  Loss: 3.9057686988576545

Epoch: 1289/3000  Loss: 3.8980893604243527

Epoch: 1290/3000  Loss: 3.902369281591422

Epoch: 1291/3000  Loss: 3.9023095699183292

Epoch: 1292/3000  Loss: 3.902854004355167

Epoch: 1293/3000  Loss: 3.9109671033756332

Epoch: 1294/3000  Loss: 3.8973687050947503

Epoch: 1295/3000  Loss: 3.897195138559002

Epoch: 1296/3000  Loss: 3.9054562889475775

Epoch: 1297/3000  Loss: 3.8986646921024257

Epoch: 1298/3000  Loss: 3.898587962926871

Epoch: 1299/3000  Loss: 3.9102908604180633

Epoch: 1300/3000  Loss: 3.9087039124130794

Epoch: 1301/3000  Loss: 3.9008575030500934

Epoch: 1302/3000  Loss: 3.8952037165824636

Epoch: 1303/3000  Loss: 3.8982453855389707

Epoch: 1304/3000  Loss: 3.889735753719623

Epoch: 1305/3000  Loss: 3.894290628170447

Epoch: 1306/3000  Loss: 3.8984702903559234

Epoch: 1307/3000  Loss: 3.8913701708363337

Epoch: 1308/3000  Loss: 3.895184054303525

Epoch: 1309/3000  Loss: 3.898532590417172

Epoch: 1310/3000  Loss: 3.894158210601106

Epoch: 1311/3000  Loss: 3.901026347474581

Epoch: 1312/3000  Loss: 3.894749404915986

Epoch: 1313/3000  Loss: 3.8957848915686975

Epoch: 1314/3000  Loss: 3.894801217844511

Epoch: 1315/3000  Loss: 3.8933717684411837

Epoch: 1316/3000  Loss: 3.9009067872635397

Epoch: 1317/3000  Loss: 3.8947951032702055

</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">workspace_utils</span> <span class="k">import</span> <span class="n">active_session</span>

<span class="k">with</span> <span class="n">active_session</span><span class="p">():</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    DON&#39;T MODIFY ANYTHING IN THIS CELL</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;sequence_length: </span><span class="si">{}</span><span class="se">\n</span><span class="s1">batch_size: </span><span class="si">{}</span><span class="se">\n</span><span class="s1">learning_rate: </span><span class="si">{}</span><span class="se">\n</span><span class="s1">embedding_dim: </span><span class="si">{}</span><span class="se">\n</span><span class="s1">n_layers: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">sequence_length</span><span class="p">,</span><span class="n">batch_size</span><span class="p">,</span><span class="n">learning_rate</span><span class="p">,</span><span class="n">embedding_dim</span><span class="p">,</span><span class="n">n_layers</span><span class="p">))</span>

    <span class="c1"># create model and move to gpu if available</span>
    <span class="n">rnn</span> <span class="o">=</span> <span class="n">RNN</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">,</span> <span class="n">n_layers</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">train_on_gpu</span><span class="p">:</span>
        <span class="n">rnn</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>

    <span class="c1"># defining loss and optimization functions for training</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">rnn</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>
    <span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>

    <span class="c1"># training the model</span>
    <span class="n">trained_rnn</span> <span class="o">=</span> <span class="n">train_rnn</span><span class="p">(</span><span class="n">rnn</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">num_epochs</span><span class="p">,</span> <span class="n">show_every_n_batches</span><span class="p">)</span>

    <span class="c1"># saving the trained model</span>
    <span class="n">helper</span><span class="o">.</span><span class="n">save_model</span><span class="p">(</span><span class="s1">&#39;/trained_rnn&#39;</span><span class="p">,</span> <span class="n">trained_rnn</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Model Trained and Saved&#39;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;sequence_length: </span><span class="si">{}</span><span class="se">\n</span><span class="s1">batch_size: </span><span class="si">{}</span><span class="se">\n</span><span class="s1">learning_rate: </span><span class="si">{}</span><span class="se">\n</span><span class="s1">embedding_dim: </span><span class="si">{}</span><span class="se">\n</span><span class="s1">n_layers: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">sequence_length</span><span class="p">,</span><span class="n">batch_size</span><span class="p">,</span><span class="n">learning_rate</span><span class="p">,</span><span class="n">embedding_dim</span><span class="p">,</span><span class="n">n_layers</span><span class="p">))</span>
    
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>sequence_length: 8
batch_size: 256
learning_rate: 0.0001
embedding_dim: 1000
n_layers: 3
Training for 3000 epoch(s)...
Epoch:    1/3000  Loss: 9.888944155375162

Epoch:    2/3000  Loss: 8.481809662128317

Epoch:    3/3000  Loss: 7.270790422373804

Epoch:    4/3000  Loss: 6.663805714968977

Epoch:    5/3000  Loss: 6.331771177533029

Epoch:    6/3000  Loss: 6.158400963092673

Epoch:    7/3000  Loss: 6.059639822203538

Epoch:    8/3000  Loss: 6.00237037724462

Epoch:    9/3000  Loss: 5.962919726316956

Epoch:   10/3000  Loss: 5.929200765456276

Epoch:   11/3000  Loss: 5.91067595317446

Epoch:   12/3000  Loss: 5.88859114044014

Epoch:   13/3000  Loss: 5.818020778962936

Epoch:   14/3000  Loss: 5.713282588432575

Epoch:   15/3000  Loss: 5.624934824581804

Epoch:   16/3000  Loss: 5.5624129065151875

Epoch:   17/3000  Loss: 5.516269770435903

Epoch:   18/3000  Loss: 5.477379350552614

Epoch:   19/3000  Loss: 5.445544637482742

Epoch:   20/3000  Loss: 5.418590889853992

Epoch:   21/3000  Loss: 5.390572992960612

Epoch:   22/3000  Loss: 5.361755854507972

Epoch:   23/3000  Loss: 5.341296075404375

Epoch:   24/3000  Loss: 5.315270054477384

Epoch:   25/3000  Loss: 5.294562849505194

Epoch:   26/3000  Loss: 5.272581096079158

Epoch:   27/3000  Loss: 5.249935883489148

Epoch:   28/3000  Loss: 5.231142239186956

Epoch:   29/3000  Loss: 5.208017052179096

Epoch:   30/3000  Loss: 5.1830969185664735

Epoch:   31/3000  Loss: 5.1650522506100005

Epoch:   32/3000  Loss: 5.146690628446382

Epoch:   33/3000  Loss: 5.128263481183984

Epoch:   34/3000  Loss: 5.105310489391458

Epoch:   35/3000  Loss: 5.090835256686156

Epoch:   36/3000  Loss: 5.0719357424768905

Epoch:   37/3000  Loss: 5.052349091672349

Epoch:   38/3000  Loss: 5.035270773131272

Epoch:   39/3000  Loss: 5.016283314803551

Epoch:   40/3000  Loss: 5.000937374158838

Epoch:   41/3000  Loss: 4.983632001109507

Epoch:   42/3000  Loss: 4.968835680512176

Epoch:   43/3000  Loss: 4.954806302607745

Epoch:   44/3000  Loss: 4.9400908985357175

Epoch:   45/3000  Loss: 4.926559140216345

Epoch:   46/3000  Loss: 4.914942750163462

Epoch:   47/3000  Loss: 4.898838065136438

Epoch:   48/3000  Loss: 4.888891316556382

Epoch:   49/3000  Loss: 4.877152761919745

Epoch:   50/3000  Loss: 4.862833734490406

Epoch:   51/3000  Loss: 4.8498443976215935

Epoch:   52/3000  Loss: 4.842068936358923

Epoch:   53/3000  Loss: 4.827962635303366

Epoch:   54/3000  Loss: 4.8178695821214

Epoch:   55/3000  Loss: 4.809771521338101

Epoch:   56/3000  Loss: 4.799425012215801

Epoch:   57/3000  Loss: 4.79076702402926

Epoch:   58/3000  Loss: 4.783831889053871

Epoch:   59/3000  Loss: 4.770017063754729

Epoch:   60/3000  Loss: 4.76201300895077

Epoch:   61/3000  Loss: 4.753050274684512

Epoch:   62/3000  Loss: 4.744755374426129

Epoch:   63/3000  Loss: 4.737036342182378

Epoch:   64/3000  Loss: 4.729573430686162

Epoch:   65/3000  Loss: 4.71940986041365

Epoch:   66/3000  Loss: 4.711165758111011

Epoch:   67/3000  Loss: 4.704487238259151

Epoch:   68/3000  Loss: 4.6935423423504

Epoch:   69/3000  Loss: 4.6882408240745805

Epoch:   70/3000  Loss: 4.679530564669905

Epoch:   71/3000  Loss: 4.67136406295601

Epoch:   72/3000  Loss: 4.663981197072172

Epoch:   73/3000  Loss: 4.65562252176219

Epoch:   74/3000  Loss: 4.649994589268476

Epoch:   75/3000  Loss: 4.642154975321101

Epoch:   76/3000  Loss: 4.638206468779465

Epoch:   77/3000  Loss: 4.630292976313624

Epoch:   78/3000  Loss: 4.619184975240422

Epoch:   79/3000  Loss: 4.615716556570995

Epoch:   80/3000  Loss: 4.6064948964393

Epoch:   81/3000  Loss: 4.6020752999974395

Epoch:   82/3000  Loss: 4.592967629706723

Epoch:   83/3000  Loss: 4.5885043072974545

Epoch:   84/3000  Loss: 4.5824297296589815

Epoch:   85/3000  Loss: 4.576432696704207

Epoch:   86/3000  Loss: 4.570240430722292

Epoch:   87/3000  Loss: 4.567068097783231

Epoch:   88/3000  Loss: 4.56401583518105

Epoch:   89/3000  Loss: 4.556694277401628

Epoch:   90/3000  Loss: 4.550167946431829

Epoch:   91/3000  Loss: 4.544633467444058

Epoch:   92/3000  Loss: 4.542120945042577

Epoch:   93/3000  Loss: 4.539091068574752

Epoch:   94/3000  Loss: 4.532312250685417

Epoch:   95/3000  Loss: 4.528126143313003

Epoch:   96/3000  Loss: 4.524165603484231

Epoch:   97/3000  Loss: 4.517687006106322

Epoch:   98/3000  Loss: 4.514369934454732

Epoch:   99/3000  Loss: 4.509317235288949

Epoch:  100/3000  Loss: 4.50911288480649

Epoch:  101/3000  Loss: 4.502738163389009

Epoch:  102/3000  Loss: 4.501709584532113

Epoch:  103/3000  Loss: 4.499342722180246

Epoch:  104/3000  Loss: 4.493284241358439

Epoch:  105/3000  Loss: 4.490689717216053

Epoch:  106/3000  Loss: 4.485848378587043

Epoch:  107/3000  Loss: 4.484968472075188

Epoch:  108/3000  Loss: 4.47676519196609

Epoch:  109/3000  Loss: 4.479678998596367

Epoch:  110/3000  Loss: 4.472187213239999

Epoch:  111/3000  Loss: 4.470700100646622

Epoch:  112/3000  Loss: 4.469371520513776

Epoch:  113/3000  Loss: 4.4607371450840745

Epoch:  114/3000  Loss: 4.463491578485774

Epoch:  115/3000  Loss: 4.454239455584822

Epoch:  116/3000  Loss: 4.4530116119604

Epoch:  117/3000  Loss: 4.456184116999308

Epoch:  118/3000  Loss: 4.447780833847221

Epoch:  119/3000  Loss: 4.446387206000844

Epoch:  120/3000  Loss: 4.44238138418088

Epoch:  121/3000  Loss: 4.437591348297294

Epoch:  122/3000  Loss: 4.439608782187276

Epoch:  123/3000  Loss: 4.435135083911063

Epoch:  124/3000  Loss: 4.430118465971673

Epoch:  125/3000  Loss: 4.425297399498951

Epoch:  126/3000  Loss: 4.4270174382746905

Epoch:  127/3000  Loss: 4.424938863995432

Epoch:  128/3000  Loss: 4.420666355374216

Epoch:  129/3000  Loss: 4.418098529727979

Epoch:  130/3000  Loss: 4.417017075659214

Epoch:  131/3000  Loss: 4.407645333498373

Epoch:  132/3000  Loss: 4.411269883999879

Epoch:  133/3000  Loss: 4.405949911029859

Epoch:  134/3000  Loss: 4.40727134408622

Epoch:  135/3000  Loss: 4.401274718361339

Epoch:  136/3000  Loss: 4.398213485739697

Epoch:  137/3000  Loss: 4.39427328603021

Epoch:  138/3000  Loss: 4.3958596284362095

Epoch:  139/3000  Loss: 4.388008761131901

Epoch:  140/3000  Loss: 4.382879170604136

Epoch:  141/3000  Loss: 4.385052267710368

Epoch:  142/3000  Loss: 4.380059553014821

Epoch:  143/3000  Loss: 4.378343023650948

Epoch:  144/3000  Loss: 4.373091884591114

Epoch:  145/3000  Loss: 4.37133113378766

Epoch:  146/3000  Loss: 4.368037112553915

Epoch:  147/3000  Loss: 4.364958046770644

Epoch:  148/3000  Loss: 4.3641739549307985

Epoch:  149/3000  Loss: 4.360327251478173

Epoch:  150/3000  Loss: 4.356453879674276

Epoch:  151/3000  Loss: 4.354044191316627

Epoch:  152/3000  Loss: 4.352073711088334

Epoch:  153/3000  Loss: 4.347139139833121

Epoch:  154/3000  Loss: 4.345684431339133

Epoch:  155/3000  Loss: 4.337467853502296

Epoch:  156/3000  Loss: 4.337300051217792

Epoch:  157/3000  Loss: 4.334567774301288

Epoch:  158/3000  Loss: 4.3289725912028345

Epoch:  159/3000  Loss: 4.33039379558344

Epoch:  160/3000  Loss: 4.323213491768673

Epoch:  161/3000  Loss: 4.320515810758218

Epoch:  162/3000  Loss: 4.315676140511173

Epoch:  163/3000  Loss: 4.316089785784141

Epoch:  164/3000  Loss: 4.30994874307479

Epoch:  165/3000  Loss: 4.307934476041246

Epoch:  166/3000  Loss: 4.303053112687736

Epoch:  167/3000  Loss: 4.298272517631794

Epoch:  168/3000  Loss: 4.297026754247732

Epoch:  169/3000  Loss: 4.297108624447351

Epoch:  170/3000  Loss: 4.297993562961447

Epoch:  171/3000  Loss: 4.288578544814011

Epoch:  172/3000  Loss: 4.283576033033174

Epoch:  173/3000  Loss: 4.286774770144759

Epoch:  174/3000  Loss: 4.281151915692735

Epoch:  175/3000  Loss: 4.27921695928464

Epoch:  176/3000  Loss: 4.27620651639741

Epoch:  177/3000  Loss: 4.276224165949328

Epoch:  178/3000  Loss: 4.269819750731019

Epoch:  179/3000  Loss: 4.269685965022822

Epoch:  180/3000  Loss: 4.264791983023457

Epoch:  181/3000  Loss: 4.260813918606988

Epoch:  182/3000  Loss: 4.256968408343436

Epoch:  183/3000  Loss: 4.25510570482276

Epoch:  184/3000  Loss: 4.251771648998918

Epoch:  185/3000  Loss: 4.247830682513357

Epoch:  186/3000  Loss: 4.246004883448283

Epoch:  187/3000  Loss: 4.239662650535847

Epoch:  188/3000  Loss: 4.2407279672293825

Epoch:  189/3000  Loss: 4.235931547208764

Epoch:  190/3000  Loss: 4.231658464190604

Epoch:  191/3000  Loss: 4.227922789803866

Epoch:  192/3000  Loss: 4.227247528098095

Epoch:  193/3000  Loss: 4.221027190109779

Epoch:  194/3000  Loss: 4.221546050323837

Epoch:  195/3000  Loss: 4.218442141872713

Epoch:  196/3000  Loss: 4.2169736264765945

Epoch:  197/3000  Loss: 4.212291082294508

Epoch:  198/3000  Loss: 4.211051874599238

Epoch:  199/3000  Loss: 4.206424755337594

Epoch:  200/3000  Loss: 4.207415430573211

Epoch:  201/3000  Loss: 4.2035459129289645

Epoch:  202/3000  Loss: 4.201158343238392

Epoch:  203/3000  Loss: 4.2028868773887895

Epoch:  204/3000  Loss: 4.1963885647126995

Epoch:  205/3000  Loss: 4.1971663491479285

Epoch:  206/3000  Loss: 4.192559139207862

Epoch:  207/3000  Loss: 4.193521774226221

Epoch:  208/3000  Loss: 4.191504150149466

Epoch:  209/3000  Loss: 4.182039977764261

Epoch:  210/3000  Loss: 4.186182845872024

Epoch:  211/3000  Loss: 4.180613050789669

Epoch:  212/3000  Loss: 4.178598889537241

Epoch:  213/3000  Loss: 4.179290836706929

Epoch:  214/3000  Loss: 4.172314960655124

Epoch:  215/3000  Loss: 4.168372483637141

Epoch:  216/3000  Loss: 4.167540943211523

Epoch:  217/3000  Loss: 4.167103592006639

Epoch:  218/3000  Loss: 4.164677030190655

Epoch:  219/3000  Loss: 4.157294687183424

Epoch:  220/3000  Loss: 4.1594087260893025

Epoch:  221/3000  Loss: 4.158639998271547

Epoch:  222/3000  Loss: 4.157936617971837

Epoch:  223/3000  Loss: 4.151059730025543

Epoch:  224/3000  Loss: 4.145502160061365

Epoch:  225/3000  Loss: 4.151499118476078

Epoch:  226/3000  Loss: 4.143201029437712

Epoch:  227/3000  Loss: 4.149409703002579

Epoch:  228/3000  Loss: 4.142744062686789

Epoch:  229/3000  Loss: 4.143023743026558

Epoch:  230/3000  Loss: 4.139037861769227

Epoch:  231/3000  Loss: 4.134583139419556

Epoch:  232/3000  Loss: 4.131977011691565

Epoch:  233/3000  Loss: 4.1280869993670235

Epoch:  234/3000  Loss: 4.130171466695852

Epoch:  235/3000  Loss: 4.130101025789633

Epoch:  236/3000  Loss: 4.123047327721256

Epoch:  237/3000  Loss: 4.125711536955559

Epoch:  238/3000  Loss: 4.122529707831898

Epoch:  239/3000  Loss: 4.120281826764688

Epoch:  240/3000  Loss: 4.120215300855966

Epoch:  241/3000  Loss: 4.118172582538649

Epoch:  242/3000  Loss: 4.1097102362534095

Epoch:  243/3000  Loss: 4.116207091561679

Epoch:  244/3000  Loss: 4.111574233263388

Epoch:  245/3000  Loss: 4.107648270157562

Epoch:  246/3000  Loss: 4.110441199664412

Epoch:  247/3000  Loss: 4.1049389296564565

Epoch:  248/3000  Loss: 4.10286290179724

Epoch:  249/3000  Loss: 4.10319752802794

Epoch:  250/3000  Loss: 4.099112898179855

Epoch:  251/3000  Loss: 4.097860862468851

Epoch:  252/3000  Loss: 4.100058340752262

Epoch:  253/3000  Loss: 4.100495207446745

Epoch:  254/3000  Loss: 4.092600318207138

Epoch:  255/3000  Loss: 4.0924858542694444

Epoch:  256/3000  Loss: 4.092412940387068

Epoch:  257/3000  Loss: 4.087919860598685

Epoch:  258/3000  Loss: 4.088456792941039

Epoch:  259/3000  Loss: 4.0864330949454475

Epoch:  260/3000  Loss: 4.08336464783241

Epoch:  261/3000  Loss: 4.0807456827711786

Epoch:  262/3000  Loss: 4.079072431586255

Epoch:  263/3000  Loss: 4.076521766596827

Epoch:  264/3000  Loss: 4.077398655880456

Epoch:  265/3000  Loss: 4.074545204228368

Epoch:  266/3000  Loss: 4.074894256153326

Epoch:  267/3000  Loss: 4.073629896668182

Epoch:  268/3000  Loss: 4.071030986720118

Epoch:  269/3000  Loss: 4.070051602111466

Epoch:  270/3000  Loss: 4.072291199366251

Epoch:  271/3000  Loss: 4.0676104507227056

Epoch:  272/3000  Loss: 4.062425337714711

Epoch:  273/3000  Loss: 4.060272211315988

Epoch:  274/3000  Loss: 4.062026050721092

Epoch:  275/3000  Loss: 4.055241054227983

Epoch:  276/3000  Loss: 4.063318018529607

Epoch:  277/3000  Loss: 4.055101902731534

Epoch:  278/3000  Loss: 4.05276629787752

Epoch:  279/3000  Loss: 4.048147752915305

Epoch:  280/3000  Loss: 4.051092288137853

Epoch:  281/3000  Loss: 4.047393582881182

Epoch:  282/3000  Loss: 4.046558491388956

Epoch:  283/3000  Loss: 4.047021520548854

Epoch:  284/3000  Loss: 4.043351629958756

Epoch:  285/3000  Loss: 4.039079212320262

Epoch:  286/3000  Loss: 4.0426463631377825

Epoch:  287/3000  Loss: 4.0422598959385665

Epoch:  288/3000  Loss: 4.039710948659086

Epoch:  289/3000  Loss: 4.038815020418715

Epoch:  290/3000  Loss: 4.036695982395917

Epoch:  291/3000  Loss: 4.037032154784805

Epoch:  292/3000  Loss: 4.032460764084739

Epoch:  293/3000  Loss: 4.028678277443195

Epoch:  294/3000  Loss: 4.025239330050589

Epoch:  295/3000  Loss: 4.03185835980821

Epoch:  296/3000  Loss: 4.023752835701252

Epoch:  297/3000  Loss: 4.0288190753980615

Epoch:  298/3000  Loss: 4.0242784593297145

Epoch:  299/3000  Loss: 4.023781746283345

Epoch:  300/3000  Loss: 4.022630442696056

Epoch:  301/3000  Loss: 4.019813436201249

Epoch:  302/3000  Loss: 4.017843118755296

Epoch:  303/3000  Loss: 4.013617051880935

Epoch:  304/3000  Loss: 4.013720467994953

Epoch:  305/3000  Loss: 4.00808219471197

Epoch:  306/3000  Loss: 4.010738368418025

Epoch:  307/3000  Loss: 4.008808805202616

Epoch:  308/3000  Loss: 4.002797079634393

Epoch:  309/3000  Loss: 4.001893296186951

Epoch:  310/3000  Loss: 4.001263725894621

Epoch:  311/3000  Loss: 3.9982403234503736

Epoch:  312/3000  Loss: 3.9966521290526993

Epoch:  313/3000  Loss: 3.9910179516364788

Epoch:  314/3000  Loss: 3.997220469069207

Epoch:  315/3000  Loss: 3.9982266163003857

Epoch:  316/3000  Loss: 3.9897654451172926

Epoch:  317/3000  Loss: 3.9874932124696927

Epoch:  318/3000  Loss: 3.9831537400168933

Epoch:  319/3000  Loss: 3.9787972104960474

Epoch:  320/3000  Loss: 3.978260491908282

Epoch:  321/3000  Loss: 3.974139692043436

Epoch:  322/3000  Loss: 3.9773215485715316

Epoch:  323/3000  Loss: 3.9789955511860464

Epoch:  324/3000  Loss: 3.96740514272931

Epoch:  325/3000  Loss: 3.9702614323846226

Epoch:  326/3000  Loss: 3.9678739153105638

Epoch:  327/3000  Loss: 3.967497314803902

Epoch:  328/3000  Loss: 3.964910660118892

Epoch:  329/3000  Loss: 3.9581525035288143

Epoch:  330/3000  Loss: 3.956315794758413

Epoch:  331/3000  Loss: 3.9600702790008193

Epoch:  332/3000  Loss: 3.954203392445356

Epoch:  333/3000  Loss: 3.958120235355421

Epoch:  334/3000  Loss: 3.9509506039235784

Epoch:  335/3000  Loss: 3.9559976418813068

Epoch:  336/3000  Loss: 3.9517001256175424

Epoch:  337/3000  Loss: 3.948644901692182

Epoch:  338/3000  Loss: 3.950349158802252

Epoch:  339/3000  Loss: 3.943711496221608

Epoch:  340/3000  Loss: 3.9482395215966237

Epoch:  341/3000  Loss: 3.942895445878478

Epoch:  342/3000  Loss: 3.9468135724122497

Epoch:  343/3000  Loss: 3.942015593079315

Epoch:  344/3000  Loss: 3.9399083981568785

Epoch:  345/3000  Loss: 3.9414989120658785

Epoch:  346/3000  Loss: 3.9346915568428478

Epoch:  347/3000  Loss: 3.936386562215871

Epoch:  348/3000  Loss: 3.9357741794366947

Epoch:  349/3000  Loss: 3.930531201965507

Epoch:  350/3000  Loss: 3.9266009939127957

Epoch:  351/3000  Loss: 3.931356386754705

Epoch:  352/3000  Loss: 3.9266262498395195

Epoch:  353/3000  Loss: 3.928726611192199

Epoch:  354/3000  Loss: 3.917225292907364

Epoch:  355/3000  Loss: 3.921060142845943

Epoch:  356/3000  Loss: 3.9135675704342194

Epoch:  357/3000  Loss: 3.9217409281895077

Epoch:  358/3000  Loss: 3.9168296211067286

Epoch:  359/3000  Loss: 3.914731964023634

Epoch:  360/3000  Loss: 3.9094592549334997

Epoch:  361/3000  Loss: 3.9094821995702285

Epoch:  362/3000  Loss: 3.9112948878058074

Epoch:  363/3000  Loss: 3.904132978395484

Epoch:  364/3000  Loss: 3.907779021098696

Epoch:  365/3000  Loss: 3.8958230150157007

Epoch:  366/3000  Loss: 3.8986173646203404

Epoch:  367/3000  Loss: 3.8980283622083993

Epoch:  368/3000  Loss: 3.890523889695091

Epoch:  369/3000  Loss: 3.8946077807196255

Epoch:  370/3000  Loss: 3.888363343271716

Epoch:  371/3000  Loss: 3.888377746494337

Epoch:  372/3000  Loss: 3.8846568578961254

Epoch:  373/3000  Loss: 3.8840001637908235

Epoch:  374/3000  Loss: 3.871788582856628

Epoch:  375/3000  Loss: 3.8762750165215856

Epoch:  376/3000  Loss: 3.8714209370229438

Epoch:  377/3000  Loss: 3.8719925041856436

Epoch:  378/3000  Loss: 3.8685960654554696

Epoch:  379/3000  Loss: 3.862771650840496

Epoch:  380/3000  Loss: 3.864411653321365

Epoch:  381/3000  Loss: 3.8608237359715605

Epoch:  382/3000  Loss: 3.861357044351512

Epoch:  383/3000  Loss: 3.8623777060673157

Epoch:  384/3000  Loss: 3.8608371975778164

Epoch:  385/3000  Loss: 3.8538774676706598

Epoch:  386/3000  Loss: 3.8486099643268803

Epoch:  387/3000  Loss: 3.851883052957469

Epoch:  388/3000  Loss: 3.8484718108999316

Epoch:  389/3000  Loss: 3.8452376798651686

Epoch:  390/3000  Loss: 3.845026122564557

Epoch:  391/3000  Loss: 3.84378574415185

Epoch:  392/3000  Loss: 3.843291636171012

Epoch:  393/3000  Loss: 3.836804767586719

Epoch:  394/3000  Loss: 3.839034704778386

Epoch:  395/3000  Loss: 3.8338858862032836

Epoch:  396/3000  Loss: 3.8379438186513966

Epoch:  397/3000  Loss: 3.8343907027408997

Epoch:  398/3000  Loss: 3.835340862712641

Epoch:  399/3000  Loss: 3.829510726051769

Epoch:  400/3000  Loss: 3.8269414452300676

Epoch:  401/3000  Loss: 3.825769441429226

Epoch:  402/3000  Loss: 3.8241010737145085

Epoch:  403/3000  Loss: 3.8273937263708007

Epoch:  404/3000  Loss: 3.8246856694934013

Epoch:  405/3000  Loss: 3.8224629489854833

Epoch:  406/3000  Loss: 3.825068080836329

Epoch:  407/3000  Loss: 3.821869331666793

Epoch:  408/3000  Loss: 3.8234796260965282

Epoch:  409/3000  Loss: 3.817997308161067

Epoch:  410/3000  Loss: 3.817463005548236

Epoch:  411/3000  Loss: 3.814067501309274

Epoch:  412/3000  Loss: 3.8155891588364526

Epoch:  413/3000  Loss: 3.810640048980713

Epoch:  414/3000  Loss: 3.809315846980303

Epoch:  415/3000  Loss: 3.8070613641848507

Epoch:  416/3000  Loss: 3.8080767341043757

Epoch:  417/3000  Loss: 3.809843535258852

Epoch:  418/3000  Loss: 3.803813867459352

Epoch:  419/3000  Loss: 3.8099505945183765

Epoch:  420/3000  Loss: 3.803713948151161

Epoch:  421/3000  Loss: 3.8030200421125038

Epoch:  422/3000  Loss: 3.7994099578638187

Epoch:  423/3000  Loss: 3.8011891162258453

Epoch:  424/3000  Loss: 3.801389535268148

Epoch:  425/3000  Loss: 3.7952630536309604

Epoch:  426/3000  Loss: 3.7927677790323893

Epoch:  427/3000  Loss: 3.7966521443991827

Epoch:  428/3000  Loss: 3.802922917508531

Epoch:  429/3000  Loss: 3.7985908853596655

Epoch:  430/3000  Loss: 3.7966255423666415

Epoch:  431/3000  Loss: 3.7911627484464097

Epoch:  432/3000  Loss: 3.796658958237747

Epoch:  433/3000  Loss: 3.787434257310012

Epoch:  434/3000  Loss: 3.7881646874307218

Epoch:  435/3000  Loss: 3.7872124984346587

Epoch:  436/3000  Loss: 3.7829222585963107

Epoch:  437/3000  Loss: 3.783541356558087

Epoch:  438/3000  Loss: 3.783681958845292

Epoch:  439/3000  Loss: 3.7772223543846746

Epoch:  440/3000  Loss: 3.7847777109036502

Epoch:  441/3000  Loss: 3.7837274195133954

Epoch:  442/3000  Loss: 3.778863664057063

Epoch:  443/3000  Loss: 3.7771759899183253

Epoch:  444/3000  Loss: 3.7758838916647024

Epoch:  445/3000  Loss: 3.774068284308773

Epoch:  446/3000  Loss: 3.7704638256423775

Epoch:  447/3000  Loss: 3.775265610903159

Epoch:  448/3000  Loss: 3.77455015237304

Epoch:  449/3000  Loss: 3.7672164549772766

Epoch:  450/3000  Loss: 3.7702115853627522

Epoch:  451/3000  Loss: 3.772366649803074

Epoch:  452/3000  Loss: 3.762764933465541

Epoch:  453/3000  Loss: 3.764457949824717

Epoch:  454/3000  Loss: 3.763167174109097

Epoch:  455/3000  Loss: 3.7634162891870258

Epoch:  456/3000  Loss: 3.759544149486498

Epoch:  457/3000  Loss: 3.7572746490610056

Epoch:  458/3000  Loss: 3.7578492904531546

Epoch:  459/3000  Loss: 3.755383282694323

Epoch:  460/3000  Loss: 3.755072970773982

Epoch:  461/3000  Loss: 3.75622565280432

Epoch:  462/3000  Loss: 3.7514087370072287

Epoch:  463/3000  Loss: 3.7493243195544714

Epoch:  464/3000  Loss: 3.757732282287773

Epoch:  465/3000  Loss: 3.7489968595833614

Epoch:  466/3000  Loss: 3.748561711146914

Epoch:  467/3000  Loss: 3.7460614505855516

Epoch:  468/3000  Loss: 3.751211534697434

Epoch:  469/3000  Loss: 3.746186920692181

Epoch:  470/3000  Loss: 3.750545581729933

Epoch:  471/3000  Loss: 3.736298874054832

Epoch:  472/3000  Loss: 3.7416687899622425

Epoch:  473/3000  Loss: 3.7358338975358283

Epoch:  474/3000  Loss: 3.737684214800254

Epoch:  475/3000  Loss: 3.7393463491023273

Epoch:  476/3000  Loss: 3.734567239366729

Epoch:  477/3000  Loss: 3.732530283653873

Epoch:  478/3000  Loss: 3.7391423806376842

Epoch:  479/3000  Loss: 3.72922260130959

Epoch:  480/3000  Loss: 3.7301250698922694

Epoch:  481/3000  Loss: 3.730417749799531

Epoch:  482/3000  Loss: 3.7330813013274096

Epoch:  483/3000  Loss: 3.7306150277455647

Epoch:  484/3000  Loss: 3.726022899013826

Epoch:  485/3000  Loss: 3.7258880280900275

Epoch:  486/3000  Loss: 3.7233064887167395

Epoch:  487/3000  Loss: 3.723129233546641

Epoch:  488/3000  Loss: 3.724281425585692

Epoch:  489/3000  Loss: 3.721673660716791

Epoch:  490/3000  Loss: 3.7153841670902295

Epoch:  491/3000  Loss: 3.718414969827937

Epoch:  492/3000  Loss: 3.7132447927847676

Epoch:  493/3000  Loss: 3.7152688042870885

Epoch:  494/3000  Loss: 3.7158479822093042

Epoch:  495/3000  Loss: 3.7115157417867377

Epoch:  496/3000  Loss: 3.715470590810666

Epoch:  497/3000  Loss: 3.714489177725781

Epoch:  498/3000  Loss: 3.713624288843966

Epoch:  499/3000  Loss: 3.713173786799113

Epoch:  500/3000  Loss: 3.707325001420646

Epoch:  501/3000  Loss: 3.7120502159513276

Epoch:  502/3000  Loss: 3.7047276535253415

Epoch:  503/3000  Loss: 3.710420770754759

Epoch:  504/3000  Loss: 3.7086710568132073

Epoch:  505/3000  Loss: 3.6985746586459807

Epoch:  506/3000  Loss: 3.7019338503651236

Epoch:  507/3000  Loss: 3.7069298557851504

Epoch:  508/3000  Loss: 3.6992319315329367

Epoch:  509/3000  Loss: 3.7079265446498475

Epoch:  510/3000  Loss: 3.700319035299893

Epoch:  511/3000  Loss: 3.700343107903141

Epoch:  512/3000  Loss: 3.691812980586085

Epoch:  513/3000  Loss: 3.6987752223836963

Epoch:  514/3000  Loss: 3.697594717179222

Epoch:  515/3000  Loss: 3.6978349285564205

Epoch:  516/3000  Loss: 3.6950862325471023

Epoch:  517/3000  Loss: 3.6950150550096885

Epoch:  518/3000  Loss: 3.691314957059663

Epoch:  519/3000  Loss: 3.6991448961455244

Epoch:  520/3000  Loss: 3.6905935336803566

Epoch:  521/3000  Loss: 3.6864743276574146

Epoch:  522/3000  Loss: 3.682875008966731

Epoch:  523/3000  Loss: 3.689525564237573

Epoch:  524/3000  Loss: 3.681729824789639

Epoch:  525/3000  Loss: 3.6826266370970626

Epoch:  526/3000  Loss: 3.6864084062905147

Epoch:  527/3000  Loss: 3.680832297226478

Epoch:  528/3000  Loss: 3.681823684977389

Epoch:  529/3000  Loss: 3.679777345986202

Epoch:  530/3000  Loss: 3.678786253655094

Epoch:  531/3000  Loss: 3.6751663958889313

Epoch:  532/3000  Loss: 3.6792327212191176

Epoch:  533/3000  Loss: 3.6763524833766894

Epoch:  534/3000  Loss: 3.67705064751636

Epoch:  535/3000  Loss: 3.679639941796489

Epoch:  536/3000  Loss: 3.6708611170450847

Epoch:  537/3000  Loss: 3.671854641793788

Epoch:  538/3000  Loss: 3.6706993420918783

Epoch:  539/3000  Loss: 3.6754821031943137

Epoch:  540/3000  Loss: 3.6681981558087227

Epoch:  541/3000  Loss: 3.6738368324849797

Epoch:  542/3000  Loss: 3.673192375555806

Epoch:  543/3000  Loss: 3.672239793580154

Epoch:  544/3000  Loss: 3.668152222140082

Epoch:  545/3000  Loss: 3.663296109780498

Epoch:  546/3000  Loss: 3.658123648851767

Epoch:  547/3000  Loss: 3.661705461041681

Epoch:  548/3000  Loss: 3.6626399763699236

Epoch:  549/3000  Loss: 3.6618152738987715

Epoch:  550/3000  Loss: 3.6645171088733894

Epoch:  551/3000  Loss: 3.663786697935784

Epoch:  552/3000  Loss: 3.660458405264493

Epoch:  553/3000  Loss: 3.6590299529590826

Epoch:  554/3000  Loss: 3.662043213570255

Epoch:  555/3000  Loss: 3.658680027380757

Epoch:  556/3000  Loss: 3.659763358105188

Epoch:  557/3000  Loss: 3.6552973018295463

Epoch:  558/3000  Loss: 3.6635854008554043

Epoch:  559/3000  Loss: 3.6533149554811675

Epoch:  560/3000  Loss: 3.6584168702706523

Epoch:  561/3000  Loss: 3.653659230265124

Epoch:  562/3000  Loss: 3.654826455828787

Epoch:  563/3000  Loss: 3.65239860436012

Epoch:  564/3000  Loss: 3.6527848265636926

Epoch:  565/3000  Loss: 3.652098137756874

Epoch:  566/3000  Loss: 3.645288538658756

Epoch:  567/3000  Loss: 3.648939213259467

Epoch:  568/3000  Loss: 3.6497495426528754

Epoch:  569/3000  Loss: 3.649572409706554

Epoch:  570/3000  Loss: 3.6460549924565457

Epoch:  571/3000  Loss: 3.6455925119334256

Epoch:  572/3000  Loss: 3.643196384934173

Epoch:  573/3000  Loss: 3.6433269621311934

Epoch:  574/3000  Loss: 3.644326003940626

Epoch:  575/3000  Loss: 3.641958957979049

Epoch:  576/3000  Loss: 3.645352477588873

Epoch:  577/3000  Loss: 3.645396911138776

Epoch:  578/3000  Loss: 3.645276566757553

Epoch:  579/3000  Loss: 3.640157420607819

Epoch:  580/3000  Loss: 3.6361152100837093

Epoch:  581/3000  Loss: 3.6390769673489975

Epoch:  582/3000  Loss: 3.6488452259151414

Epoch:  583/3000  Loss: 3.6375268393549427

Epoch:  584/3000  Loss: 3.635580441595494

Epoch:  585/3000  Loss: 3.639664686685321

Epoch:  586/3000  Loss: 3.6399446602525383

Epoch:  587/3000  Loss: 3.6318660390788113

Epoch:  588/3000  Loss: 3.6306359181458925

Epoch:  589/3000  Loss: 3.6381869469565906

Epoch:  590/3000  Loss: 3.6349846313739644

Epoch:  591/3000  Loss: 3.627906213409599

Epoch:  592/3000  Loss: 3.631770327447475

Epoch:  593/3000  Loss: 3.635910742310272

Epoch:  594/3000  Loss: 3.6325328426799555

Epoch:  595/3000  Loss: 3.631961830731096

Epoch:  596/3000  Loss: 3.6282889382592565

Epoch:  597/3000  Loss: 3.630817564602556

Epoch:  598/3000  Loss: 3.6344052007828638

Epoch:  599/3000  Loss: 3.626126044920121

Epoch:  600/3000  Loss: 3.6256570372088204

Epoch:  601/3000  Loss: 3.6271877086025546

Epoch:  602/3000  Loss: 3.6254383838039703

Epoch:  603/3000  Loss: 3.628650300804226

Epoch:  604/3000  Loss: 3.619848837797669

Epoch:  605/3000  Loss: 3.62467982741608

Epoch:  606/3000  Loss: 3.6201784605267404

Epoch:  607/3000  Loss: 3.6247454950179177

Epoch:  608/3000  Loss: 3.6215864499409993

Epoch:  609/3000  Loss: 3.6206093059189017

Epoch:  610/3000  Loss: 3.6192610263824463

Epoch:  611/3000  Loss: 3.622344862181565

Epoch:  612/3000  Loss: 3.616880069381889

Epoch:  613/3000  Loss: 3.614087361302869

Epoch:  614/3000  Loss: 3.615980909062528

Epoch:  615/3000  Loss: 3.6157209166165054

Epoch:  616/3000  Loss: 3.6085945940565787

Epoch:  617/3000  Loss: 3.6135560698892877

Epoch:  618/3000  Loss: 3.6153897784222133

Epoch:  619/3000  Loss: 3.6178010825453133

Epoch:  620/3000  Loss: 3.612488192525403

Epoch:  621/3000  Loss: 3.6012128024265686

Epoch:  622/3000  Loss: 3.607079902188531

Epoch:  623/3000  Loss: 3.6152153365913478

Epoch:  624/3000  Loss: 3.6118225223716647

Epoch:  625/3000  Loss: 3.607106241138502

Epoch:  626/3000  Loss: 3.607288135879341

Epoch:  627/3000  Loss: 3.604639832178752

Epoch:  628/3000  Loss: 3.6055543510393164

Epoch:  629/3000  Loss: 3.603491045962805

Epoch:  630/3000  Loss: 3.607764166250996

Epoch:  631/3000  Loss: 3.6058552511807145

Epoch:  632/3000  Loss: 3.602468983332316

Epoch:  633/3000  Loss: 3.606854259556737

Epoch:  634/3000  Loss: 3.6009995312526306

Epoch:  635/3000  Loss: 3.5973522833023948

Epoch:  636/3000  Loss: 3.601345963313662

Epoch:  637/3000  Loss: 3.5930932472492088

Epoch:  638/3000  Loss: 3.606083170024828

Epoch:  639/3000  Loss: 3.5941279427758577

Epoch:  640/3000  Loss: 3.600617582496555

Epoch:  641/3000  Loss: 3.5969090308266125

Epoch:  642/3000  Loss: 3.597998834478444

Epoch:  643/3000  Loss: 3.5952029200806015

Epoch:  644/3000  Loss: 3.594783562079243

Epoch:  645/3000  Loss: 3.598798072201082

Epoch:  646/3000  Loss: 3.5870311079354122

Epoch:  647/3000  Loss: 3.594366963156338

Epoch:  648/3000  Loss: 3.5898754284299654

Epoch:  649/3000  Loss: 3.5944060501010937

Epoch:  650/3000  Loss: 3.5894890703003983

Epoch:  651/3000  Loss: 3.591600008668571

Epoch:  652/3000  Loss: 3.5899855191680206

Epoch:  653/3000  Loss: 3.589319656635153

Epoch:  654/3000  Loss: 3.5873848531438015

Epoch:  655/3000  Loss: 3.5911763278917332

Epoch:  656/3000  Loss: 3.5889114834796425

Epoch:  657/3000  Loss: 3.5874240710817533

Epoch:  658/3000  Loss: 3.5864603766079606

Epoch:  659/3000  Loss: 3.57735818281941

Epoch:  660/3000  Loss: 3.5856752686116886

Epoch:  661/3000  Loss: 3.585698593073878

Epoch:  662/3000  Loss: 3.590576183932951

Epoch:  663/3000  Loss: 3.5808090741606966

Epoch:  664/3000  Loss: 3.588002203798842

Epoch:  665/3000  Loss: 3.578084971438879

Epoch:  666/3000  Loss: 3.570977485042879

Epoch:  667/3000  Loss: 3.577599138501047

Epoch:  668/3000  Loss: 3.5743888071213648

Epoch:  669/3000  Loss: 3.5813742451284125

Epoch:  670/3000  Loss: 3.572903611742217

Epoch:  671/3000  Loss: 3.5772754751402758

Epoch:  672/3000  Loss: 3.5724411246420322

Epoch:  673/3000  Loss: 3.5692883140739355

Epoch:  674/3000  Loss: 3.5686449692167086

Epoch:  675/3000  Loss: 3.570666990060916

Epoch:  676/3000  Loss: 3.5777154357954

Epoch:  677/3000  Loss: 3.5723068993667075

Epoch:  678/3000  Loss: 3.57550978441348

Epoch:  679/3000  Loss: 3.5722293903087747

Epoch:  680/3000  Loss: 3.574252306181809

Epoch:  681/3000  Loss: 3.569916747082239

Epoch:  682/3000  Loss: 3.5681460161318723

Epoch:  683/3000  Loss: 3.5733519745969224

Epoch:  684/3000  Loss: 3.5725860190117498

Epoch:  685/3000  Loss: 3.568135656707588

Epoch:  686/3000  Loss: 3.563002982632867

Epoch:  687/3000  Loss: 3.562760391454587

Epoch:  688/3000  Loss: 3.5618072685154005

Epoch:  689/3000  Loss: 3.5610872378294496

Epoch:  690/3000  Loss: 3.566773594932995

Epoch:  691/3000  Loss: 3.5644448872270256

Epoch:  692/3000  Loss: 3.5621472649190618

Epoch:  693/3000  Loss: 3.5626350994767813

Epoch:  694/3000  Loss: 3.559266217549642

Epoch:  695/3000  Loss: 3.5603267949202966

Epoch:  696/3000  Loss: 3.551771721346625

Epoch:  697/3000  Loss: 3.557837048892317

Epoch:  698/3000  Loss: 3.5520250484861178

Epoch:  699/3000  Loss: 3.554632757998061

Epoch:  700/3000  Loss: 3.553370567299854

Epoch:  701/3000  Loss: 3.5509807055023894

Epoch:  702/3000  Loss: 3.5564283584726266

Epoch:  703/3000  Loss: 3.551573349415571

Epoch:  704/3000  Loss: 3.556269984409727

Epoch:  705/3000  Loss: 3.550248479295051

Epoch:  706/3000  Loss: 3.552703338381888

Epoch:  707/3000  Loss: 3.5533347096936456

Epoch:  708/3000  Loss: 3.5533460852743564

Epoch:  709/3000  Loss: 3.55168611975922

Epoch:  710/3000  Loss: 3.5450451319245087

Epoch:  711/3000  Loss: 3.5433940284553613

Epoch:  712/3000  Loss: 3.5389974446132264

Epoch:  713/3000  Loss: 3.5442679909453996

Epoch:  714/3000  Loss: 3.5419158299763995

Epoch:  715/3000  Loss: 3.5410515516653827

Epoch:  716/3000  Loss: 3.545039913572114

Epoch:  717/3000  Loss: 3.542240681045357

Epoch:  718/3000  Loss: 3.5406955691589705

</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[53]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">workspace_utils</span> <span class="k">import</span> <span class="n">active_session</span>

<span class="k">with</span> <span class="n">active_session</span><span class="p">():</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    DON&#39;T MODIFY ANYTHING IN THIS CELL</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;sequence_length: </span><span class="si">{}</span><span class="se">\n</span><span class="s1">batch_size: </span><span class="si">{}</span><span class="se">\n</span><span class="s1">learning_rate: </span><span class="si">{}</span><span class="se">\n</span><span class="s1">embedding_dim: </span><span class="si">{}</span><span class="se">\n</span><span class="s1">n_layers: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">sequence_length</span><span class="p">,</span><span class="n">batch_size</span><span class="p">,</span><span class="n">learning_rate</span><span class="p">,</span><span class="n">embedding_dim</span><span class="p">,</span><span class="n">n_layers</span><span class="p">))</span>

    <span class="c1"># create model and move to gpu if available</span>
    <span class="n">rnn</span> <span class="o">=</span> <span class="n">RNN</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">,</span> <span class="n">n_layers</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">train_on_gpu</span><span class="p">:</span>
        <span class="n">rnn</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>

    <span class="c1"># defining loss and optimization functions for training</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">rnn</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>
    <span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>

    <span class="c1"># training the model</span>
    <span class="n">trained_rnn</span> <span class="o">=</span> <span class="n">train_rnn</span><span class="p">(</span><span class="n">rnn</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">num_epochs</span><span class="p">,</span> <span class="n">show_every_n_batches</span><span class="p">)</span>

    <span class="c1"># saving the trained model</span>
    <span class="n">helper</span><span class="o">.</span><span class="n">save_model</span><span class="p">(</span><span class="s1">&#39;/trained_rnn&#39;</span><span class="p">,</span> <span class="n">trained_rnn</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Model Trained and Saved&#39;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;sequence_length: </span><span class="si">{}</span><span class="se">\n</span><span class="s1">batch_size: </span><span class="si">{}</span><span class="se">\n</span><span class="s1">learning_rate: </span><span class="si">{}</span><span class="se">\n</span><span class="s1">embedding_dim: </span><span class="si">{}</span><span class="se">\n</span><span class="s1">n_layers: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">sequence_length</span><span class="p">,</span><span class="n">batch_size</span><span class="p">,</span><span class="n">learning_rate</span><span class="p">,</span><span class="n">embedding_dim</span><span class="p">,</span><span class="n">n_layers</span><span class="p">))</span>
    
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>sequence_length: 8
batch_size: 256
learning_rate: 0.0001
embedding_dim: 1000
n_layers: 2
Training for 3000 epoch(s)...
Epoch:    1/3000  Loss: 9.824233853022257

Epoch:    2/3000  Loss: 8.382291491278286

Epoch:    3/3000  Loss: 7.169598748218054

Epoch:    4/3000  Loss: 6.598437259937155

Epoch:    5/3000  Loss: 6.2991787132175485

Epoch:    6/3000  Loss: 6.134664025799982

Epoch:    7/3000  Loss: 6.029405585102651

Epoch:    8/3000  Loss: 5.935897772339569

Epoch:    9/3000  Loss: 5.8358773878251

Epoch:   10/3000  Loss: 5.7491682414350835

Epoch:   11/3000  Loss: 5.66413636043154

Epoch:   12/3000  Loss: 5.602039262618142

Epoch:   13/3000  Loss: 5.545062911373446

Epoch:   14/3000  Loss: 5.499353295907207

Epoch:   15/3000  Loss: 5.456617101033529

Epoch:   16/3000  Loss: 5.422627278031974

Epoch:   17/3000  Loss: 5.390671954757866

Epoch:   18/3000  Loss: 5.358663776003081

Epoch:   19/3000  Loss: 5.329554732092496

Epoch:   20/3000  Loss: 5.303960079982363

Epoch:   21/3000  Loss: 5.279344080782485

Epoch:   22/3000  Loss: 5.255288834407412

Epoch:   23/3000  Loss: 5.23317930835417

Epoch:   24/3000  Loss: 5.2105424826172575

Epoch:   25/3000  Loss: 5.190338898801255

Epoch:   26/3000  Loss: 5.171061496076913

Epoch:   27/3000  Loss: 5.147252149691527

Epoch:   28/3000  Loss: 5.126123875585096

Epoch:   29/3000  Loss: 5.110413815509314

Epoch:   30/3000  Loss: 5.089764984174707

Epoch:   31/3000  Loss: 5.073375059544355

Epoch:   32/3000  Loss: 5.049725603783267

Epoch:   33/3000  Loss: 5.032071022603704

Epoch:   34/3000  Loss: 5.0127120620902925

Epoch:   35/3000  Loss: 4.9927416362981685

Epoch:   36/3000  Loss: 4.975349328709745

Epoch:   37/3000  Loss: 4.955464638238666

Epoch:   38/3000  Loss: 4.934996198237627

Epoch:   39/3000  Loss: 4.919945787013262

Epoch:   40/3000  Loss: 4.898217059826028

Epoch:   41/3000  Loss: 4.877491589250235

Epoch:   42/3000  Loss: 4.859434516950586

Epoch:   43/3000  Loss: 4.841130176631884

Epoch:   44/3000  Loss: 4.824143375747505

Epoch:   45/3000  Loss: 4.805470948931815

Epoch:   46/3000  Loss: 4.783914768832854

Epoch:   47/3000  Loss: 4.765816087832396

Epoch:   48/3000  Loss: 4.749139920596419

Epoch:   49/3000  Loss: 4.7279963515270715

Epoch:   50/3000  Loss: 4.713522368463977

Epoch:   51/3000  Loss: 4.699839567864078

Epoch:   52/3000  Loss: 4.677722706191841

Epoch:   53/3000  Loss: 4.660026311326301

Epoch:   54/3000  Loss: 4.642262417694618

Epoch:   55/3000  Loss: 4.624195012278941

Epoch:   56/3000  Loss: 4.610279668062582

Epoch:   57/3000  Loss: 4.595152967825703

Epoch:   58/3000  Loss: 4.575253580904555

Epoch:   59/3000  Loss: 4.561557322535021

Epoch:   60/3000  Loss: 4.545637715547934

Epoch:   61/3000  Loss: 4.528676235812834

Epoch:   62/3000  Loss: 4.5138526006676685

Epoch:   63/3000  Loss: 4.497794893418235

Epoch:   64/3000  Loss: 4.484001299978673

Epoch:   65/3000  Loss: 4.464701232690921

Epoch:   66/3000  Loss: 4.451523249176727

Epoch:   67/3000  Loss: 4.436962504222476

Epoch:   68/3000  Loss: 4.42133893473395

Epoch:   69/3000  Loss: 4.410439404125872

Epoch:   70/3000  Loss: 4.396178211014846

Epoch:   71/3000  Loss: 4.383606282047841

Epoch:   72/3000  Loss: 4.367819929122925

Epoch:   73/3000  Loss: 4.356922206659426

Epoch:   74/3000  Loss: 4.342481080416976

Epoch:   75/3000  Loss: 4.337481275098077

Epoch:   76/3000  Loss: 4.317877630255688

Epoch:   77/3000  Loss: 4.31154321582838

Epoch:   78/3000  Loss: 4.299207425939626

Epoch:   79/3000  Loss: 4.288710328902321

Epoch:   80/3000  Loss: 4.276659535265517

Epoch:   81/3000  Loss: 4.264956769175913

Epoch:   82/3000  Loss: 4.2533290884960655

Epoch:   83/3000  Loss: 4.241228889596873

Epoch:   84/3000  Loss: 4.232313825344217

Epoch:   85/3000  Loss: 4.222919829686483

Epoch:   86/3000  Loss: 4.216488285722404

Epoch:   87/3000  Loss: 4.2041268754279475

Epoch:   88/3000  Loss: 4.198304186744251

Epoch:   89/3000  Loss: 4.185808245340983

Epoch:   90/3000  Loss: 4.182512319499049

Epoch:   91/3000  Loss: 4.167171090224693

Epoch:   92/3000  Loss: 4.1567143423803925

Epoch:   93/3000  Loss: 4.151687375430403

Epoch:   94/3000  Loss: 4.145505473257481

Epoch:   95/3000  Loss: 4.137490355283364

Epoch:   96/3000  Loss: 4.1317004910830795

Epoch:   97/3000  Loss: 4.120203245097193

Epoch:   98/3000  Loss: 4.115640376080042

Epoch:   99/3000  Loss: 4.10608017033544

Epoch:  100/3000  Loss: 4.100433201625429

Epoch:  101/3000  Loss: 4.0910290756444825

Epoch:  102/3000  Loss: 4.084768276104982

Epoch:  103/3000  Loss: 4.078213390262648

Epoch:  104/3000  Loss: 4.0716797746461015

Epoch:  105/3000  Loss: 4.062784459673125

Epoch:  106/3000  Loss: 4.053560114454949

Epoch:  107/3000  Loss: 4.053125764035631

Epoch:  108/3000  Loss: 4.041974927091051

Epoch:  109/3000  Loss: 4.032308121385245

Epoch:  110/3000  Loss: 4.031590560935009

Epoch:  111/3000  Loss: 4.0231194583848975

Epoch:  112/3000  Loss: 4.021356887927

Epoch:  113/3000  Loss: 4.00704497304456

Epoch:  114/3000  Loss: 3.999421810281688

Epoch:  115/3000  Loss: 3.9988836480283187

Epoch:  116/3000  Loss: 3.9932423005158872

Epoch:  117/3000  Loss: 3.984507175971722

Epoch:  118/3000  Loss: 3.9812601867763475

Epoch:  119/3000  Loss: 3.9728704743001653

Epoch:  120/3000  Loss: 3.970934332376239

Epoch:  121/3000  Loss: 3.961923411248744

Epoch:  122/3000  Loss: 3.953491527184673

Epoch:  123/3000  Loss: 3.9521931039875953

Epoch:  124/3000  Loss: 3.948588547213324

Epoch:  125/3000  Loss: 3.9385013530994284

Epoch:  126/3000  Loss: 3.933558672872083

Epoch:  127/3000  Loss: 3.9239253592217107

Epoch:  128/3000  Loss: 3.922240451286579

Epoch:  129/3000  Loss: 3.9173320381120704

Epoch:  130/3000  Loss: 3.9102720742938164

Epoch:  131/3000  Loss: 3.9056355196854162

Epoch:  132/3000  Loss: 3.8962005094550123

Epoch:  133/3000  Loss: 3.891245730717977

Epoch:  134/3000  Loss: 3.888302278518677

Epoch:  135/3000  Loss: 3.8854566513806925

Epoch:  136/3000  Loss: 3.8757461750644375

Epoch:  137/3000  Loss: 3.8786091683924884

Epoch:  138/3000  Loss: 3.8706457138061525

Epoch:  139/3000  Loss: 3.8663455798708157

Epoch:  140/3000  Loss: 3.8612629068308864

Epoch:  141/3000  Loss: 3.859245417583948

Epoch:  142/3000  Loss: 3.850923264163664

Epoch:  143/3000  Loss: 3.845185297933118

Epoch:  144/3000  Loss: 3.8421477284924737

Epoch:  145/3000  Loss: 3.835178125315699

Epoch:  146/3000  Loss: 3.8312090731215203

Epoch:  147/3000  Loss: 3.8233864345769772

Epoch:  148/3000  Loss: 3.8202321584197296

Epoch:  149/3000  Loss: 3.8219567172828763

Epoch:  150/3000  Loss: 3.814921790155871

Epoch:  151/3000  Loss: 3.8104098829729804

Epoch:  152/3000  Loss: 3.8028129056952467

Epoch:  153/3000  Loss: 3.7986089119966002

Epoch:  154/3000  Loss: 3.803629480011162

Epoch:  155/3000  Loss: 3.7947176944250347

Epoch:  156/3000  Loss: 3.7889043495572845

Epoch:  157/3000  Loss: 3.7852980438320114

Epoch:  158/3000  Loss: 3.7818349734119985

Epoch:  159/3000  Loss: 3.7768784133867284

Epoch:  160/3000  Loss: 3.77609100286988

Epoch:  161/3000  Loss: 3.76979090098677

Epoch:  162/3000  Loss: 3.7689711839303204

Epoch:  163/3000  Loss: 3.7550028066525516

Epoch:  164/3000  Loss: 3.7611290104087742

Epoch:  165/3000  Loss: 3.7575134655524947

Epoch:  166/3000  Loss: 3.7546958451983574

Epoch:  167/3000  Loss: 3.751482230219348

Epoch:  168/3000  Loss: 3.7427980170852835

Epoch:  169/3000  Loss: 3.738852733305131

Epoch:  170/3000  Loss: 3.7392163638410896

Epoch:  171/3000  Loss: 3.7365591230063604

Epoch:  172/3000  Loss: 3.7327596510963876

Epoch:  173/3000  Loss: 3.7261042534619913

Epoch:  174/3000  Loss: 3.727051322213535

Epoch:  175/3000  Loss: 3.720892999364042

Epoch:  176/3000  Loss: 3.7172666028998362

Epoch:  177/3000  Loss: 3.715260185592476

Epoch:  178/3000  Loss: 3.7065749787736215

Epoch:  179/3000  Loss: 3.709383592934444

Epoch:  180/3000  Loss: 3.70168669196381

Epoch:  181/3000  Loss: 3.70042615594535

Epoch:  182/3000  Loss: 3.7021562526965965

Epoch:  183/3000  Loss: 3.6913705946385176

Epoch:  184/3000  Loss: 3.6888652374004494

Epoch:  185/3000  Loss: 3.6848310744625397

Epoch:  186/3000  Loss: 3.6829464654812867

Epoch:  187/3000  Loss: 3.677515776404019

Epoch:  188/3000  Loss: 3.6730805528574977

Epoch:  189/3000  Loss: 3.6716800766429682

Epoch:  190/3000  Loss: 3.6643114703825153

Epoch:  191/3000  Loss: 3.6670298242020882

Epoch:  192/3000  Loss: 3.661429737354147

Epoch:  193/3000  Loss: 3.6599779627789024

Epoch:  194/3000  Loss: 3.6516244099057955

Epoch:  195/3000  Loss: 3.6541932933631984

Epoch:  196/3000  Loss: 3.6494464918114673

Epoch:  197/3000  Loss: 3.644149468410974

Epoch:  198/3000  Loss: 3.6410404868509576

Epoch:  199/3000  Loss: 3.6367009979554976

Epoch:  200/3000  Loss: 3.62786553700765

Epoch:  201/3000  Loss: 3.628086361391791

Epoch:  202/3000  Loss: 3.6229642736500707

Epoch:  203/3000  Loss: 3.6174962378096307

Epoch:  204/3000  Loss: 3.6197487792749516

Epoch:  205/3000  Loss: 3.607819215730689

Epoch:  206/3000  Loss: 3.6092986556305284

Epoch:  207/3000  Loss: 3.607667919137012

Epoch:  208/3000  Loss: 3.5965434721146505

Epoch:  209/3000  Loss: 3.5986521183759317

Epoch:  210/3000  Loss: 3.5974436716101637

Epoch:  211/3000  Loss: 3.5918606248395197

Epoch:  212/3000  Loss: 3.5866131355022564

Epoch:  213/3000  Loss: 3.5792922348811707

Epoch:  214/3000  Loss: 3.5809236323696445

Epoch:  215/3000  Loss: 3.5740890371388403

Epoch:  216/3000  Loss: 3.5734239846810527

Epoch:  217/3000  Loss: 3.565763075598355

Epoch:  218/3000  Loss: 3.569355405610183

Epoch:  219/3000  Loss: 3.5631278333992795

Epoch:  220/3000  Loss: 3.5592820222350374

Epoch:  221/3000  Loss: 3.564233606163113

Epoch:  222/3000  Loss: 3.5555409678097427

Epoch:  223/3000  Loss: 3.548533964705193

Epoch:  224/3000  Loss: 3.5458492169434996

Epoch:  225/3000  Loss: 3.542777349208963

Epoch:  226/3000  Loss: 3.5354690507910718

Epoch:  227/3000  Loss: 3.5393433280374813

Epoch:  228/3000  Loss: 3.539394439499954

Epoch:  229/3000  Loss: 3.5366213436784415

Epoch:  230/3000  Loss: 3.5340148142014427

Epoch:  231/3000  Loss: 3.5326457336031156

Epoch:  232/3000  Loss: 3.5298742206617333

Epoch:  233/3000  Loss: 3.5220462853881136

Epoch:  234/3000  Loss: 3.5174072249182338

Epoch:  235/3000  Loss: 3.520919435873799

Epoch:  236/3000  Loss: 3.5164800320548575

Epoch:  237/3000  Loss: 3.512796056133577

Epoch:  238/3000  Loss: 3.5077958271421235

Epoch:  239/3000  Loss: 3.5079672139266442

Epoch:  240/3000  Loss: 3.505186803861596

Epoch:  241/3000  Loss: 3.505046880656275

Epoch:  242/3000  Loss: 3.50277399468696

Epoch:  243/3000  Loss: 3.499201953822169

Epoch:  244/3000  Loss: 3.501365332767881

Epoch:  245/3000  Loss: 3.49850495820758

Epoch:  246/3000  Loss: 3.4968665090100517

Epoch:  247/3000  Loss: 3.489643956874979

Epoch:  248/3000  Loss: 3.483261621409449

Epoch:  249/3000  Loss: 3.4863665010737277

Epoch:  250/3000  Loss: 3.486492033662467

Epoch:  251/3000  Loss: 3.479585168005406

Epoch:  252/3000  Loss: 3.479619781998382

Epoch:  253/3000  Loss: 3.4753736364430394

Epoch:  254/3000  Loss: 3.4769387206812015

Epoch:  255/3000  Loss: 3.4720973403974513

Epoch:  256/3000  Loss: 3.474854976829441

Epoch:  257/3000  Loss: 3.4661046559783233

Epoch:  258/3000  Loss: 3.465543226812078

Epoch:  259/3000  Loss: 3.464322414617429

Epoch:  260/3000  Loss: 3.459962997217288

Epoch:  261/3000  Loss: 3.4592197374365794

Epoch:  262/3000  Loss: 3.463789031149327

Epoch:  263/3000  Loss: 3.4568349662868454

Epoch:  264/3000  Loss: 3.452923158119465

Epoch:  265/3000  Loss: 3.4537123049812757

Epoch:  266/3000  Loss: 3.45091459614107

Epoch:  267/3000  Loss: 3.449799180852956

Epoch:  268/3000  Loss: 3.4475834281965234

Epoch:  269/3000  Loss: 3.443845050636379

Epoch:  270/3000  Loss: 3.443957544195241

Epoch:  271/3000  Loss: 3.4386151154836018

Epoch:  272/3000  Loss: 3.4380315780639648

Epoch:  273/3000  Loss: 3.4383006967347245

Epoch:  274/3000  Loss: 3.4397784424924303

Epoch:  275/3000  Loss: 3.4336059016742926

Epoch:  276/3000  Loss: 3.4313482838115474

Epoch:  277/3000  Loss: 3.4256581295495745

Epoch:  278/3000  Loss: 3.4255280554979697

Epoch:  279/3000  Loss: 3.421955297185087

Epoch:  280/3000  Loss: 3.424977987662129

Epoch:  281/3000  Loss: 3.4239956170663066

Epoch:  282/3000  Loss: 3.4273472561233347

Epoch:  283/3000  Loss: 3.419755327838591

Epoch:  284/3000  Loss: 3.4152537148574305

Epoch:  285/3000  Loss: 3.408555049457769

Epoch:  286/3000  Loss: 3.410268190537376

Epoch:  287/3000  Loss: 3.4177342524473695

Epoch:  288/3000  Loss: 3.4062418449884175

Epoch:  289/3000  Loss: 3.4069156580957873

Epoch:  290/3000  Loss: 3.4025501651325447

Epoch:  291/3000  Loss: 3.397393340625982

Epoch:  292/3000  Loss: 3.3956033503872223

Epoch:  293/3000  Loss: 3.3995610368662867

Epoch:  294/3000  Loss: 3.4046510148322446

Epoch:  295/3000  Loss: 3.3983912709115565

Epoch:  296/3000  Loss: 3.393510380010495

Epoch:  297/3000  Loss: 3.3950492376568673

Epoch:  298/3000  Loss: 3.387186082204183

Epoch:  299/3000  Loss: 3.388777577191934

Epoch:  300/3000  Loss: 3.392338254533965

Epoch:  301/3000  Loss: 3.382560736557533

Epoch:  302/3000  Loss: 3.3847280683188603

Epoch:  303/3000  Loss: 3.383570691908913

Epoch:  304/3000  Loss: 3.382646226334846

Epoch:  305/3000  Loss: 3.3783869047274533

Epoch:  306/3000  Loss: 3.382658212486355

Epoch:  307/3000  Loss: 3.380849570241468

</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_text output_error">
<pre>
<span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">KeyboardInterrupt</span>                         Traceback (most recent call last)
<span class="ansi-green-fg">&lt;ipython-input-53-08f0cb61c5e7&gt;</span> in <span class="ansi-cyan-fg">&lt;module&gt;</span><span class="ansi-blue-fg">()</span>
<span class="ansi-green-intense-fg ansi-bold">     18</span> 
<span class="ansi-green-intense-fg ansi-bold">     19</span>     <span class="ansi-red-fg"># training the model</span>
<span class="ansi-green-fg">---&gt; 20</span><span class="ansi-red-fg">     </span>trained_rnn <span class="ansi-blue-fg">=</span> train_rnn<span class="ansi-blue-fg">(</span>rnn<span class="ansi-blue-fg">,</span> batch_size<span class="ansi-blue-fg">,</span> optimizer<span class="ansi-blue-fg">,</span> criterion<span class="ansi-blue-fg">,</span> num_epochs<span class="ansi-blue-fg">,</span> show_every_n_batches<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">     21</span> 
<span class="ansi-green-intense-fg ansi-bold">     22</span>     <span class="ansi-red-fg"># saving the trained model</span>

<span class="ansi-green-fg">&lt;ipython-input-16-54bd9dc2f859&gt;</span> in <span class="ansi-cyan-fg">train_rnn</span><span class="ansi-blue-fg">(rnn, batch_size, optimizer, criterion, n_epochs, show_every_n_batches)</span>
<span class="ansi-green-intense-fg ansi-bold">     22</span> 
<span class="ansi-green-intense-fg ansi-bold">     23</span>             <span class="ansi-red-fg"># forward, back prop</span>
<span class="ansi-green-fg">---&gt; 24</span><span class="ansi-red-fg">             </span>loss<span class="ansi-blue-fg">,</span> hidden <span class="ansi-blue-fg">=</span> forward_back_prop<span class="ansi-blue-fg">(</span>rnn<span class="ansi-blue-fg">,</span> optimizer<span class="ansi-blue-fg">,</span> criterion<span class="ansi-blue-fg">,</span> inputs<span class="ansi-blue-fg">,</span> labels<span class="ansi-blue-fg">,</span> hidden<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">     25</span>             <span class="ansi-red-fg"># record loss</span>
<span class="ansi-green-intense-fg ansi-bold">     26</span>             batch_losses<span class="ansi-blue-fg">.</span>append<span class="ansi-blue-fg">(</span>loss<span class="ansi-blue-fg">)</span>

<span class="ansi-green-fg">&lt;ipython-input-15-c302a43168d3&gt;</span> in <span class="ansi-cyan-fg">forward_back_prop</span><span class="ansi-blue-fg">(rnn, optimizer, criterion, inp, target, hidden)</span>
<span class="ansi-green-intense-fg ansi-bold">     26</span>     <span class="ansi-red-fg"># calculate the loss and perform backprop</span>
<span class="ansi-green-intense-fg ansi-bold">     27</span>     loss <span class="ansi-blue-fg">=</span> criterion<span class="ansi-blue-fg">(</span>output<span class="ansi-blue-fg">.</span>squeeze<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">,</span> target<span class="ansi-blue-fg">.</span>long<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">)</span>
<span class="ansi-green-fg">---&gt; 28</span><span class="ansi-red-fg">     </span>loss<span class="ansi-blue-fg">.</span>backward<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">     29</span>     loss<span class="ansi-blue-fg">=</span> loss<span class="ansi-blue-fg">.</span>item<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">     30</span>     optimizer<span class="ansi-blue-fg">.</span>step<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span>

<span class="ansi-green-fg">/opt/conda/lib/python3.6/site-packages/torch/tensor.py</span> in <span class="ansi-cyan-fg">backward</span><span class="ansi-blue-fg">(self, gradient, retain_graph, create_graph)</span>
<span class="ansi-green-intense-fg ansi-bold">     91</span>                 products<span class="ansi-blue-fg">.</span> Defaults to<span class="ansi-red-fg"> </span><span class="ansi-red-fg">`</span><span class="ansi-red-fg">`</span><span class="ansi-green-fg">False</span><span class="ansi-red-fg">`</span><span class="ansi-red-fg">`</span><span class="ansi-blue-fg">.</span>
<span class="ansi-green-intense-fg ansi-bold">     92</span>         &#34;&#34;&#34;
<span class="ansi-green-fg">---&gt; 93</span><span class="ansi-red-fg">         </span>torch<span class="ansi-blue-fg">.</span>autograd<span class="ansi-blue-fg">.</span>backward<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">,</span> gradient<span class="ansi-blue-fg">,</span> retain_graph<span class="ansi-blue-fg">,</span> create_graph<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">     94</span> 
<span class="ansi-green-intense-fg ansi-bold">     95</span>     <span class="ansi-green-fg">def</span> register_hook<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">,</span> hook<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">:</span>

<span class="ansi-green-fg">/opt/conda/lib/python3.6/site-packages/torch/autograd/__init__.py</span> in <span class="ansi-cyan-fg">backward</span><span class="ansi-blue-fg">(tensors, grad_tensors, retain_graph, create_graph, grad_variables)</span>
<span class="ansi-green-intense-fg ansi-bold">     87</span>     Variable._execution_engine.run_backward(
<span class="ansi-green-intense-fg ansi-bold">     88</span>         tensors<span class="ansi-blue-fg">,</span> grad_tensors<span class="ansi-blue-fg">,</span> retain_graph<span class="ansi-blue-fg">,</span> create_graph<span class="ansi-blue-fg">,</span>
<span class="ansi-green-fg">---&gt; 89</span><span class="ansi-red-fg">         allow_unreachable=True)  # allow_unreachable flag
</span><span class="ansi-green-intense-fg ansi-bold">     90</span> 
<span class="ansi-green-intense-fg ansi-bold">     91</span> 

<span class="ansi-red-fg">KeyboardInterrupt</span>: </pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[50]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">workspace_utils</span> <span class="k">import</span> <span class="n">active_session</span>

<span class="k">with</span> <span class="n">active_session</span><span class="p">():</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    DON&#39;T MODIFY ANYTHING IN THIS CELL</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;sequence_length: </span><span class="si">{}</span><span class="se">\n</span><span class="s1">batch_size: </span><span class="si">{}</span><span class="se">\n</span><span class="s1">learning_rate: </span><span class="si">{}</span><span class="se">\n</span><span class="s1">embedding_dim: </span><span class="si">{}</span><span class="se">\n</span><span class="s1">n_layers: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">sequence_length</span><span class="p">,</span><span class="n">batch_size</span><span class="p">,</span><span class="n">learning_rate</span><span class="p">,</span><span class="n">embedding_dim</span><span class="p">,</span><span class="n">n_layers</span><span class="p">))</span>

    <span class="c1"># create model and move to gpu if available</span>
    <span class="n">rnn</span> <span class="o">=</span> <span class="n">RNN</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">,</span> <span class="n">n_layers</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">train_on_gpu</span><span class="p">:</span>
        <span class="n">rnn</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>

    <span class="c1"># defining loss and optimization functions for training</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">rnn</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>
    <span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>

    <span class="c1"># training the model</span>
    <span class="n">trained_rnn</span> <span class="o">=</span> <span class="n">train_rnn</span><span class="p">(</span><span class="n">rnn</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">num_epochs</span><span class="p">,</span> <span class="n">show_every_n_batches</span><span class="p">)</span>

    <span class="c1"># saving the trained model</span>
    <span class="n">helper</span><span class="o">.</span><span class="n">save_model</span><span class="p">(</span><span class="s1">&#39;/trained_rnn&#39;</span><span class="p">,</span> <span class="n">trained_rnn</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Model Trained and Saved&#39;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;sequence_length: </span><span class="si">{}</span><span class="se">\n</span><span class="s1">batch_size: </span><span class="si">{}</span><span class="se">\n</span><span class="s1">learning_rate: </span><span class="si">{}</span><span class="se">\n</span><span class="s1">embedding_dim: </span><span class="si">{}</span><span class="se">\n</span><span class="s1">n_layers: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">sequence_length</span><span class="p">,</span><span class="n">batch_size</span><span class="p">,</span><span class="n">learning_rate</span><span class="p">,</span><span class="n">embedding_dim</span><span class="p">,</span><span class="n">n_layers</span><span class="p">))</span>
    
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>sequence_length: 8
batch_size: 256
learning_rate: 0.0001
embedding_dim: 300
n_layers: 2
Training for 3000 epoch(s)...
Epoch:    1/3000  Loss: 9.81199777285258

Epoch:    2/3000  Loss: 8.006715022558453

Epoch:    3/3000  Loss: 6.743958465532325

Epoch:    4/3000  Loss: 6.281857250476706

Epoch:    5/3000  Loss: 6.083624680836995

Epoch:    6/3000  Loss: 5.976886859981493

Epoch:    7/3000  Loss: 5.867835347405795

Epoch:    8/3000  Loss: 5.745765932675066

Epoch:    9/3000  Loss: 5.6365704141814135

Epoch:   10/3000  Loss: 5.5505092851046856

Epoch:   11/3000  Loss: 5.481542950114985

Epoch:   12/3000  Loss: 5.417606043541569

Epoch:   13/3000  Loss: 5.3647027487042305

Epoch:   14/3000  Loss: 5.316270245080707

Epoch:   15/3000  Loss: 5.270434730354396

Epoch:   16/3000  Loss: 5.228882394987961

Epoch:   17/3000  Loss: 5.18576642617412

Epoch:   18/3000  Loss: 5.147305793323736

Epoch:   19/3000  Loss: 5.112871199640734

Epoch:   20/3000  Loss: 5.076744826086636

Epoch:   21/3000  Loss: 5.042915544838741

Epoch:   22/3000  Loss: 5.009080925207028

Epoch:   23/3000  Loss: 4.977457190108026

Epoch:   24/3000  Loss: 4.944533458797411

Epoch:   25/3000  Loss: 4.9126507002731845

Epoch:   26/3000  Loss: 4.881080767752111

Epoch:   27/3000  Loss: 4.853128776331057

Epoch:   28/3000  Loss: 4.819754909646922

Epoch:   29/3000  Loss: 4.7885883594381395

Epoch:   30/3000  Loss: 4.763706699459032

Epoch:   31/3000  Loss: 4.737323691927154

Epoch:   32/3000  Loss: 4.708748669459902

Epoch:   33/3000  Loss: 4.684437165315124

Epoch:   34/3000  Loss: 4.659465442854783

Epoch:   35/3000  Loss: 4.634612228678561

Epoch:   36/3000  Loss: 4.6073522321109115

Epoch:   37/3000  Loss: 4.584594007470142

Epoch:   38/3000  Loss: 4.560305934116758

Epoch:   39/3000  Loss: 4.541756372890253

Epoch:   40/3000  Loss: 4.521320209283938

Epoch:   41/3000  Loss: 4.500214707714388

Epoch:   42/3000  Loss: 4.477603608164294

Epoch:   43/3000  Loss: 4.459256022551964

Epoch:   44/3000  Loss: 4.4405770115468695

Epoch:   45/3000  Loss: 4.418391146056954

Epoch:   46/3000  Loss: 4.401893079143831

Epoch:   47/3000  Loss: 4.383596478933575

Epoch:   48/3000  Loss: 4.364748677440073

Epoch:   49/3000  Loss: 4.348658510734295

Epoch:   50/3000  Loss: 4.32877421927178

Epoch:   51/3000  Loss: 4.311791153063719

Epoch:   52/3000  Loss: 4.296381265267558

Epoch:   53/3000  Loss: 4.278744732648477

Epoch:   54/3000  Loss: 4.26504948221404

Epoch:   55/3000  Loss: 4.248923072595706

Epoch:   56/3000  Loss: 4.230802807862731

Epoch:   57/3000  Loss: 4.216991106669108

Epoch:   58/3000  Loss: 4.207896472667826

Epoch:   59/3000  Loss: 4.194126528707044

Epoch:   60/3000  Loss: 4.173399344257925

Epoch:   61/3000  Loss: 4.159902987534973

Epoch:   62/3000  Loss: 4.145932279236015

Epoch:   63/3000  Loss: 4.132073481877645

Epoch:   64/3000  Loss: 4.120736591295264

Epoch:   65/3000  Loss: 4.1088862145084075

Epoch:   66/3000  Loss: 4.096968520920852

Epoch:   67/3000  Loss: 4.08288524123444

Epoch:   68/3000  Loss: 4.067981346174218

Epoch:   69/3000  Loss: 4.057581909223535

Epoch:   70/3000  Loss: 4.042052545218632

Epoch:   71/3000  Loss: 4.025770171483358

Epoch:   72/3000  Loss: 4.015252837367441

Epoch:   73/3000  Loss: 3.9989195807226774

Epoch:   74/3000  Loss: 3.98734163854314

Epoch:   75/3000  Loss: 3.977972962938506

Epoch:   76/3000  Loss: 3.9646171695884616

Epoch:   77/3000  Loss: 3.954739892893824

Epoch:   78/3000  Loss: 3.9400519760175685

Epoch:   79/3000  Loss: 3.926734247426877

Epoch:   80/3000  Loss: 3.9134478141521587

Epoch:   81/3000  Loss: 3.90022360209761

Epoch:   82/3000  Loss: 3.8900535353298844

Epoch:   83/3000  Loss: 3.877554775106496

Epoch:   84/3000  Loss: 3.865672132886689

Epoch:   85/3000  Loss: 3.856303033061411

Epoch:   86/3000  Loss: 3.8411529277933054

Epoch:   87/3000  Loss: 3.828480728193261

Epoch:   88/3000  Loss: 3.8187502060813467

Epoch:   89/3000  Loss: 3.8075898411630216

Epoch:   90/3000  Loss: 3.7943440683956804

Epoch:   91/3000  Loss: 3.7872724269998486

Epoch:   92/3000  Loss: 3.7737056096394856

Epoch:   93/3000  Loss: 3.7645021641391447

Epoch:   94/3000  Loss: 3.7488615732083375

Epoch:   95/3000  Loss: 3.7398034194420124

Epoch:   96/3000  Loss: 3.7271928047311715

Epoch:   97/3000  Loss: 3.713800697765131

Epoch:   98/3000  Loss: 3.7019654679572445

Epoch:   99/3000  Loss: 3.689121475438962

Epoch:  100/3000  Loss: 3.6809843430573914

Epoch:  101/3000  Loss: 3.66960335216303

Epoch:  102/3000  Loss: 3.659212540484023

Epoch:  103/3000  Loss: 3.651261724274734

Epoch:  104/3000  Loss: 3.635273073459494

Epoch:  105/3000  Loss: 3.6299468358357747

Epoch:  106/3000  Loss: 3.6109635698384253

Epoch:  107/3000  Loss: 3.6079048255394244

Epoch:  108/3000  Loss: 3.5986170708448038

Epoch:  109/3000  Loss: 3.5822404269514414

Epoch:  110/3000  Loss: 3.573671351356068

Epoch:  111/3000  Loss: 3.565495257542051

Epoch:  112/3000  Loss: 3.5533610974235095

Epoch:  113/3000  Loss: 3.542011111906205

Epoch:  114/3000  Loss: 3.5247280011231874

Epoch:  115/3000  Loss: 3.5159202005671357

Epoch:  116/3000  Loss: 3.500845474484323

Epoch:  117/3000  Loss: 3.489936659253877

Epoch:  118/3000  Loss: 3.4811586336157787

Epoch:  119/3000  Loss: 3.4741640759610584

Epoch:  120/3000  Loss: 3.4587463735163895

Epoch:  121/3000  Loss: 3.4524714278078625

Epoch:  122/3000  Loss: 3.446549576726453

Epoch:  123/3000  Loss: 3.4413793574804545

Epoch:  124/3000  Loss: 3.433823216646567

Epoch:  125/3000  Loss: 3.418310055787536

Epoch:  126/3000  Loss: 3.410338658848028

Epoch:  127/3000  Loss: 3.4041004734477776

Epoch:  128/3000  Loss: 3.3927731179642953

Epoch:  129/3000  Loss: 3.3859526266996887

Epoch:  130/3000  Loss: 3.3810056911117727

Epoch:  131/3000  Loss: 3.3766370340325365

Epoch:  132/3000  Loss: 3.3666759918475972

Epoch:  133/3000  Loss: 3.358562890962623

Epoch:  134/3000  Loss: 3.3539811594732876

Epoch:  135/3000  Loss: 3.3448152722983524

Epoch:  136/3000  Loss: 3.343013380313742

Epoch:  137/3000  Loss: 3.328166641586128

Epoch:  138/3000  Loss: 3.3255782916628083

Epoch:  139/3000  Loss: 3.315073366274779

Epoch:  140/3000  Loss: 3.3098127765217047

Epoch:  141/3000  Loss: 3.3081998748340826

Epoch:  142/3000  Loss: 3.2976600997749417

Epoch:  143/3000  Loss: 3.2926406164278927

Epoch:  144/3000  Loss: 3.295508191229283

Epoch:  145/3000  Loss: 3.282500058755107

Epoch:  146/3000  Loss: 3.2713720003763833

Epoch:  147/3000  Loss: 3.2718868239172574

Epoch:  148/3000  Loss: 3.2619052810230476

Epoch:  149/3000  Loss: 3.261252542473804

Epoch:  150/3000  Loss: 3.2495485026260904

Epoch:  151/3000  Loss: 3.2450785099774944

Epoch:  152/3000  Loss: 3.240841888011187

Epoch:  153/3000  Loss: 3.23717946841799

Epoch:  154/3000  Loss: 3.2307424161626006

Epoch:  155/3000  Loss: 3.225619917902453

Epoch:  156/3000  Loss: 3.219891012126002

Epoch:  157/3000  Loss: 3.2128283697983315

Epoch:  158/3000  Loss: 3.207377142193674

Epoch:  159/3000  Loss: 3.200634993081805

Epoch:  160/3000  Loss: 3.2002551763907245

Epoch:  161/3000  Loss: 3.1889388500959024

Epoch:  162/3000  Loss: 3.1879396263210253

Epoch:  163/3000  Loss: 3.1799259147424808

Epoch:  164/3000  Loss: 3.1735554042903855

Epoch:  165/3000  Loss: 3.1752801116855665

Epoch:  166/3000  Loss: 3.17023013706865

Epoch:  167/3000  Loss: 3.1698997848335355

Epoch:  168/3000  Loss: 3.1602348398888247

Epoch:  169/3000  Loss: 3.1527629441228404

Epoch:  170/3000  Loss: 3.1541831060387624

Epoch:  171/3000  Loss: 3.143393817441217

Epoch:  172/3000  Loss: 3.1446524861215175

Epoch:  173/3000  Loss: 3.146630190706801

Epoch:  174/3000  Loss: 3.1329697904915643

Epoch:  175/3000  Loss: 3.12536639893192

Epoch:  176/3000  Loss: 3.1294385268770415

Epoch:  177/3000  Loss: 3.119809354036704

Epoch:  178/3000  Loss: 3.116888877167099

Epoch:  179/3000  Loss: 3.1084656030282205

Epoch:  180/3000  Loss: 3.1064226715044043

Epoch:  181/3000  Loss: 3.103075031302441

Epoch:  182/3000  Loss: 3.0984379664234734

Epoch:  183/3000  Loss: 3.0974105818518276

Epoch:  184/3000  Loss: 3.09790898520371

Epoch:  185/3000  Loss: 3.0941015331224464

Epoch:  186/3000  Loss: 3.0853046844745506

Epoch:  187/3000  Loss: 3.0809881933804215

Epoch:  188/3000  Loss: 3.0838823455503617

Epoch:  189/3000  Loss: 3.07588313256187

Epoch:  190/3000  Loss: 3.0715731686559216

Epoch:  191/3000  Loss: 3.0672682307232386

Epoch:  192/3000  Loss: 3.0645530980208826

Epoch:  193/3000  Loss: 3.063295350129577

Epoch:  194/3000  Loss: 3.0560990064993674

Epoch:  195/3000  Loss: 3.0508512749069037

Epoch:  196/3000  Loss: 3.0481489691241035

Epoch:  197/3000  Loss: 3.043376108147632

Epoch:  198/3000  Loss: 3.0449800946246617

Epoch:  199/3000  Loss: 3.043288561393475

Epoch:  200/3000  Loss: 3.0374663901054997

Epoch:  201/3000  Loss: 3.031935098801536

Epoch:  202/3000  Loss: 3.032428492622814

Epoch:  203/3000  Loss: 3.0291035252055902

Epoch:  204/3000  Loss: 3.02245636041137

Epoch:  205/3000  Loss: 3.0199029982775105

Epoch:  206/3000  Loss: 3.0198470652788534

Epoch:  207/3000  Loss: 3.0140105548946337

Epoch:  208/3000  Loss: 3.01569270868411

Epoch:  209/3000  Loss: 3.0086400366377557

Epoch:  210/3000  Loss: 3.0017098761152945

Epoch:  211/3000  Loss: 2.999189910669436

Epoch:  212/3000  Loss: 2.999564444881746

Epoch:  213/3000  Loss: 2.9948138094496453

Epoch:  214/3000  Loss: 2.99650592420293

Epoch:  215/3000  Loss: 2.991711176400897

Epoch:  216/3000  Loss: 2.9842787901560466

Epoch:  217/3000  Loss: 2.979794721493776

Epoch:  218/3000  Loss: 2.977611241395446

Epoch:  219/3000  Loss: 2.980186126423978

Epoch:  220/3000  Loss: 2.978551656350322

Epoch:  221/3000  Loss: 2.971561202783694

Epoch:  222/3000  Loss: 2.9697032418744316

Epoch:  223/3000  Loss: 2.964754714636967

Epoch:  224/3000  Loss: 2.9642658118543954

Epoch:  225/3000  Loss: 2.956629519627012

Epoch:  226/3000  Loss: 2.9526813797567084

Epoch:  227/3000  Loss: 2.956679572730229

Epoch:  228/3000  Loss: 2.949218188209095

Epoch:  229/3000  Loss: 2.9481917518308793

Epoch:  230/3000  Loss: 2.9459546977076037

Epoch:  231/3000  Loss: 2.9415325581342326

Epoch:  232/3000  Loss: 2.9508270066359947

Epoch:  233/3000  Loss: 2.939721626522897

Epoch:  234/3000  Loss: 2.936414412794442

Epoch:  235/3000  Loss: 2.9332226857371713

Epoch:  236/3000  Loss: 2.9340233967222016

Epoch:  237/3000  Loss: 2.9271923695487536

</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_text output_error">
<pre>
<span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">KeyboardInterrupt</span>                         Traceback (most recent call last)
<span class="ansi-green-fg">&lt;ipython-input-50-08f0cb61c5e7&gt;</span> in <span class="ansi-cyan-fg">&lt;module&gt;</span><span class="ansi-blue-fg">()</span>
<span class="ansi-green-intense-fg ansi-bold">     18</span> 
<span class="ansi-green-intense-fg ansi-bold">     19</span>     <span class="ansi-red-fg"># training the model</span>
<span class="ansi-green-fg">---&gt; 20</span><span class="ansi-red-fg">     </span>trained_rnn <span class="ansi-blue-fg">=</span> train_rnn<span class="ansi-blue-fg">(</span>rnn<span class="ansi-blue-fg">,</span> batch_size<span class="ansi-blue-fg">,</span> optimizer<span class="ansi-blue-fg">,</span> criterion<span class="ansi-blue-fg">,</span> num_epochs<span class="ansi-blue-fg">,</span> show_every_n_batches<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">     21</span> 
<span class="ansi-green-intense-fg ansi-bold">     22</span>     <span class="ansi-red-fg"># saving the trained model</span>

<span class="ansi-green-fg">&lt;ipython-input-16-54bd9dc2f859&gt;</span> in <span class="ansi-cyan-fg">train_rnn</span><span class="ansi-blue-fg">(rnn, batch_size, optimizer, criterion, n_epochs, show_every_n_batches)</span>
<span class="ansi-green-intense-fg ansi-bold">     22</span> 
<span class="ansi-green-intense-fg ansi-bold">     23</span>             <span class="ansi-red-fg"># forward, back prop</span>
<span class="ansi-green-fg">---&gt; 24</span><span class="ansi-red-fg">             </span>loss<span class="ansi-blue-fg">,</span> hidden <span class="ansi-blue-fg">=</span> forward_back_prop<span class="ansi-blue-fg">(</span>rnn<span class="ansi-blue-fg">,</span> optimizer<span class="ansi-blue-fg">,</span> criterion<span class="ansi-blue-fg">,</span> inputs<span class="ansi-blue-fg">,</span> labels<span class="ansi-blue-fg">,</span> hidden<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">     25</span>             <span class="ansi-red-fg"># record loss</span>
<span class="ansi-green-intense-fg ansi-bold">     26</span>             batch_losses<span class="ansi-blue-fg">.</span>append<span class="ansi-blue-fg">(</span>loss<span class="ansi-blue-fg">)</span>

<span class="ansi-green-fg">&lt;ipython-input-15-c302a43168d3&gt;</span> in <span class="ansi-cyan-fg">forward_back_prop</span><span class="ansi-blue-fg">(rnn, optimizer, criterion, inp, target, hidden)</span>
<span class="ansi-green-intense-fg ansi-bold">     28</span>     loss<span class="ansi-blue-fg">.</span>backward<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">     29</span>     loss<span class="ansi-blue-fg">=</span> loss<span class="ansi-blue-fg">.</span>item<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span>
<span class="ansi-green-fg">---&gt; 30</span><span class="ansi-red-fg">     </span>optimizer<span class="ansi-blue-fg">.</span>step<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">     31</span>     <span class="ansi-green-fg">return</span> loss<span class="ansi-blue-fg">,</span> hidden
<span class="ansi-green-intense-fg ansi-bold">     32</span> <span class="ansi-red-fg"># Note that these tests aren&#39;t completely extensive.</span>

<span class="ansi-green-fg">/opt/conda/lib/python3.6/site-packages/torch/optim/adam.py</span> in <span class="ansi-cyan-fg">step</span><span class="ansi-blue-fg">(self, closure)</span>
<span class="ansi-green-intense-fg ansi-bold">     98</span>                     denom <span class="ansi-blue-fg">=</span> max_exp_avg_sq<span class="ansi-blue-fg">.</span>sqrt<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">.</span>add_<span class="ansi-blue-fg">(</span>group<span class="ansi-blue-fg">[</span><span class="ansi-blue-fg">&#39;eps&#39;</span><span class="ansi-blue-fg">]</span><span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">     99</span>                 <span class="ansi-green-fg">else</span><span class="ansi-blue-fg">:</span>
<span class="ansi-green-fg">--&gt; 100</span><span class="ansi-red-fg">                     </span>denom <span class="ansi-blue-fg">=</span> exp_avg_sq<span class="ansi-blue-fg">.</span>sqrt<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">.</span>add_<span class="ansi-blue-fg">(</span>group<span class="ansi-blue-fg">[</span><span class="ansi-blue-fg">&#39;eps&#39;</span><span class="ansi-blue-fg">]</span><span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    101</span> 
<span class="ansi-green-intense-fg ansi-bold">    102</span>                 bias_correction1 <span class="ansi-blue-fg">=</span> <span class="ansi-cyan-fg">1</span> <span class="ansi-blue-fg">-</span> beta1 <span class="ansi-blue-fg">**</span> state<span class="ansi-blue-fg">[</span><span class="ansi-blue-fg">&#39;step&#39;</span><span class="ansi-blue-fg">]</span>

<span class="ansi-red-fg">KeyboardInterrupt</span>: </pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[45]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">workspace_utils</span> <span class="k">import</span> <span class="n">active_session</span>

<span class="k">with</span> <span class="n">active_session</span><span class="p">():</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    DON&#39;T MODIFY ANYTHING IN THIS CELL</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;sequence_length: </span><span class="si">{}</span><span class="se">\n</span><span class="s1">batch_size: </span><span class="si">{}</span><span class="se">\n</span><span class="s1">learning_rate: </span><span class="si">{}</span><span class="se">\n</span><span class="s1">embedding_dim: </span><span class="si">{}</span><span class="se">\n</span><span class="s1">n_layers: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">sequence_length</span><span class="p">,</span><span class="n">batch_size</span><span class="p">,</span><span class="n">learning_rate</span><span class="p">,</span><span class="n">embedding_dim</span><span class="p">,</span><span class="n">n_layers</span><span class="p">))</span>

    <span class="c1"># create model and move to gpu if available</span>
    <span class="n">rnn</span> <span class="o">=</span> <span class="n">RNN</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">,</span> <span class="n">n_layers</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">train_on_gpu</span><span class="p">:</span>
        <span class="n">rnn</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>

    <span class="c1"># defining loss and optimization functions for training</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">rnn</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>
    <span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>

    <span class="c1"># training the model</span>
    <span class="n">trained_rnn</span> <span class="o">=</span> <span class="n">train_rnn</span><span class="p">(</span><span class="n">rnn</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">num_epochs</span><span class="p">,</span> <span class="n">show_every_n_batches</span><span class="p">)</span>

    <span class="c1"># saving the trained model</span>
    <span class="n">helper</span><span class="o">.</span><span class="n">save_model</span><span class="p">(</span><span class="s1">&#39;/trained_rnn&#39;</span><span class="p">,</span> <span class="n">trained_rnn</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Model Trained and Saved&#39;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;sequence_length: </span><span class="si">{}</span><span class="se">\n</span><span class="s1">batch_size: </span><span class="si">{}</span><span class="se">\n</span><span class="s1">learning_rate: </span><span class="si">{}</span><span class="se">\n</span><span class="s1">embedding_dim: </span><span class="si">{}</span><span class="se">\n</span><span class="s1">n_layers: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">sequence_length</span><span class="p">,</span><span class="n">batch_size</span><span class="p">,</span><span class="n">learning_rate</span><span class="p">,</span><span class="n">embedding_dim</span><span class="p">,</span><span class="n">n_layers</span><span class="p">))</span>
    
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>sequence_length: 8
batch_size: 256
learning_rate: 0.0006
embedding_dim: 300
n_layers: 2
Training for 3000 epoch(s)...
Epoch:    1/3000  Loss: 8.081838583946228

Epoch:    1/3000  Loss: 6.120721523761749

Epoch:    2/3000  Loss: 5.652303864093537

Epoch:    2/3000  Loss: 5.4752587890625

Epoch:    3/3000  Loss: 5.304156993297821

Epoch:    3/3000  Loss: 5.247151322364807

Epoch:    4/3000  Loss: 5.141844925981887

Epoch:    4/3000  Loss: 5.094993231296539

Epoch:    5/3000  Loss: 4.996995662121063

Epoch:    5/3000  Loss: 4.954444153308868

Epoch:    6/3000  Loss: 4.8597314469357755

Epoch:    6/3000  Loss: 4.810218162536621

Epoch:    7/3000  Loss: 4.706127327046496

Epoch:    7/3000  Loss: 4.657008006572723

Epoch:    8/3000  Loss: 4.562745825787808

Epoch:    8/3000  Loss: 4.528742846250534

Epoch:    9/3000  Loss: 4.435271200220636

Epoch:    9/3000  Loss: 4.402329622507096

Epoch:   10/3000  Loss: 4.323272229255514

Epoch:   10/3000  Loss: 4.2924572014808655

Epoch:   11/3000  Loss: 4.211496955790418

Epoch:   11/3000  Loss: 4.199072033166885

Epoch:   12/3000  Loss: 4.127175750123693

Epoch:   12/3000  Loss: 4.118938990831375

Epoch:   13/3000  Loss: 4.05157019736919

Epoch:   13/3000  Loss: 4.044633485078812

Epoch:   14/3000  Loss: 3.9817868831309866

Epoch:   14/3000  Loss: 3.9692168188095094

Epoch:   15/3000  Loss: 3.9048657153515105

Epoch:   15/3000  Loss: 3.9084026193618775

Epoch:   16/3000  Loss: 3.848904673596646

Epoch:   16/3000  Loss: 3.8445257353782654

Epoch:   17/3000  Loss: 3.791740579808012

Epoch:   17/3000  Loss: 3.791750725507736

Epoch:   18/3000  Loss: 3.7475802563606426

Epoch:   18/3000  Loss: 3.7511862087249757

Epoch:   19/3000  Loss: 3.695538261088919

Epoch:   19/3000  Loss: 3.704168722629547

Epoch:   20/3000  Loss: 3.652292537689209

Epoch:   20/3000  Loss: 3.669219205379486

Epoch:   21/3000  Loss: 3.6103322637842057

Epoch:   21/3000  Loss: 3.623743339776993

Epoch:   22/3000  Loss: 3.571423165341641

Epoch:   22/3000  Loss: 3.5874414372444154

Epoch:   23/3000  Loss: 3.536178033909899

Epoch:   23/3000  Loss: 3.5480160903930664

Epoch:   24/3000  Loss: 3.481774705521604

Epoch:   24/3000  Loss: 3.494359332323074

Epoch:   25/3000  Loss: 3.440820744697084

Epoch:   25/3000  Loss: 3.4570290231704712

Epoch:   26/3000  Loss: 3.395540184670306

Epoch:   26/3000  Loss: 3.409793657064438

Epoch:   27/3000  Loss: 3.3614150605303177

Epoch:   27/3000  Loss: 3.3761148333549498

Epoch:   28/3000  Loss: 3.3204926003801063

Epoch:   28/3000  Loss: 3.3361409986019135

Epoch:   29/3000  Loss: 3.277727081420574

Epoch:   29/3000  Loss: 3.2917860698699952

Epoch:   30/3000  Loss: 3.232989872262833

Epoch:   30/3000  Loss: 3.259508761167526

Epoch:   31/3000  Loss: 3.2023641890667855

Epoch:   31/3000  Loss: 3.221570219993591

Epoch:   32/3000  Loss: 3.1745161228991567

Epoch:   32/3000  Loss: 3.190513685941696

Epoch:   33/3000  Loss: 3.1469002967185165

Epoch:   33/3000  Loss: 3.168348171710968

Epoch:   34/3000  Loss: 3.1162490885308447

Epoch:   34/3000  Loss: 3.1487307143211365

Epoch:   35/3000  Loss: 3.097056008399801

Epoch:   35/3000  Loss: 3.108847597837448

Epoch:   36/3000  Loss: 3.0706196480609003

Epoch:   36/3000  Loss: 3.0902770936489103

Epoch:   37/3000  Loss: 3.0492787158235592

Epoch:   37/3000  Loss: 3.0695779049396514

Epoch:   38/3000  Loss: 3.0240286867669286

Epoch:   38/3000  Loss: 3.052732541561127

Epoch:   39/3000  Loss: 3.006506685500449

Epoch:   39/3000  Loss: 3.029424899816513

Epoch:   40/3000  Loss: 2.988537030524396

Epoch:   40/3000  Loss: 3.016130670309067

Epoch:   41/3000  Loss: 2.9759949531960994

</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_text output_error">
<pre>
<span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">KeyboardInterrupt</span>                         Traceback (most recent call last)
<span class="ansi-green-fg">&lt;ipython-input-45-08f0cb61c5e7&gt;</span> in <span class="ansi-cyan-fg">&lt;module&gt;</span><span class="ansi-blue-fg">()</span>
<span class="ansi-green-intense-fg ansi-bold">     18</span> 
<span class="ansi-green-intense-fg ansi-bold">     19</span>     <span class="ansi-red-fg"># training the model</span>
<span class="ansi-green-fg">---&gt; 20</span><span class="ansi-red-fg">     </span>trained_rnn <span class="ansi-blue-fg">=</span> train_rnn<span class="ansi-blue-fg">(</span>rnn<span class="ansi-blue-fg">,</span> batch_size<span class="ansi-blue-fg">,</span> optimizer<span class="ansi-blue-fg">,</span> criterion<span class="ansi-blue-fg">,</span> num_epochs<span class="ansi-blue-fg">,</span> show_every_n_batches<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">     21</span> 
<span class="ansi-green-intense-fg ansi-bold">     22</span>     <span class="ansi-red-fg"># saving the trained model</span>

<span class="ansi-green-fg">&lt;ipython-input-16-54bd9dc2f859&gt;</span> in <span class="ansi-cyan-fg">train_rnn</span><span class="ansi-blue-fg">(rnn, batch_size, optimizer, criterion, n_epochs, show_every_n_batches)</span>
<span class="ansi-green-intense-fg ansi-bold">     22</span> 
<span class="ansi-green-intense-fg ansi-bold">     23</span>             <span class="ansi-red-fg"># forward, back prop</span>
<span class="ansi-green-fg">---&gt; 24</span><span class="ansi-red-fg">             </span>loss<span class="ansi-blue-fg">,</span> hidden <span class="ansi-blue-fg">=</span> forward_back_prop<span class="ansi-blue-fg">(</span>rnn<span class="ansi-blue-fg">,</span> optimizer<span class="ansi-blue-fg">,</span> criterion<span class="ansi-blue-fg">,</span> inputs<span class="ansi-blue-fg">,</span> labels<span class="ansi-blue-fg">,</span> hidden<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">     25</span>             <span class="ansi-red-fg"># record loss</span>
<span class="ansi-green-intense-fg ansi-bold">     26</span>             batch_losses<span class="ansi-blue-fg">.</span>append<span class="ansi-blue-fg">(</span>loss<span class="ansi-blue-fg">)</span>

<span class="ansi-green-fg">&lt;ipython-input-15-c302a43168d3&gt;</span> in <span class="ansi-cyan-fg">forward_back_prop</span><span class="ansi-blue-fg">(rnn, optimizer, criterion, inp, target, hidden)</span>
<span class="ansi-green-intense-fg ansi-bold">     27</span>     loss <span class="ansi-blue-fg">=</span> criterion<span class="ansi-blue-fg">(</span>output<span class="ansi-blue-fg">.</span>squeeze<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">,</span> target<span class="ansi-blue-fg">.</span>long<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">     28</span>     loss<span class="ansi-blue-fg">.</span>backward<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span>
<span class="ansi-green-fg">---&gt; 29</span><span class="ansi-red-fg">     </span>loss<span class="ansi-blue-fg">=</span> loss<span class="ansi-blue-fg">.</span>item<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">     30</span>     optimizer<span class="ansi-blue-fg">.</span>step<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">     31</span>     <span class="ansi-green-fg">return</span> loss<span class="ansi-blue-fg">,</span> hidden

<span class="ansi-red-fg">KeyboardInterrupt</span>: </pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[40]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">workspace_utils</span> <span class="k">import</span> <span class="n">active_session</span>

<span class="k">with</span> <span class="n">active_session</span><span class="p">():</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    DON&#39;T MODIFY ANYTHING IN THIS CELL</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;sequence_length: </span><span class="si">{}</span><span class="se">\n</span><span class="s1">batch_size: </span><span class="si">{}</span><span class="se">\n</span><span class="s1">learning_rate: </span><span class="si">{}</span><span class="se">\n</span><span class="s1">embedding_dim: </span><span class="si">{}</span><span class="se">\n</span><span class="s1">n_layers: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">sequence_length</span><span class="p">,</span><span class="n">batch_size</span><span class="p">,</span><span class="n">learning_rate</span><span class="p">,</span><span class="n">embedding_dim</span><span class="p">,</span><span class="n">n_layers</span><span class="p">))</span>

    <span class="c1"># create model and move to gpu if available</span>
    <span class="n">rnn</span> <span class="o">=</span> <span class="n">RNN</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">,</span> <span class="n">n_layers</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">train_on_gpu</span><span class="p">:</span>
        <span class="n">rnn</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>

    <span class="c1"># defining loss and optimization functions for training</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">rnn</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>
    <span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>

    <span class="c1"># training the model</span>
    <span class="n">trained_rnn</span> <span class="o">=</span> <span class="n">train_rnn</span><span class="p">(</span><span class="n">rnn</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">num_epochs</span><span class="p">,</span> <span class="n">show_every_n_batches</span><span class="p">)</span>

    <span class="c1"># saving the trained model</span>
    <span class="n">helper</span><span class="o">.</span><span class="n">save_model</span><span class="p">(</span><span class="s1">&#39;/trained_rnn&#39;</span><span class="p">,</span> <span class="n">trained_rnn</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Model Trained and Saved&#39;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;sequence_length: </span><span class="si">{}</span><span class="se">\n</span><span class="s1">batch_size: </span><span class="si">{}</span><span class="se">\n</span><span class="s1">learning_rate: </span><span class="si">{}</span><span class="se">\n</span><span class="s1">embedding_dim: </span><span class="si">{}</span><span class="se">\n</span><span class="s1">n_layers: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">sequence_length</span><span class="p">,</span><span class="n">batch_size</span><span class="p">,</span><span class="n">learning_rate</span><span class="p">,</span><span class="n">embedding_dim</span><span class="p">,</span><span class="n">n_layers</span><span class="p">))</span>
    
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>sequence_length: 8
batch_size: 256
learning_rate: 0.001
embedding_dim: 1000
n_layers: 2
Training for 1000 epoch(s)...
Epoch:    1/1000  Loss: 7.634065723419189

Epoch:    1/1000  Loss: 5.955679357051849

Epoch:    2/1000  Loss: 5.511626470849869

Epoch:    2/1000  Loss: 5.392672443389893

Epoch:    3/1000  Loss: 5.242239536123073

Epoch:    3/1000  Loss: 5.202348861694336

Epoch:    4/1000  Loss: 5.090768749155897

Epoch:    4/1000  Loss: 5.053709847927093

Epoch:    5/1000  Loss: 4.957887150378937

Epoch:    5/1000  Loss: 4.916481184959411

Epoch:    6/1000  Loss: 4.816588998348155

Epoch:    6/1000  Loss: 4.771510138511657

Epoch:    7/1000  Loss: 4.675779872244977

Epoch:    7/1000  Loss: 4.61408976316452

Epoch:    8/1000  Loss: 4.525214684263188

Epoch:    8/1000  Loss: 4.4839259624481205

Epoch:    9/1000  Loss: 4.4155620574951175

Epoch:    9/1000  Loss: 4.385436755418778

Epoch:   10/1000  Loss: 4.3220946910533495

Epoch:   10/1000  Loss: 4.297259469032287

Epoch:   11/1000  Loss: 4.240090514243917

Epoch:   11/1000  Loss: 4.220571632385254

Epoch:   12/1000  Loss: 4.152716540275736

Epoch:   12/1000  Loss: 4.137719215154648

Epoch:   13/1000  Loss: 4.079534976025845

Epoch:   13/1000  Loss: 4.057082784175873

Epoch:   14/1000  Loss: 4.016883562980814

Epoch:   14/1000  Loss: 4.011215419769287

Epoch:   15/1000  Loss: 3.9694536807689262

Epoch:   15/1000  Loss: 3.9624884712696073

Epoch:   16/1000  Loss: 3.9196416499766897

Epoch:   16/1000  Loss: 3.9184029376506806

Epoch:   17/1000  Loss: 3.8880261512512857

Epoch:   17/1000  Loss: 3.885869483947754

Epoch:   18/1000  Loss: 3.8428219257517062

Epoch:   18/1000  Loss: 3.8511102998256685

Epoch:   19/1000  Loss: 3.813191530552316

Epoch:   19/1000  Loss: 3.8064615333080294

Epoch:   20/1000  Loss: 3.7797339398810204

Epoch:   20/1000  Loss: 3.783803313970566

Epoch:   21/1000  Loss: 3.7504092459983016

Epoch:   21/1000  Loss: 3.7622399175167085

Epoch:   22/1000  Loss: 3.728695869445801

Epoch:   22/1000  Loss: 3.7373017299175264

Epoch:   23/1000  Loss: 3.7017721987785177

Epoch:   23/1000  Loss: 3.7233747494220735

Epoch:   24/1000  Loss: 3.6796937242467354

Epoch:   24/1000  Loss: 3.6884656739234924

Epoch:   25/1000  Loss: 3.661184925728656

Epoch:   25/1000  Loss: 3.6726776552200318

Epoch:   26/1000  Loss: 3.642161052785021

Epoch:   26/1000  Loss: 3.6478824281692503

Epoch:   27/1000  Loss: 3.6244838197180567

Epoch:   27/1000  Loss: 3.6310182869434358

Epoch:   28/1000  Loss: 3.5986186220290812

Epoch:   28/1000  Loss: 3.6137875258922576

Epoch:   29/1000  Loss: 3.5833811283111574

Epoch:   29/1000  Loss: 3.593203703165054

Epoch:   30/1000  Loss: 3.5576834009048786

Epoch:   30/1000  Loss: 3.5724657654762266

Epoch:   31/1000  Loss: 3.5400689490298007

Epoch:   31/1000  Loss: 3.5559047055244446

Epoch:   32/1000  Loss: 3.5169536407957684

Epoch:   32/1000  Loss: 3.531295983791351

Epoch:   33/1000  Loss: 3.496426431168901

Epoch:   33/1000  Loss: 3.5082546186447146

Epoch:   34/1000  Loss: 3.4845676665610457

Epoch:   34/1000  Loss: 3.4986316537857056

Epoch:   35/1000  Loss: 3.459781482371878

Epoch:   35/1000  Loss: 3.4850313007831573

Epoch:   36/1000  Loss: 3.451435857123517

Epoch:   36/1000  Loss: 3.4706027138233186

</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_text output_error">
<pre>
<span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">KeyboardInterrupt</span>                         Traceback (most recent call last)
<span class="ansi-green-fg">&lt;ipython-input-40-08f0cb61c5e7&gt;</span> in <span class="ansi-cyan-fg">&lt;module&gt;</span><span class="ansi-blue-fg">()</span>
<span class="ansi-green-intense-fg ansi-bold">     18</span> 
<span class="ansi-green-intense-fg ansi-bold">     19</span>     <span class="ansi-red-fg"># training the model</span>
<span class="ansi-green-fg">---&gt; 20</span><span class="ansi-red-fg">     </span>trained_rnn <span class="ansi-blue-fg">=</span> train_rnn<span class="ansi-blue-fg">(</span>rnn<span class="ansi-blue-fg">,</span> batch_size<span class="ansi-blue-fg">,</span> optimizer<span class="ansi-blue-fg">,</span> criterion<span class="ansi-blue-fg">,</span> num_epochs<span class="ansi-blue-fg">,</span> show_every_n_batches<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">     21</span> 
<span class="ansi-green-intense-fg ansi-bold">     22</span>     <span class="ansi-red-fg"># saving the trained model</span>

<span class="ansi-green-fg">&lt;ipython-input-16-54bd9dc2f859&gt;</span> in <span class="ansi-cyan-fg">train_rnn</span><span class="ansi-blue-fg">(rnn, batch_size, optimizer, criterion, n_epochs, show_every_n_batches)</span>
<span class="ansi-green-intense-fg ansi-bold">     22</span> 
<span class="ansi-green-intense-fg ansi-bold">     23</span>             <span class="ansi-red-fg"># forward, back prop</span>
<span class="ansi-green-fg">---&gt; 24</span><span class="ansi-red-fg">             </span>loss<span class="ansi-blue-fg">,</span> hidden <span class="ansi-blue-fg">=</span> forward_back_prop<span class="ansi-blue-fg">(</span>rnn<span class="ansi-blue-fg">,</span> optimizer<span class="ansi-blue-fg">,</span> criterion<span class="ansi-blue-fg">,</span> inputs<span class="ansi-blue-fg">,</span> labels<span class="ansi-blue-fg">,</span> hidden<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">     25</span>             <span class="ansi-red-fg"># record loss</span>
<span class="ansi-green-intense-fg ansi-bold">     26</span>             batch_losses<span class="ansi-blue-fg">.</span>append<span class="ansi-blue-fg">(</span>loss<span class="ansi-blue-fg">)</span>

<span class="ansi-green-fg">&lt;ipython-input-15-c302a43168d3&gt;</span> in <span class="ansi-cyan-fg">forward_back_prop</span><span class="ansi-blue-fg">(rnn, optimizer, criterion, inp, target, hidden)</span>
<span class="ansi-green-intense-fg ansi-bold">     27</span>     loss <span class="ansi-blue-fg">=</span> criterion<span class="ansi-blue-fg">(</span>output<span class="ansi-blue-fg">.</span>squeeze<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">,</span> target<span class="ansi-blue-fg">.</span>long<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">     28</span>     loss<span class="ansi-blue-fg">.</span>backward<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span>
<span class="ansi-green-fg">---&gt; 29</span><span class="ansi-red-fg">     </span>loss<span class="ansi-blue-fg">=</span> loss<span class="ansi-blue-fg">.</span>item<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">     30</span>     optimizer<span class="ansi-blue-fg">.</span>step<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">     31</span>     <span class="ansi-green-fg">return</span> loss<span class="ansi-blue-fg">,</span> hidden

<span class="ansi-red-fg">KeyboardInterrupt</span>: </pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[37]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">workspace_utils</span> <span class="k">import</span> <span class="n">active_session</span>

<span class="k">with</span> <span class="n">active_session</span><span class="p">():</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    DON&#39;T MODIFY ANYTHING IN THIS CELL</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;sequence_length: </span><span class="si">{}</span><span class="se">\n</span><span class="s1">batch_size: </span><span class="si">{}</span><span class="se">\n</span><span class="s1">learning_rate: </span><span class="si">{}</span><span class="se">\n</span><span class="s1">embedding_dim: </span><span class="si">{}</span><span class="se">\n</span><span class="s1">n_layers: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">sequence_length</span><span class="p">,</span><span class="n">batch_size</span><span class="p">,</span><span class="n">learning_rate</span><span class="p">,</span><span class="n">embedding_dim</span><span class="p">,</span><span class="n">n_layers</span><span class="p">))</span>

    <span class="c1"># create model and move to gpu if available</span>
    <span class="n">rnn</span> <span class="o">=</span> <span class="n">RNN</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">,</span> <span class="n">n_layers</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">train_on_gpu</span><span class="p">:</span>
        <span class="n">rnn</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>

    <span class="c1"># defining loss and optimization functions for training</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">rnn</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>
    <span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>

    <span class="c1"># training the model</span>
    <span class="n">trained_rnn</span> <span class="o">=</span> <span class="n">train_rnn</span><span class="p">(</span><span class="n">rnn</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">num_epochs</span><span class="p">,</span> <span class="n">show_every_n_batches</span><span class="p">)</span>

    <span class="c1"># saving the trained model</span>
    <span class="n">helper</span><span class="o">.</span><span class="n">save_model</span><span class="p">(</span><span class="s1">&#39;/trained_rnn&#39;</span><span class="p">,</span> <span class="n">trained_rnn</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Model Trained and Saved&#39;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;sequence_length: </span><span class="si">{}</span><span class="se">\n</span><span class="s1">batch_size: </span><span class="si">{}</span><span class="se">\n</span><span class="s1">learning_rate: </span><span class="si">{}</span><span class="se">\n</span><span class="s1">embedding_dim: </span><span class="si">{}</span><span class="se">\n</span><span class="s1">n_layers: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">sequence_length</span><span class="p">,</span><span class="n">batch_size</span><span class="p">,</span><span class="n">learning_rate</span><span class="p">,</span><span class="n">embedding_dim</span><span class="p">,</span><span class="n">n_layers</span><span class="p">))</span>
    
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>sequence_length: 8
batch_size: 256
learning_rate: 0.001
embedding_dim: 300
n_layers: 2
Training for 1000 epoch(s)...
Epoch:    1/1000  Loss: 7.404851336479187

Epoch:    1/1000  Loss: 5.821729054450989

Epoch:    2/1000  Loss: 5.358581626161616

Epoch:    2/1000  Loss: 5.223323135375977

Epoch:    3/1000  Loss: 5.0404612561489675

Epoch:    3/1000  Loss: 4.9523903131484985

Epoch:    4/1000  Loss: 4.795686090753434

Epoch:    4/1000  Loss: 4.70781599521637

Epoch:    5/1000  Loss: 4.5727390157415515

Epoch:    5/1000  Loss: 4.492256083488464

Epoch:    6/1000  Loss: 4.378561362814396

Epoch:    6/1000  Loss: 4.320309098958969

Epoch:    7/1000  Loss: 4.2158536048645665

Epoch:    7/1000  Loss: 4.169388976097107

Epoch:    8/1000  Loss: 4.071609021247702

Epoch:    8/1000  Loss: 4.03673201918602

Epoch:    9/1000  Loss: 3.9398714704716458

Epoch:    9/1000  Loss: 3.8923615515232086

Epoch:   10/1000  Loss: 3.7964081541020818

Epoch:   10/1000  Loss: 3.7774944508075716

Epoch:   11/1000  Loss: 3.6990053044988755

Epoch:   11/1000  Loss: 3.679222697019577

Epoch:   12/1000  Loss: 3.5972913539156

Epoch:   12/1000  Loss: 3.576837348937988

Epoch:   13/1000  Loss: 3.5041437940394626

Epoch:   13/1000  Loss: 3.4941977715492247

Epoch:   14/1000  Loss: 3.427031762549218

Epoch:   14/1000  Loss: 3.4294348764419555

Epoch:   15/1000  Loss: 3.367478006444079

Epoch:   15/1000  Loss: 3.36379022359848

Epoch:   16/1000  Loss: 3.3099374649372506

Epoch:   16/1000  Loss: 3.305512881278992

Epoch:   17/1000  Loss: 3.257860543880057

Epoch:   17/1000  Loss: 3.257421495914459

Epoch:   18/1000  Loss: 3.2177584729296096

Epoch:   18/1000  Loss: 3.2119882881641386

Epoch:   19/1000  Loss: 3.1675134009503303

Epoch:   19/1000  Loss: 3.1752310478687287

Epoch:   20/1000  Loss: 3.131264847897469

Epoch:   20/1000  Loss: 3.1394532752037048

Epoch:   21/1000  Loss: 3.0893883745721045

Epoch:   21/1000  Loss: 3.1039215433597565

Epoch:   22/1000  Loss: 3.054159348061744

Epoch:   22/1000  Loss: 3.0741441118717194

Epoch:   23/1000  Loss: 3.030919186612393

Epoch:   23/1000  Loss: 3.0428610587120057

Epoch:   24/1000  Loss: 2.995635051930204

Epoch:   24/1000  Loss: 3.009693522453308

Epoch:   25/1000  Loss: 2.962107984055864

Epoch:   25/1000  Loss: 2.983493866920471

</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_text output_error">
<pre>
<span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">KeyboardInterrupt</span>                         Traceback (most recent call last)
<span class="ansi-green-fg">&lt;ipython-input-37-08f0cb61c5e7&gt;</span> in <span class="ansi-cyan-fg">&lt;module&gt;</span><span class="ansi-blue-fg">()</span>
<span class="ansi-green-intense-fg ansi-bold">     18</span> 
<span class="ansi-green-intense-fg ansi-bold">     19</span>     <span class="ansi-red-fg"># training the model</span>
<span class="ansi-green-fg">---&gt; 20</span><span class="ansi-red-fg">     </span>trained_rnn <span class="ansi-blue-fg">=</span> train_rnn<span class="ansi-blue-fg">(</span>rnn<span class="ansi-blue-fg">,</span> batch_size<span class="ansi-blue-fg">,</span> optimizer<span class="ansi-blue-fg">,</span> criterion<span class="ansi-blue-fg">,</span> num_epochs<span class="ansi-blue-fg">,</span> show_every_n_batches<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">     21</span> 
<span class="ansi-green-intense-fg ansi-bold">     22</span>     <span class="ansi-red-fg"># saving the trained model</span>

<span class="ansi-green-fg">&lt;ipython-input-16-54bd9dc2f859&gt;</span> in <span class="ansi-cyan-fg">train_rnn</span><span class="ansi-blue-fg">(rnn, batch_size, optimizer, criterion, n_epochs, show_every_n_batches)</span>
<span class="ansi-green-intense-fg ansi-bold">     22</span> 
<span class="ansi-green-intense-fg ansi-bold">     23</span>             <span class="ansi-red-fg"># forward, back prop</span>
<span class="ansi-green-fg">---&gt; 24</span><span class="ansi-red-fg">             </span>loss<span class="ansi-blue-fg">,</span> hidden <span class="ansi-blue-fg">=</span> forward_back_prop<span class="ansi-blue-fg">(</span>rnn<span class="ansi-blue-fg">,</span> optimizer<span class="ansi-blue-fg">,</span> criterion<span class="ansi-blue-fg">,</span> inputs<span class="ansi-blue-fg">,</span> labels<span class="ansi-blue-fg">,</span> hidden<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">     25</span>             <span class="ansi-red-fg"># record loss</span>
<span class="ansi-green-intense-fg ansi-bold">     26</span>             batch_losses<span class="ansi-blue-fg">.</span>append<span class="ansi-blue-fg">(</span>loss<span class="ansi-blue-fg">)</span>

<span class="ansi-green-fg">&lt;ipython-input-15-c302a43168d3&gt;</span> in <span class="ansi-cyan-fg">forward_back_prop</span><span class="ansi-blue-fg">(rnn, optimizer, criterion, inp, target, hidden)</span>
<span class="ansi-green-intense-fg ansi-bold">     27</span>     loss <span class="ansi-blue-fg">=</span> criterion<span class="ansi-blue-fg">(</span>output<span class="ansi-blue-fg">.</span>squeeze<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">,</span> target<span class="ansi-blue-fg">.</span>long<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">     28</span>     loss<span class="ansi-blue-fg">.</span>backward<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span>
<span class="ansi-green-fg">---&gt; 29</span><span class="ansi-red-fg">     </span>loss<span class="ansi-blue-fg">=</span> loss<span class="ansi-blue-fg">.</span>item<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">     30</span>     optimizer<span class="ansi-blue-fg">.</span>step<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">     31</span>     <span class="ansi-green-fg">return</span> loss<span class="ansi-blue-fg">,</span> hidden

<span class="ansi-red-fg">KeyboardInterrupt</span>: </pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[32]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">workspace_utils</span> <span class="k">import</span> <span class="n">active_session</span>

<span class="k">with</span> <span class="n">active_session</span><span class="p">():</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    DON&#39;T MODIFY ANYTHING IN THIS CELL</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;sequence_length: </span><span class="si">{}</span><span class="se">\n</span><span class="s1">batch_size: </span><span class="si">{}</span><span class="se">\n</span><span class="s1">learning_rate: </span><span class="si">{}</span><span class="se">\n</span><span class="s1">embedding_dim: </span><span class="si">{}</span><span class="se">\n</span><span class="s1">n_layers: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">sequence_length</span><span class="p">,</span><span class="n">batch_size</span><span class="p">,</span><span class="n">learning_rate</span><span class="p">,</span><span class="n">embedding_dim</span><span class="p">,</span><span class="n">n_layers</span><span class="p">))</span>

    <span class="c1"># create model and move to gpu if available</span>
    <span class="n">rnn</span> <span class="o">=</span> <span class="n">RNN</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">,</span> <span class="n">n_layers</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">train_on_gpu</span><span class="p">:</span>
        <span class="n">rnn</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>

    <span class="c1"># defining loss and optimization functions for training</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">rnn</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>
    <span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>

    <span class="c1"># training the model</span>
    <span class="n">trained_rnn</span> <span class="o">=</span> <span class="n">train_rnn</span><span class="p">(</span><span class="n">rnn</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">num_epochs</span><span class="p">,</span> <span class="n">show_every_n_batches</span><span class="p">)</span>

    <span class="c1"># saving the trained model</span>
    <span class="n">helper</span><span class="o">.</span><span class="n">save_model</span><span class="p">(</span><span class="s1">&#39;/trained_rnn&#39;</span><span class="p">,</span> <span class="n">trained_rnn</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Model Trained and Saved&#39;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;sequence_length: </span><span class="si">{}</span><span class="se">\n</span><span class="s1">batch_size: </span><span class="si">{}</span><span class="se">\n</span><span class="s1">learning_rate: </span><span class="si">{}</span><span class="se">\n</span><span class="s1">embedding_dim: </span><span class="si">{}</span><span class="se">\n</span><span class="s1">n_layers: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">sequence_length</span><span class="p">,</span><span class="n">batch_size</span><span class="p">,</span><span class="n">learning_rate</span><span class="p">,</span><span class="n">embedding_dim</span><span class="p">,</span><span class="n">n_layers</span><span class="p">))</span>
    
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>sequence_length: 8
batch_size: 256
learning_rate: 0.0001
embedding_dim: 300
n_layers: 2
Training for 1000 epoch(s)...
Epoch:    1/1000  Loss: 9.978208637237548

Epoch:    1/1000  Loss: 9.891534905433655

Epoch:    2/1000  Loss: 9.703820244809414

Epoch:    2/1000  Loss: 9.317116713523864

Epoch:    3/1000  Loss: 8.797745177086364

Epoch:    3/1000  Loss: 8.388129572868348

Epoch:    4/1000  Loss: 8.063018916515594

Epoch:    4/1000  Loss: 7.801344282627106

Epoch:    5/1000  Loss: 7.569390552601916

Epoch:    5/1000  Loss: 7.386419682502747

Epoch:    6/1000  Loss: 7.206695465331382

Epoch:    6/1000  Loss: 7.0723322796821595

Epoch:    7/1000  Loss: 6.9276548953766515

Epoch:    7/1000  Loss: 6.83409030675888

Epoch:    8/1000  Loss: 6.719126171761371

Epoch:    8/1000  Loss: 6.6382817673683165

Epoch:    9/1000  Loss: 6.5461004155747435

Epoch:    9/1000  Loss: 6.49627852678299

Epoch:   10/1000  Loss: 6.4200994166922065

Epoch:   10/1000  Loss: 6.382921674251556

Epoch:   11/1000  Loss: 6.321444399813388

Epoch:   11/1000  Loss: 6.285646097660065

Epoch:   12/1000  Loss: 6.2473670371035315

Epoch:   12/1000  Loss: 6.231439304351807

Epoch:   13/1000  Loss: 6.185988050826053

Epoch:   13/1000  Loss: 6.181914131641388

Epoch:   14/1000  Loss: 6.140003587844524

Epoch:   14/1000  Loss: 6.138145518302918

Epoch:   15/1000  Loss: 6.086116537134698

Epoch:   15/1000  Loss: 6.07407142162323

Epoch:   16/1000  Loss: 6.026988479938913

Epoch:   16/1000  Loss: 6.012018513679505

Epoch:   17/1000  Loss: 5.964695658582322

Epoch:   17/1000  Loss: 5.954612820148468

Epoch:   18/1000  Loss: 5.913186345201858

Epoch:   18/1000  Loss: 5.906388354301453

Epoch:   19/1000  Loss: 5.865938227227393

Epoch:   19/1000  Loss: 5.862877149581909

Epoch:   20/1000  Loss: 5.824143014055617

Epoch:   20/1000  Loss: 5.811884987354278

Epoch:   21/1000  Loss: 5.784598269361131

Epoch:   21/1000  Loss: 5.769442088603974

Epoch:   22/1000  Loss: 5.7477911847703

Epoch:   22/1000  Loss: 5.736540956497192

Epoch:   23/1000  Loss: 5.706269605109032

Epoch:   23/1000  Loss: 5.7067915320396425

Epoch:   24/1000  Loss: 5.681618929923849

Epoch:   24/1000  Loss: 5.679111993312835

Epoch:   25/1000  Loss: 5.648731278358622

Epoch:   25/1000  Loss: 5.645325281620026

Epoch:   26/1000  Loss: 5.6244516940827065

Epoch:   26/1000  Loss: 5.623613321781159

Epoch:   27/1000  Loss: 5.595018699321341

Epoch:   27/1000  Loss: 5.5985422372818

Epoch:   28/1000  Loss: 5.57308245111019

Epoch:   28/1000  Loss: 5.573099238872528

Epoch:   29/1000  Loss: 5.552005558825554

Epoch:   29/1000  Loss: 5.5490543031692505

Epoch:   30/1000  Loss: 5.529583760525318

Epoch:   30/1000  Loss: 5.526688816547394

Epoch:   31/1000  Loss: 5.510974969255163

Epoch:   31/1000  Loss: 5.507002253532409

Epoch:   32/1000  Loss: 5.487734908245979

Epoch:   32/1000  Loss: 5.491084780693054

Epoch:   33/1000  Loss: 5.473580922471716

Epoch:   33/1000  Loss: 5.474848222732544

Epoch:   34/1000  Loss: 5.4519493224773

Epoch:   34/1000  Loss: 5.4524915599823

Epoch:   35/1000  Loss: 5.432201294188804

Epoch:   35/1000  Loss: 5.439803228378296

Epoch:   36/1000  Loss: 5.4230491719347365

Epoch:   36/1000  Loss: 5.42236711025238

Epoch:   37/1000  Loss: 5.40101649913382

Epoch:   37/1000  Loss: 5.408276205062866

Epoch:   38/1000  Loss: 5.388944035388054

Epoch:   38/1000  Loss: 5.389273951053619

Epoch:   39/1000  Loss: 5.37465997655341

Epoch:   39/1000  Loss: 5.381905021667481

Epoch:   40/1000  Loss: 5.363420731970605

Epoch:   40/1000  Loss: 5.365838122367859

Epoch:   41/1000  Loss: 5.347412902750867

Epoch:   41/1000  Loss: 5.347951147556305

Epoch:   42/1000  Loss: 5.333122645033167

Epoch:   42/1000  Loss: 5.335942981243133

Epoch:   43/1000  Loss: 5.313704180210195

Epoch:   43/1000  Loss: 5.320814712047577

Epoch:   44/1000  Loss: 5.3021033449375885

Epoch:   44/1000  Loss: 5.313941838741303

Epoch:   45/1000  Loss: 5.294989648778388

Epoch:   45/1000  Loss: 5.297031085491181

Epoch:   46/1000  Loss: 5.2817573303871965

Epoch:   46/1000  Loss: 5.283247199058533

Epoch:   47/1000  Loss: 5.267198128395893

Epoch:   47/1000  Loss: 5.273597481250763

Epoch:   48/1000  Loss: 5.251425917605136

Epoch:   48/1000  Loss: 5.257729089260101

Epoch:   49/1000  Loss: 5.243436884372793

Epoch:   49/1000  Loss: 5.24878678560257

Epoch:   50/1000  Loss: 5.230296885713618

Epoch:   50/1000  Loss: 5.241787438392639

Epoch:   51/1000  Loss: 5.218157875791509

Epoch:   51/1000  Loss: 5.227270662784576

Epoch:   52/1000  Loss: 5.211244286882116

Epoch:   52/1000  Loss: 5.221967115402221

Epoch:   53/1000  Loss: 5.199943923950196

Epoch:   53/1000  Loss: 5.208795578479767

Epoch:   54/1000  Loss: 5.185664605079813

Epoch:   54/1000  Loss: 5.195363614559174

Epoch:   55/1000  Loss: 5.177210381690492

Epoch:   55/1000  Loss: 5.1867444443702695

Epoch:   56/1000  Loss: 5.168951618925054

Epoch:   56/1000  Loss: 5.175655870437622

Epoch:   57/1000  Loss: 5.159507950316084

Epoch:   57/1000  Loss: 5.170859954357147

Epoch:   58/1000  Loss: 5.147530245273671

Epoch:   58/1000  Loss: 5.163370225429535

Epoch:   59/1000  Loss: 5.1409970547290555

Epoch:   59/1000  Loss: 5.15608948469162

Epoch:   60/1000  Loss: 5.139308992345282

Epoch:   60/1000  Loss: 5.1436072444915775

Epoch:   61/1000  Loss: 5.124387207437069

Epoch:   61/1000  Loss: 5.136296217441559

Epoch:   62/1000  Loss: 5.11967967824733

Epoch:   62/1000  Loss: 5.131652941703797

Epoch:   63/1000  Loss: 5.106804650895139

Epoch:   63/1000  Loss: 5.1247581243515015

Epoch:   64/1000  Loss: 5.102573897990775

Epoch:   64/1000  Loss: 5.11302845954895

Epoch:   65/1000  Loss: 5.095948436412406

Epoch:   65/1000  Loss: 5.104613404273987

Epoch:   66/1000  Loss: 5.085404083576608

Epoch:   66/1000  Loss: 5.101729738712311

Epoch:   67/1000  Loss: 5.081991335686217

Epoch:   67/1000  Loss: 5.095168631076813

Epoch:   68/1000  Loss: 5.07601950219337

Epoch:   68/1000  Loss: 5.092648994922638

Epoch:   69/1000  Loss: 5.0676332899864684

Epoch:   69/1000  Loss: 5.0859035205841066

Epoch:   70/1000  Loss: 5.06410036492855

Epoch:   70/1000  Loss: 5.078396902084351

Epoch:   71/1000  Loss: 5.057145605696008

Epoch:   71/1000  Loss: 5.067893171310425

Epoch:   72/1000  Loss: 5.050471612240406

Epoch:   72/1000  Loss: 5.063775408267975

Epoch:   73/1000  Loss: 5.042662876210314

Epoch:   73/1000  Loss: 5.061268684864044

Epoch:   74/1000  Loss: 5.037676638745247

Epoch:   74/1000  Loss: 5.053265566825867

Epoch:   75/1000  Loss: 5.034887476170317

Epoch:   75/1000  Loss: 5.048784685134888

Epoch:   76/1000  Loss: 5.029196733109495

Epoch:   76/1000  Loss: 5.045136704444885

Epoch:   77/1000  Loss: 5.023619065386184

Epoch:   77/1000  Loss: 5.038173761367798

Epoch:   78/1000  Loss: 5.019431595092124

Epoch:   78/1000  Loss: 5.035609276294708

Epoch:   79/1000  Loss: 5.013838910041971

Epoch:   79/1000  Loss: 5.029077289104461

Epoch:   80/1000  Loss: 5.00801247130049

Epoch:   80/1000  Loss: 5.017686429023743

Epoch:   81/1000  Loss: 5.001466621236598

Epoch:   81/1000  Loss: 5.021345381736755

Epoch:   82/1000  Loss: 4.9960445099688595

Epoch:   82/1000  Loss: 5.0128890419006344

Epoch:   83/1000  Loss: 4.997759100731383

Epoch:   83/1000  Loss: 5.010842728614807

Epoch:   84/1000  Loss: 4.986338043212891

Epoch:   84/1000  Loss: 5.004144735336304

Epoch:   85/1000  Loss: 4.983236809994312

Epoch:   85/1000  Loss: 4.995813992023468

Epoch:   86/1000  Loss: 4.9792610980094745

Epoch:   86/1000  Loss: 4.995380365848542

Epoch:   87/1000  Loss: 4.979274609748353

Epoch:   87/1000  Loss: 4.991364643573761

Epoch:   88/1000  Loss: 4.973299048809295

Epoch:   88/1000  Loss: 4.984921061992646

Epoch:   89/1000  Loss: 4.96292745914865

Epoch:   89/1000  Loss: 4.981340837478638

Epoch:   90/1000  Loss: 4.961056191870507

Epoch:   90/1000  Loss: 4.9816199088096615

Epoch:   91/1000  Loss: 4.9598574131093125

Epoch:   91/1000  Loss: 4.974317717552185

Epoch:   92/1000  Loss: 4.9534374784916

Epoch:   92/1000  Loss: 4.969009356498718

Epoch:   93/1000  Loss: 4.948266776064609

Epoch:   93/1000  Loss: 4.965587148666382

Epoch:   94/1000  Loss: 4.944940802391539

Epoch:   94/1000  Loss: 4.959346661567688

Epoch:   95/1000  Loss: 4.940312369326327

Epoch:   95/1000  Loss: 4.958992207050324

Epoch:   96/1000  Loss: 4.936118963931469

Epoch:   96/1000  Loss: 4.949335548877716

Epoch:   97/1000  Loss: 4.93604360336953

Epoch:   97/1000  Loss: 4.948787691593171

Epoch:   98/1000  Loss: 4.930171096071284

Epoch:   98/1000  Loss: 4.943124990463257

Epoch:   99/1000  Loss: 4.921475353646786

Epoch:   99/1000  Loss: 4.938651809692383

Epoch:  100/1000  Loss: 4.922233916343527

Epoch:  100/1000  Loss: 4.943068854808807

Epoch:  101/1000  Loss: 4.915639816446507

Epoch:  101/1000  Loss: 4.929651412963867

Epoch:  102/1000  Loss: 4.914856050369587

Epoch:  102/1000  Loss: 4.9260815906524655

Epoch:  103/1000  Loss: 4.909216619045177

Epoch:  103/1000  Loss: 4.921838624477386

Epoch:  104/1000  Loss: 4.905169551930529

Epoch:  104/1000  Loss: 4.926055064201355

Epoch:  105/1000  Loss: 4.9008836036032815

Epoch:  105/1000  Loss: 4.917802314758301

Epoch:  106/1000  Loss: 4.8956652377514125

Epoch:  106/1000  Loss: 4.908680415153503

Epoch:  107/1000  Loss: 4.895350249270176

Epoch:  107/1000  Loss: 4.9095566892623905

Epoch:  108/1000  Loss: 4.890389144166987

Epoch:  108/1000  Loss: 4.9028742361068725

Epoch:  109/1000  Loss: 4.887334987965036

Epoch:  109/1000  Loss: 4.897494387626648

Epoch:  110/1000  Loss: 4.880909223759428

Epoch:  110/1000  Loss: 4.89737877368927

Epoch:  111/1000  Loss: 4.877719743201073

Epoch:  111/1000  Loss: 4.890427122116089

Epoch:  112/1000  Loss: 4.875072081545566

Epoch:  112/1000  Loss: 4.890444691181183

Epoch:  113/1000  Loss: 4.869699366549228

Epoch:  113/1000  Loss: 4.887332236766815

Epoch:  114/1000  Loss: 4.863553274438736

Epoch:  114/1000  Loss: 4.881323633193969

Epoch:  115/1000  Loss: 4.864619374782481

Epoch:  115/1000  Loss: 4.877804615497589

Epoch:  116/1000  Loss: 4.860612211836146

Epoch:  116/1000  Loss: 4.878655803203583

Epoch:  117/1000  Loss: 4.855532335727773

Epoch:  117/1000  Loss: 4.869343225955963

Epoch:  118/1000  Loss: 4.849144716465727

Epoch:  118/1000  Loss: 4.863438103199005

Epoch:  119/1000  Loss: 4.847431105755745

Epoch:  119/1000  Loss: 4.864275915622711

Epoch:  120/1000  Loss: 4.844732941972448

Epoch:  120/1000  Loss: 4.852984335422516

Epoch:  121/1000  Loss: 4.840894664602077

Epoch:  121/1000  Loss: 4.854276437759399

Epoch:  122/1000  Loss: 4.838355608189359

Epoch:  122/1000  Loss: 4.851533558368683

Epoch:  123/1000  Loss: 4.829491960241439

Epoch:  123/1000  Loss: 4.850315945148468

Epoch:  124/1000  Loss: 4.829670305454985

Epoch:  124/1000  Loss: 4.842209362983704

Epoch:  125/1000  Loss: 4.827190271336981

Epoch:  125/1000  Loss: 4.841266419887543

Epoch:  126/1000  Loss: 4.822213745117187

Epoch:  126/1000  Loss: 4.836829047203064

Epoch:  127/1000  Loss: 4.818780736720308

Epoch:  127/1000  Loss: 4.833251898288727

Epoch:  128/1000  Loss: 4.817078675615027

Epoch:  128/1000  Loss: 4.827058808803558

Epoch:  129/1000  Loss: 4.81453583088327

Epoch:  129/1000  Loss: 4.829089498519897

Epoch:  130/1000  Loss: 4.811278475091813

Epoch:  130/1000  Loss: 4.824033501148224

Epoch:  131/1000  Loss: 4.803498722644562

Epoch:  131/1000  Loss: 4.816400742530822

Epoch:  132/1000  Loss: 4.80183555724773

Epoch:  132/1000  Loss: 4.809485635757446

Epoch:  133/1000  Loss: 4.797475390738629

Epoch:  133/1000  Loss: 4.811865723133087

Epoch:  134/1000  Loss: 4.792220764971794

Epoch:  134/1000  Loss: 4.808233354091644

Epoch:  135/1000  Loss: 4.7923236197613654

Epoch:  135/1000  Loss: 4.807541117668152

Epoch:  136/1000  Loss: 4.790537432406811

Epoch:  136/1000  Loss: 4.802995212078095

Epoch:  137/1000  Loss: 4.784790972445873

Epoch:  137/1000  Loss: 4.800748045444489

Epoch:  138/1000  Loss: 4.78471229431477

Epoch:  138/1000  Loss: 4.79474534034729

Epoch:  139/1000  Loss: 4.777956100220376

Epoch:  139/1000  Loss: 4.790985424518585

Epoch:  140/1000  Loss: 4.776008713499029

Epoch:  140/1000  Loss: 4.785281281471253

Epoch:  141/1000  Loss: 4.769500265730188

Epoch:  141/1000  Loss: 4.788146846294403

Epoch:  142/1000  Loss: 4.768508793445344

Epoch:  142/1000  Loss: 4.783086009025574

Epoch:  143/1000  Loss: 4.769991209151897

Epoch:  143/1000  Loss: 4.779928488731384

Epoch:  144/1000  Loss: 4.757765556903595

Epoch:  144/1000  Loss: 4.773344576358795

Epoch:  145/1000  Loss: 4.760631106762176

Epoch:  145/1000  Loss: 4.775544695854187

Epoch:  146/1000  Loss: 4.751887731349215

Epoch:  146/1000  Loss: 4.769081308841705

Epoch:  147/1000  Loss: 4.750912209774586

Epoch:  147/1000  Loss: 4.758831808567047

Epoch:  148/1000  Loss: 4.752566674415101

Epoch:  148/1000  Loss: 4.759856657981873

Epoch:  149/1000  Loss: 4.747798879095849

Epoch:  149/1000  Loss: 4.761043162345886

Epoch:  150/1000  Loss: 4.74115133082613

Epoch:  150/1000  Loss: 4.755136396884918

Epoch:  151/1000  Loss: 4.7391768435214425

Epoch:  151/1000  Loss: 4.753643298149109

Epoch:  152/1000  Loss: 4.735864570293021

Epoch:  152/1000  Loss: 4.74634596824646

Epoch:  153/1000  Loss: 4.736875931760098

Epoch:  153/1000  Loss: 4.751075298786163

Epoch:  154/1000  Loss: 4.732921080893658

Epoch:  154/1000  Loss: 4.741616458892822

Epoch:  155/1000  Loss: 4.727365242166722

Epoch:  155/1000  Loss: 4.73808792591095

Epoch:  156/1000  Loss: 4.724973006958657

Epoch:  156/1000  Loss: 4.739068715572357

Epoch:  157/1000  Loss: 4.724469915349433

Epoch:  157/1000  Loss: 4.733215334415436

Epoch:  158/1000  Loss: 4.719295519970833

Epoch:  158/1000  Loss: 4.732938818931579

Epoch:  159/1000  Loss: 4.724960239897383

Epoch:  159/1000  Loss: 4.73022558927536

Epoch:  160/1000  Loss: 4.717325399277058

Epoch:  160/1000  Loss: 4.727521226406098

Epoch:  161/1000  Loss: 4.709485096627094

Epoch:  161/1000  Loss: 4.724322180747986

Epoch:  162/1000  Loss: 4.715486772009667

Epoch:  162/1000  Loss: 4.720167198181152

Epoch:  163/1000  Loss: 4.7098666333137675

Epoch:  163/1000  Loss: 4.724021632671356

Epoch:  164/1000  Loss: 4.70411319732666

Epoch:  164/1000  Loss: 4.719188916683197

Epoch:  165/1000  Loss: 4.702071289305992

Epoch:  165/1000  Loss: 4.7198585247993465

Epoch:  166/1000  Loss: 4.700450182975607

Epoch:  166/1000  Loss: 4.715302822589874

Epoch:  167/1000  Loss: 4.695840212639342

Epoch:  167/1000  Loss: 4.70498104095459

Epoch:  168/1000  Loss: 4.693888479598025

Epoch:  168/1000  Loss: 4.708552634716034

Epoch:  169/1000  Loss: 4.69277360794392

Epoch:  169/1000  Loss: 4.703921070098877

Epoch:  170/1000  Loss: 4.689759175320889

Epoch:  170/1000  Loss: 4.704201385974884

Epoch:  171/1000  Loss: 4.694680999187713

Epoch:  171/1000  Loss: 4.702398402690887

Epoch:  172/1000  Loss: 4.686175127232328

Epoch:  172/1000  Loss: 4.701786274909973

Epoch:  173/1000  Loss: 4.68960279708213

Epoch:  173/1000  Loss: 4.69711373090744

Epoch:  174/1000  Loss: 4.68532927695741

Epoch:  174/1000  Loss: 4.698308038711548

Epoch:  175/1000  Loss: 4.677068008260524

Epoch:  175/1000  Loss: 4.692825877666474

Epoch:  176/1000  Loss: 4.676106032919376

Epoch:  176/1000  Loss: 4.693077805042267

Epoch:  177/1000  Loss: 4.6793475374262385

Epoch:  177/1000  Loss: 4.685717604160309

Epoch:  178/1000  Loss: 4.6742251112106

Epoch:  178/1000  Loss: 4.686051166057586

Epoch:  179/1000  Loss: 4.671250355497319

Epoch:  179/1000  Loss: 4.686733820438385

Epoch:  180/1000  Loss: 4.6692580831811785

Epoch:  180/1000  Loss: 4.6825828671455385

Epoch:  181/1000  Loss: 4.666521340228142

Epoch:  181/1000  Loss: 4.676208111047745

Epoch:  182/1000  Loss: 4.66355533802763

Epoch:  182/1000  Loss: 4.675919446945191

Epoch:  183/1000  Loss: 4.661235074794039

Epoch:  183/1000  Loss: 4.680389566421509

Epoch:  184/1000  Loss: 4.660018724076291

Epoch:  184/1000  Loss: 4.676980307102204

Epoch:  185/1000  Loss: 4.65904612439744

Epoch:  185/1000  Loss: 4.6739366602897645

Epoch:  186/1000  Loss: 4.658058600730084

Epoch:  186/1000  Loss: 4.66798273563385

Epoch:  187/1000  Loss: 4.6541383783868016

Epoch:  187/1000  Loss: 4.668837895393372

Epoch:  188/1000  Loss: 4.65546569012581

Epoch:  188/1000  Loss: 4.668104037046432

Epoch:  189/1000  Loss: 4.654198569439827

Epoch:  189/1000  Loss: 4.663062660694123

Epoch:  190/1000  Loss: 4.6491343518520925

Epoch:  190/1000  Loss: 4.664548952579498

Epoch:  191/1000  Loss: 4.648885820267048

Epoch:  191/1000  Loss: 4.658499553203582

Epoch:  192/1000  Loss: 4.644682230847947

Epoch:  192/1000  Loss: 4.657595139741898

Epoch:  193/1000  Loss: 4.641193174808583

Epoch:  193/1000  Loss: 4.654971945285797

Epoch:  194/1000  Loss: 4.638999159792636

Epoch:  194/1000  Loss: 4.654784479141235

Epoch:  195/1000  Loss: 4.641525100139861

Epoch:  195/1000  Loss: 4.653811945915222

Epoch:  196/1000  Loss: 4.639534002669314

Epoch:  196/1000  Loss: 4.653953876495361

Epoch:  197/1000  Loss: 4.633316624418218

Epoch:  197/1000  Loss: 4.652689502239228

Epoch:  198/1000  Loss: 4.632338895189001

Epoch:  198/1000  Loss: 4.651049538850784

Epoch:  199/1000  Loss: 4.63409705263503

Epoch:  199/1000  Loss: 4.648226745128632

Epoch:  200/1000  Loss: 4.63485803604126

Epoch:  200/1000  Loss: 4.646501636505127

Epoch:  201/1000  Loss: 4.63280399606583

Epoch:  201/1000  Loss: 4.646165707111359

Epoch:  202/1000  Loss: 4.627995422038627

Epoch:  202/1000  Loss: 4.6468836259841915

Epoch:  203/1000  Loss: 4.629906514350404

Epoch:  203/1000  Loss: 4.638774724006653

Epoch:  204/1000  Loss: 4.622320820422883

Epoch:  204/1000  Loss: 4.6380987703800205

Epoch:  205/1000  Loss: 4.624807942167242

Epoch:  205/1000  Loss: 4.635461778640747

Epoch:  206/1000  Loss: 4.621564976712491

Epoch:  206/1000  Loss: 4.634307783842087

Epoch:  207/1000  Loss: 4.623361910150407

Epoch:  207/1000  Loss: 4.637787201404572

Epoch:  208/1000  Loss: 4.6179266503516665

Epoch:  208/1000  Loss: 4.635496079921722

Epoch:  209/1000  Loss: 4.620413205978719

Epoch:  209/1000  Loss: 4.632644636631012

Epoch:  210/1000  Loss: 4.612802996533983

Epoch:  210/1000  Loss: 4.631238709688187

Epoch:  211/1000  Loss: 4.616107094541509

Epoch:  211/1000  Loss: 4.6277258324623105

Epoch:  212/1000  Loss: 4.612602935953343

Epoch:  212/1000  Loss: 4.6267982506752015

Epoch:  213/1000  Loss: 4.615447123507236

Epoch:  213/1000  Loss: 4.623373920917511

Epoch:  214/1000  Loss: 4.613193840676166

Epoch:  214/1000  Loss: 4.6233182668685915

Epoch:  215/1000  Loss: 4.612836884437724

Epoch:  215/1000  Loss: 4.627621986865997

Epoch:  216/1000  Loss: 4.615610987074832

Epoch:  216/1000  Loss: 4.619676609039306

Epoch:  217/1000  Loss: 4.605588596425158

Epoch:  217/1000  Loss: 4.62558422923088

Epoch:  218/1000  Loss: 4.604342215111915

Epoch:  218/1000  Loss: 4.619706842899323

Epoch:  219/1000  Loss: 4.606760645927267

Epoch:  219/1000  Loss: 4.615903067588806

Epoch:  220/1000  Loss: 4.601741792800579

Epoch:  220/1000  Loss: 4.617821519374847

Epoch:  221/1000  Loss: 4.605068056634132

Epoch:  221/1000  Loss: 4.612829532623291

Epoch:  222/1000  Loss: 4.599611436559798

Epoch:  222/1000  Loss: 4.616146117448807

Epoch:  223/1000  Loss: 4.601728453534715

Epoch:  223/1000  Loss: 4.611442908048629

Epoch:  224/1000  Loss: 4.598309961278388

Epoch:  224/1000  Loss: 4.61462331533432

Epoch:  225/1000  Loss: 4.595543195846233

Epoch:  225/1000  Loss: 4.608962140083313

Epoch:  226/1000  Loss: 4.597107120270425

Epoch:  226/1000  Loss: 4.613747740983963

Epoch:  227/1000  Loss: 4.5974727437851275

Epoch:  227/1000  Loss: 4.607047513723374

Epoch:  228/1000  Loss: 4.598646314093407

Epoch:  228/1000  Loss: 4.608953216075897

Epoch:  229/1000  Loss: 4.594929179739445

Epoch:  229/1000  Loss: 4.60974179983139

Epoch:  230/1000  Loss: 4.592127891297036

Epoch:  230/1000  Loss: 4.60081144452095

Epoch:  231/1000  Loss: 4.590990677285702

Epoch:  231/1000  Loss: 4.604928889274597

Epoch:  232/1000  Loss: 4.5909054350345695

Epoch:  232/1000  Loss: 4.600671803951263

Epoch:  233/1000  Loss: 4.590114794386194

Epoch:  233/1000  Loss: 4.607625412940979

Epoch:  234/1000  Loss: 4.587000676418873

Epoch:  234/1000  Loss: 4.600400766134262

Epoch:  235/1000  Loss: 4.582535037588566

Epoch:  235/1000  Loss: 4.59955979347229

Epoch:  236/1000  Loss: 4.585865929786195

Epoch:  236/1000  Loss: 4.591457452774048

Epoch:  237/1000  Loss: 4.582025832318245

Epoch:  237/1000  Loss: 4.597228336334228

Epoch:  238/1000  Loss: 4.582211220518071

Epoch:  238/1000  Loss: 4.597741633653641

Epoch:  239/1000  Loss: 4.580888232778996

Epoch:  239/1000  Loss: 4.59554727435112

Epoch:  240/1000  Loss: 4.584377152869042

Epoch:  240/1000  Loss: 4.590223959684372

Epoch:  241/1000  Loss: 4.579143220820326

Epoch:  241/1000  Loss: 4.597822035551071

Epoch:  242/1000  Loss: 4.575739402973905

Epoch:  242/1000  Loss: 4.589013574123382

Epoch:  243/1000  Loss: 4.573634041116593

Epoch:  243/1000  Loss: 4.592701036930084

Epoch:  244/1000  Loss: 4.577726177459067

Epoch:  244/1000  Loss: 4.585890315771103

Epoch:  245/1000  Loss: 4.578324423445032

Epoch:  245/1000  Loss: 4.587801488637925

Epoch:  246/1000  Loss: 4.572556892354438

Epoch:  246/1000  Loss: 4.585485188961029

Epoch:  247/1000  Loss: 4.573163840111266

Epoch:  247/1000  Loss: 4.588594299554825

Epoch:  248/1000  Loss: 4.572335561792901

Epoch:  248/1000  Loss: 4.581584879159927

Epoch:  249/1000  Loss: 4.573556883791659

Epoch:  249/1000  Loss: 4.587537227869034

Epoch:  250/1000  Loss: 4.5671609482866655

Epoch:  250/1000  Loss: 4.585157270431519

Epoch:  251/1000  Loss: 4.567622804641724

Epoch:  251/1000  Loss: 4.588208873271942

Epoch:  252/1000  Loss: 4.565257722773451

Epoch:  252/1000  Loss: 4.577914097309113

Epoch:  253/1000  Loss: 4.564688138758883

Epoch:  253/1000  Loss: 4.583254919052124

Epoch:  254/1000  Loss: 4.571436096759553

Epoch:  254/1000  Loss: 4.582348648309708

Epoch:  255/1000  Loss: 4.5672401002112855

Epoch:  255/1000  Loss: 4.581561757326126

Epoch:  256/1000  Loss: 4.565028068867136

Epoch:  256/1000  Loss: 4.57830078125

Epoch:  257/1000  Loss: 4.565503688568764

Epoch:  257/1000  Loss: 4.581364688873291

Epoch:  258/1000  Loss: 4.565618456170914

Epoch:  258/1000  Loss: 4.5853381502628325

Epoch:  259/1000  Loss: 4.560661667965828

Epoch:  259/1000  Loss: 4.579946489334106

Epoch:  260/1000  Loss: 4.561387234545768

Epoch:  260/1000  Loss: 4.5774806416034695

Epoch:  261/1000  Loss: 4.560528783595308

Epoch:  261/1000  Loss: 4.568893623352051

Epoch:  262/1000  Loss: 4.560747818236655

Epoch:  262/1000  Loss: 4.5717312943935395

Epoch:  263/1000  Loss: 4.555987919137833

Epoch:  263/1000  Loss: 4.574153562784195

Epoch:  264/1000  Loss: 4.559420627228757

Epoch:  264/1000  Loss: 4.576294076442719

Epoch:  265/1000  Loss: 4.555492245897334

Epoch:  265/1000  Loss: 4.568827240467072

Epoch:  266/1000  Loss: 4.5568313902996955

Epoch:  266/1000  Loss: 4.569964150190353

Epoch:  267/1000  Loss: 4.5533093949581716

Epoch:  267/1000  Loss: 4.567046446800232

Epoch:  268/1000  Loss: 4.551821438809658

Epoch:  268/1000  Loss: 4.570313972234726

Epoch:  269/1000  Loss: 4.554730707533816

Epoch:  269/1000  Loss: 4.56776249051094

Epoch:  270/1000  Loss: 4.550431096300166

Epoch:  270/1000  Loss: 4.571247066259384

Epoch:  271/1000  Loss: 4.550730847297831

Epoch:  271/1000  Loss: 4.563138915300369

Epoch:  272/1000  Loss: 4.552076885548043

Epoch:  272/1000  Loss: 4.561448916196823

Epoch:  273/1000  Loss: 4.547343047121738

Epoch:  273/1000  Loss: 4.563844262361527

Epoch:  274/1000  Loss: 4.5495325524756245

Epoch:  274/1000  Loss: 4.560360033512115

Epoch:  275/1000  Loss: 4.550210994355222

Epoch:  275/1000  Loss: 4.556812230348587

Epoch:  276/1000  Loss: 4.546426322612357

Epoch:  276/1000  Loss: 4.564365261793137

Epoch:  277/1000  Loss: 4.541365436797446

Epoch:  277/1000  Loss: 4.555522385835648

Epoch:  278/1000  Loss: 4.5506669927150645

Epoch:  278/1000  Loss: 4.559349858760834

Epoch:  279/1000  Loss: 4.542751618649097

Epoch:  279/1000  Loss: 4.558951723575592

Epoch:  280/1000  Loss: 4.543272550055321

Epoch:  280/1000  Loss: 4.558971141576767

Epoch:  281/1000  Loss: 4.542251803012604

Epoch:  281/1000  Loss: 4.553618009090424

Epoch:  282/1000  Loss: 4.54050541025527

Epoch:  282/1000  Loss: 4.558592772483825

Epoch:  283/1000  Loss: 4.543247701766643

Epoch:  283/1000  Loss: 4.557927803993225

Epoch:  284/1000  Loss: 4.542316198348999

Epoch:  284/1000  Loss: 4.551651643514633

Epoch:  285/1000  Loss: 4.53824204891286

Epoch:  285/1000  Loss: 4.554040269851685

Epoch:  286/1000  Loss: 4.534088288976791

Epoch:  286/1000  Loss: 4.550059700012207

Epoch:  287/1000  Loss: 4.531885511317151

Epoch:  287/1000  Loss: 4.548173687458038

Epoch:  288/1000  Loss: 4.537433839351573

Epoch:  288/1000  Loss: 4.549674181938172

Epoch:  289/1000  Loss: 4.533675679754704

Epoch:  289/1000  Loss: 4.550656433105469

Epoch:  290/1000  Loss: 4.530772259894838

Epoch:  290/1000  Loss: 4.53951824426651

Epoch:  291/1000  Loss: 4.534897568885317

Epoch:  291/1000  Loss: 4.542259608507156

Epoch:  292/1000  Loss: 4.5322213203349015

Epoch:  292/1000  Loss: 4.548176305294037

Epoch:  293/1000  Loss: 4.527793367872847

Epoch:  293/1000  Loss: 4.542443614006043

Epoch:  294/1000  Loss: 4.52877098144369

Epoch:  294/1000  Loss: 4.542702199220657

Epoch:  295/1000  Loss: 4.525372160241959

Epoch:  295/1000  Loss: 4.546953125

Epoch:  296/1000  Loss: 4.527573342018939

Epoch:  296/1000  Loss: 4.5424331700801845

Epoch:  297/1000  Loss: 4.524502005475632

Epoch:  297/1000  Loss: 4.542256280183792

Epoch:  298/1000  Loss: 4.528161038743689

Epoch:  298/1000  Loss: 4.539916479587555

Epoch:  299/1000  Loss: 4.528588816460143

Epoch:  299/1000  Loss: 4.537885998487472

Epoch:  300/1000  Loss: 4.526459466650131

Epoch:  300/1000  Loss: 4.53800057888031

Epoch:  301/1000  Loss: 4.5258452090811225

Epoch:  301/1000  Loss: 4.5355172073841095

Epoch:  302/1000  Loss: 4.526781708128909

Epoch:  302/1000  Loss: 4.535528509616852

Epoch:  303/1000  Loss: 4.519971172860328

Epoch:  303/1000  Loss: 4.530818580389023

Epoch:  304/1000  Loss: 4.52092150830208

Epoch:  304/1000  Loss: 4.535367940664291

Epoch:  305/1000  Loss: 4.522167270741564

Epoch:  305/1000  Loss: 4.537889581918717

Epoch:  306/1000  Loss: 4.515318364285408

Epoch:  306/1000  Loss: 4.53221283197403

Epoch:  307/1000  Loss: 4.518088801363681

Epoch:  307/1000  Loss: 4.534174554347992

Epoch:  308/1000  Loss: 4.520097127873846

Epoch:  308/1000  Loss: 4.535928041934967

Epoch:  309/1000  Loss: 4.518616231958917

Epoch:  309/1000  Loss: 4.532360110282898

Epoch:  310/1000  Loss: 4.5148754748892275

Epoch:  310/1000  Loss: 4.5305598139762875

Epoch:  311/1000  Loss: 4.5154496608896455

Epoch:  311/1000  Loss: 4.528603037595749

Epoch:  312/1000  Loss: 4.512443921921101

Epoch:  312/1000  Loss: 4.533642460107803

Epoch:  313/1000  Loss: 4.511468730074294

Epoch:  313/1000  Loss: 4.531410471200943

Epoch:  314/1000  Loss: 4.516494757063845

Epoch:  314/1000  Loss: 4.5245118331909175

Epoch:  315/1000  Loss: 4.512877107173838

Epoch:  315/1000  Loss: 4.522727110385895

Epoch:  316/1000  Loss: 4.5100991117193345

Epoch:  316/1000  Loss: 4.51959233045578

Epoch:  317/1000  Loss: 4.509110531908401

Epoch:  317/1000  Loss: 4.529148195981979

Epoch:  318/1000  Loss: 4.507430689385597

Epoch:  318/1000  Loss: 4.525657207965851

Epoch:  319/1000  Loss: 4.510251190307293

Epoch:  319/1000  Loss: 4.524448544979095

Epoch:  320/1000  Loss: 4.509676384418569

Epoch:  320/1000  Loss: 4.521691784858704

Epoch:  321/1000  Loss: 4.507670427890534

Epoch:  321/1000  Loss: 4.517006585597992

Epoch:  322/1000  Loss: 4.50152333442201

Epoch:  322/1000  Loss: 4.519984409809113

Epoch:  323/1000  Loss: 4.499128999101354

Epoch:  323/1000  Loss: 4.521433180570602

Epoch:  324/1000  Loss: 4.500164841591044

Epoch:  324/1000  Loss: 4.519676648378372

Epoch:  325/1000  Loss: 4.504084842763048

Epoch:  325/1000  Loss: 4.519579141139984

Epoch:  326/1000  Loss: 4.5050225957911065

Epoch:  326/1000  Loss: 4.516716293096542

Epoch:  327/1000  Loss: 4.497845082587384

Epoch:  327/1000  Loss: 4.519032530784607

Epoch:  328/1000  Loss: 4.502791226163824

Epoch:  328/1000  Loss: 4.51340106010437

Epoch:  329/1000  Loss: 4.498638933262926

Epoch:  329/1000  Loss: 4.518962475061417

Epoch:  330/1000  Loss: 4.499233491370019

Epoch:  330/1000  Loss: 4.513205726146698

Epoch:  331/1000  Loss: 4.495802076826704

Epoch:  331/1000  Loss: 4.510101536512375

Epoch:  332/1000  Loss: 4.498765896736307

Epoch:  332/1000  Loss: 4.5189155888557435

Epoch:  333/1000  Loss: 4.493966874670475

Epoch:  333/1000  Loss: 4.512685471773148

Epoch:  334/1000  Loss: 4.498791170120239

Epoch:  334/1000  Loss: 4.5116565227508545

Epoch:  335/1000  Loss: 4.497399691317944

Epoch:  335/1000  Loss: 4.5054533839225765

Epoch:  336/1000  Loss: 4.500601140488969

Epoch:  336/1000  Loss: 4.507298703193665

Epoch:  337/1000  Loss: 4.49541503216358

Epoch:  337/1000  Loss: 4.510577653646469

Epoch:  338/1000  Loss: 4.500840428534975

Epoch:  338/1000  Loss: 4.507297880649567

Epoch:  339/1000  Loss: 4.493356231932944

Epoch:  339/1000  Loss: 4.5159208512306215

Epoch:  340/1000  Loss: 4.498519338445461

Epoch:  340/1000  Loss: 4.5100532817840575

Epoch:  341/1000  Loss: 4.49556955479561

Epoch:  341/1000  Loss: 4.510744017362595

Epoch:  342/1000  Loss: 4.492388376276544

Epoch:  342/1000  Loss: 4.506069808006287

Epoch:  343/1000  Loss: 4.4922522930388755

Epoch:  343/1000  Loss: 4.5094440865516665

Epoch:  344/1000  Loss: 4.495828788838488

Epoch:  344/1000  Loss: 4.502783620357514

Epoch:  345/1000  Loss: 4.494197206294283

Epoch:  345/1000  Loss: 4.503128988742828

Epoch:  346/1000  Loss: 4.481801607253703

Epoch:  346/1000  Loss: 4.502311943769455

Epoch:  347/1000  Loss: 4.487764513746221

Epoch:  347/1000  Loss: 4.506434504985809

Epoch:  348/1000  Loss: 4.4865093352946825

Epoch:  348/1000  Loss: 4.506291998624802

Epoch:  349/1000  Loss: 4.494134565109902

Epoch:  349/1000  Loss: 4.501052560806275

Epoch:  350/1000  Loss: 4.486057883120598

Epoch:  350/1000  Loss: 4.494926081895828

Epoch:  351/1000  Loss: 4.486189034644593

Epoch:  351/1000  Loss: 4.503977726697922

Epoch:  352/1000  Loss: 4.4909573494119845

Epoch:  352/1000  Loss: 4.505056976079941

Epoch:  353/1000  Loss: 4.484490367199513

Epoch:  353/1000  Loss: 4.5064800679683685

Epoch:  354/1000  Loss: 4.481976786065609

Epoch:  354/1000  Loss: 4.496106131076813

Epoch:  355/1000  Loss: 4.485961447370813

Epoch:  355/1000  Loss: 4.499924221038818

Epoch:  356/1000  Loss: 4.478560442620135

Epoch:  356/1000  Loss: 4.501485755443573

Epoch:  357/1000  Loss: 4.484008524265695

Epoch:  357/1000  Loss: 4.494251052141189

Epoch:  358/1000  Loss: 4.481736084755431

Epoch:  358/1000  Loss: 4.496644186973572

Epoch:  359/1000  Loss: 4.482815991056726

Epoch:  359/1000  Loss: 4.500034584999084

Epoch:  360/1000  Loss: 4.481769055508552

Epoch:  360/1000  Loss: 4.4896273922920225

Epoch:  361/1000  Loss: 4.478965169825452

Epoch:  361/1000  Loss: 4.493142169713974

Epoch:  362/1000  Loss: 4.476034259796142

Epoch:  362/1000  Loss: 4.493342106342316

Epoch:  363/1000  Loss: 4.476725927312323

Epoch:  363/1000  Loss: 4.495808049440384

Epoch:  364/1000  Loss: 4.473535372348542

Epoch:  364/1000  Loss: 4.494495573043824

Epoch:  365/1000  Loss: 4.475683356345968

Epoch:  365/1000  Loss: 4.490858099460602

Epoch:  366/1000  Loss: 4.476286550278359

Epoch:  366/1000  Loss: 4.4866251170635225

Epoch:  367/1000  Loss: 4.4766917735972305

Epoch:  367/1000  Loss: 4.487699378728866

Epoch:  368/1000  Loss: 4.471221640769471

Epoch:  368/1000  Loss: 4.492968828678131

Epoch:  369/1000  Loss: 4.469426346839742

Epoch:  369/1000  Loss: 4.490237044095993

Epoch:  370/1000  Loss: 4.474299910727968

Epoch:  370/1000  Loss: 4.486133354902267

Epoch:  371/1000  Loss: 4.471430633423176

Epoch:  371/1000  Loss: 4.487852298021316

Epoch:  372/1000  Loss: 4.471416871091153

Epoch:  372/1000  Loss: 4.487612127065659

Epoch:  373/1000  Loss: 4.473950190239765

Epoch:  373/1000  Loss: 4.483992968797684

Epoch:  374/1000  Loss: 4.468910760067879

Epoch:  374/1000  Loss: 4.489228211641311

Epoch:  375/1000  Loss: 4.472297742518973

Epoch:  375/1000  Loss: 4.486207745075226

Epoch:  376/1000  Loss: 4.466654380838921

Epoch:  376/1000  Loss: 4.483179323673248

Epoch:  377/1000  Loss: 4.467654982018978

Epoch:  377/1000  Loss: 4.486126909255981

Epoch:  378/1000  Loss: 4.46973488178659

Epoch:  378/1000  Loss: 4.48861470580101

Epoch:  379/1000  Loss: 4.465119187375333

Epoch:  379/1000  Loss: 4.480044251680374

Epoch:  380/1000  Loss: 4.465836715698242

Epoch:  380/1000  Loss: 4.481546918153763

Epoch:  381/1000  Loss: 4.467817033605373

Epoch:  381/1000  Loss: 4.479252301454544

Epoch:  382/1000  Loss: 4.46352194725199

Epoch:  382/1000  Loss: 4.489015257358551

Epoch:  383/1000  Loss: 4.46534346012359

Epoch:  383/1000  Loss: 4.487105865478515

Epoch:  384/1000  Loss: 4.461186350152848

Epoch:  384/1000  Loss: 4.480562608242035

Epoch:  385/1000  Loss: 4.466663469152247

Epoch:  385/1000  Loss: 4.4813993966579435

Epoch:  386/1000  Loss: 4.466244205515435

Epoch:  386/1000  Loss: 4.475033702850342

Epoch:  387/1000  Loss: 4.463886915369237

Epoch:  387/1000  Loss: 4.475060620307922

Epoch:  388/1000  Loss: 4.458168523869616

Epoch:  388/1000  Loss: 4.477310931682586

Epoch:  389/1000  Loss: 4.46323941920666

Epoch:  389/1000  Loss: 4.475747730731964

Epoch:  390/1000  Loss: 4.455548115994068

Epoch:  390/1000  Loss: 4.473899348974228

Epoch:  391/1000  Loss: 4.457909445052452

Epoch:  391/1000  Loss: 4.471949026584626

Epoch:  392/1000  Loss: 4.464323496311269

Epoch:  392/1000  Loss: 4.474196871519089

Epoch:  393/1000  Loss: 4.4582765051659115

Epoch:  393/1000  Loss: 4.467185311317444

Epoch:  394/1000  Loss: 4.456499995576574

Epoch:  394/1000  Loss: 4.471625615358352

Epoch:  395/1000  Loss: 4.457803687643498

Epoch:  395/1000  Loss: 4.473035839796066

Epoch:  396/1000  Loss: 4.45303699209335

Epoch:  396/1000  Loss: 4.471644928455353

Epoch:  397/1000  Loss: 4.458475374668202

Epoch:  397/1000  Loss: 4.476087625026703

Epoch:  398/1000  Loss: 4.456630256328177

Epoch:  398/1000  Loss: 4.472941787242889

Epoch:  399/1000  Loss: 4.45437771513107

Epoch:  399/1000  Loss: 4.472926313877106

Epoch:  400/1000  Loss: 4.453722125925916

Epoch:  400/1000  Loss: 4.467969620227814

Epoch:  401/1000  Loss: 4.4552466615717465

Epoch:  401/1000  Loss: 4.469546455144882

Epoch:  402/1000  Loss: 4.453876667834343

Epoch:  402/1000  Loss: 4.472671791315078

Epoch:  403/1000  Loss: 4.4530604636415525

Epoch:  403/1000  Loss: 4.470817294120788

Epoch:  404/1000  Loss: 4.453278412717454

Epoch:  404/1000  Loss: 4.464456930160522

Epoch:  405/1000  Loss: 4.44952637083987

Epoch:  405/1000  Loss: 4.469934054613113

Epoch:  406/1000  Loss: 4.454374405678283

Epoch:  406/1000  Loss: 4.460051993131637

Epoch:  407/1000  Loss: 4.456015414380013

Epoch:  407/1000  Loss: 4.468625285625458

Epoch:  408/1000  Loss: 4.45165575311539

Epoch:  408/1000  Loss: 4.468776944875717

Epoch:  409/1000  Loss: 4.451945604162013

Epoch:  409/1000  Loss: 4.460498311519623

Epoch:  410/1000  Loss: 4.445530543428786

Epoch:  410/1000  Loss: 4.468130049705505

Epoch:  411/1000  Loss: 4.446539035756537

Epoch:  411/1000  Loss: 4.467711261510849

Epoch:  412/1000  Loss: 4.4481163826394585

Epoch:  412/1000  Loss: 4.4670010650157925

Epoch:  413/1000  Loss: 4.441191472398474

Epoch:  413/1000  Loss: 4.467908067703247

Epoch:  414/1000  Loss: 4.445451801381212

Epoch:  414/1000  Loss: 4.46131711602211

Epoch:  415/1000  Loss: 4.439109278739767

Epoch:  415/1000  Loss: 4.461442470550537

Epoch:  416/1000  Loss: 4.445085485945357

Epoch:  416/1000  Loss: 4.458602312803269

Epoch:  417/1000  Loss: 4.442510804724186

Epoch:  417/1000  Loss: 4.456765315532684

Epoch:  418/1000  Loss: 4.43888166914595

Epoch:  418/1000  Loss: 4.4601145195961

Epoch:  419/1000  Loss: 4.441211698410359

Epoch:  419/1000  Loss: 4.458064086437226

Epoch:  420/1000  Loss: 4.446143644414049

Epoch:  420/1000  Loss: 4.455958849191665

Epoch:  421/1000  Loss: 4.441535127923844

Epoch:  421/1000  Loss: 4.461740372180938

Epoch:  422/1000  Loss: 4.439132276494452

Epoch:  422/1000  Loss: 4.460000905990601

Epoch:  423/1000  Loss: 4.440425928602827

Epoch:  423/1000  Loss: 4.4525877428054805

Epoch:  424/1000  Loss: 4.438563469622998

Epoch:  424/1000  Loss: 4.455969896316528

Epoch:  425/1000  Loss: 4.436976124377961

Epoch:  425/1000  Loss: 4.45352569937706

Epoch:  426/1000  Loss: 4.440209864555521

Epoch:  426/1000  Loss: 4.458895115852356

Epoch:  427/1000  Loss: 4.439935602025783

Epoch:  427/1000  Loss: 4.45577795624733

Epoch:  428/1000  Loss: 4.435252940401118

Epoch:  428/1000  Loss: 4.4583794569969175

Epoch:  429/1000  Loss: 4.436566459371688

Epoch:  429/1000  Loss: 4.4519711995124815

Epoch:  430/1000  Loss: 4.436691673765791

Epoch:  430/1000  Loss: 4.451440732479096

Epoch:  431/1000  Loss: 4.433077190277424

Epoch:  431/1000  Loss: 4.460903050899506

Epoch:  432/1000  Loss: 4.4318645619331525

Epoch:  432/1000  Loss: 4.452346005439758

Epoch:  433/1000  Loss: 4.433882737667003

Epoch:  433/1000  Loss: 4.454216189384461

Epoch:  434/1000  Loss: 4.431362112532271

Epoch:  434/1000  Loss: 4.45132206439972

Epoch:  435/1000  Loss: 4.430985054056695

Epoch:  435/1000  Loss: 4.448263001441956

Epoch:  436/1000  Loss: 4.430635876351214

Epoch:  436/1000  Loss: 4.4563911235332485

Epoch:  437/1000  Loss: 4.432988714664541

Epoch:  437/1000  Loss: 4.448329458236694

Epoch:  438/1000  Loss: 4.431585509726342

Epoch:  438/1000  Loss: 4.450425363779068

Epoch:  439/1000  Loss: 4.4310840870471715

Epoch:  439/1000  Loss: 4.4513396525383

Epoch:  440/1000  Loss: 4.428440924908252

Epoch:  440/1000  Loss: 4.442906289100647

Epoch:  441/1000  Loss: 4.429356208760687

Epoch:  441/1000  Loss: 4.444739981889724

Epoch:  442/1000  Loss: 4.430729526154538

Epoch:  442/1000  Loss: 4.442616084814071

Epoch:  443/1000  Loss: 4.429839421333151

Epoch:  443/1000  Loss: 4.442335873842239

Epoch:  444/1000  Loss: 4.424982222090376

Epoch:  444/1000  Loss: 4.447579616308213

Epoch:  445/1000  Loss: 4.42596801595485

Epoch:  445/1000  Loss: 4.447371550798416

Epoch:  446/1000  Loss: 4.431181514009516

Epoch:  446/1000  Loss: 4.438666062355042

Epoch:  447/1000  Loss: 4.429087694655073

Epoch:  447/1000  Loss: 4.435044828653336

Epoch:  448/1000  Loss: 4.423271773723846

Epoch:  448/1000  Loss: 4.441582381725311

Epoch:  449/1000  Loss: 4.426208877563477

Epoch:  449/1000  Loss: 4.432250654697418

Epoch:  450/1000  Loss: 4.4244763323601255

Epoch:  450/1000  Loss: 4.4400572514534

Epoch:  451/1000  Loss: 4.42373104805642

Epoch:  451/1000  Loss: 4.43976773738861

Epoch:  452/1000  Loss: 4.421812254317263

Epoch:  452/1000  Loss: 4.435465685129166

Epoch:  453/1000  Loss: 4.42365943320254

Epoch:  453/1000  Loss: 4.435123510360718

Epoch:  454/1000  Loss: 4.4234081004528285

Epoch:  454/1000  Loss: 4.437765362262726

Epoch:  455/1000  Loss: 4.423507867975438

Epoch:  455/1000  Loss: 4.437261402606964

Epoch:  456/1000  Loss: 4.416064652990787

Epoch:  456/1000  Loss: 4.436880598068237

Epoch:  457/1000  Loss: 4.421794363792906

Epoch:  457/1000  Loss: 4.437907387018203

Epoch:  458/1000  Loss: 4.41863360506423

Epoch:  458/1000  Loss: 4.439829223155975

Epoch:  459/1000  Loss: 4.411716182181176

Epoch:  459/1000  Loss: 4.437218979597092

Epoch:  460/1000  Loss: 4.415123150196481

Epoch:  460/1000  Loss: 4.435820834636688

Epoch:  461/1000  Loss: 4.418656995448661

Epoch:  461/1000  Loss: 4.428723032474518

Epoch:  462/1000  Loss: 4.4111895500345435

Epoch:  462/1000  Loss: 4.428813787698746

Epoch:  463/1000  Loss: 4.410184635000026

Epoch:  463/1000  Loss: 4.4319604921340945

Epoch:  464/1000  Loss: 4.413968121751826

Epoch:  464/1000  Loss: 4.421633268594742

Epoch:  465/1000  Loss: 4.414942769801363

Epoch:  465/1000  Loss: 4.428988724946976

Epoch:  466/1000  Loss: 4.414184028544325

Epoch:  466/1000  Loss: 4.424693648815155

Epoch:  467/1000  Loss: 4.412439371677155

Epoch:  467/1000  Loss: 4.425325208902359

Epoch:  468/1000  Loss: 4.412888316905245

Epoch:  468/1000  Loss: 4.427961345911026

Epoch:  469/1000  Loss: 4.406562615455465

Epoch:  469/1000  Loss: 4.424297552108765

Epoch:  470/1000  Loss: 4.401510008345259

Epoch:  470/1000  Loss: 4.425130113363266

Epoch:  471/1000  Loss: 4.403101420909801

Epoch:  471/1000  Loss: 4.421966553926468

Epoch:  472/1000  Loss: 4.404350279747171

Epoch:  472/1000  Loss: 4.424873101711273

Epoch:  473/1000  Loss: 4.4063755431073774

Epoch:  473/1000  Loss: 4.426281514167786

Epoch:  474/1000  Loss: 4.403657003159219

Epoch:  474/1000  Loss: 4.420587706565857

Epoch:  475/1000  Loss: 4.406035732715688

Epoch:  475/1000  Loss: 4.4195574796199795

Epoch:  476/1000  Loss: 4.402528803399268

Epoch:  476/1000  Loss: 4.417341246604919

Epoch:  477/1000  Loss: 4.400089898007981

Epoch:  477/1000  Loss: 4.4188322567939755

Epoch:  478/1000  Loss: 4.404194404723796

Epoch:  478/1000  Loss: 4.418641637563706

Epoch:  479/1000  Loss: 4.395890880138316

Epoch:  479/1000  Loss: 4.415235441923142

Epoch:  480/1000  Loss: 4.399440953072081

Epoch:  480/1000  Loss: 4.41466243982315

Epoch:  481/1000  Loss: 4.39898490804307

Epoch:  481/1000  Loss: 4.413056936264038

Epoch:  482/1000  Loss: 4.391550119887007

Epoch:  482/1000  Loss: 4.408946634531021

Epoch:  483/1000  Loss: 4.402776554797558

Epoch:  483/1000  Loss: 4.407458121776581

Epoch:  484/1000  Loss: 4.397428546053298

Epoch:  484/1000  Loss: 4.414951944351197

Epoch:  485/1000  Loss: 4.394816758784842

Epoch:  485/1000  Loss: 4.413056025505066

Epoch:  486/1000  Loss: 4.398717146731437

Epoch:  486/1000  Loss: 4.4119273841381075

Epoch:  487/1000  Loss: 4.392782670893568

Epoch:  487/1000  Loss: 4.41277575969696

Epoch:  488/1000  Loss: 4.390298053051563

Epoch:  488/1000  Loss: 4.404184342622757

Epoch:  489/1000  Loss: 4.3898258158501156

Epoch:  489/1000  Loss: 4.408045158386231

Epoch:  490/1000  Loss: 4.384436104145456

Epoch:  490/1000  Loss: 4.407472914457321

Epoch:  491/1000  Loss: 4.3910265760218845

Epoch:  491/1000  Loss: 4.404168246984482

Epoch:  492/1000  Loss: 4.388599056893207

Epoch:  492/1000  Loss: 4.405051679611206

Epoch:  493/1000  Loss: 4.385997453648994

Epoch:  493/1000  Loss: 4.40926056265831

Epoch:  494/1000  Loss: 4.375942541690583

Epoch:  494/1000  Loss: 4.399519078731537

Epoch:  495/1000  Loss: 4.3844347324777155

Epoch:  495/1000  Loss: 4.406236406564712

Epoch:  496/1000  Loss: 4.381642284799129

Epoch:  496/1000  Loss: 4.400529994964599

Epoch:  497/1000  Loss: 4.3844659947334454

Epoch:  497/1000  Loss: 4.402906800508499

Epoch:  498/1000  Loss: 4.385767685098851

Epoch:  498/1000  Loss: 4.4028353071212765

Epoch:  499/1000  Loss: 4.37861388896374

Epoch:  499/1000  Loss: 4.4019582867622375

Epoch:  500/1000  Loss: 4.378138234767508

Epoch:  500/1000  Loss: 4.404699648618698

Epoch:  501/1000  Loss: 4.381704077822096

Epoch:  501/1000  Loss: 4.3931661581993104

Epoch:  502/1000  Loss: 4.380212764537081

Epoch:  502/1000  Loss: 4.403463834524155

Epoch:  503/1000  Loss: 4.382643817333465

Epoch:  503/1000  Loss: 4.399172679185868

Epoch:  504/1000  Loss: 4.381588118127052

Epoch:  504/1000  Loss: 4.4002253949642185

Epoch:  505/1000  Loss: 4.378815002644315

Epoch:  505/1000  Loss: 4.391401232481003

Epoch:  506/1000  Loss: 4.377602986071972

Epoch:  506/1000  Loss: 4.395464648008346

Epoch:  507/1000  Loss: 4.378545242674807

Epoch:  507/1000  Loss: 4.395140128135681

Epoch:  508/1000  Loss: 4.372951934692708

Epoch:  508/1000  Loss: 4.390679442882538

Epoch:  509/1000  Loss: 4.3804226581086505

Epoch:  509/1000  Loss: 4.39027353644371

Epoch:  510/1000  Loss: 4.37471414221094

Epoch:  510/1000  Loss: 4.391741658449173

Epoch:  511/1000  Loss: 4.37778053790965

Epoch:  511/1000  Loss: 4.387940125465393

Epoch:  512/1000  Loss: 4.376119984971716

Epoch:  512/1000  Loss: 4.393975104093552

Epoch:  513/1000  Loss: 4.376410373728326

Epoch:  513/1000  Loss: 4.392227761745453

Epoch:  514/1000  Loss: 4.372616106398562

Epoch:  514/1000  Loss: 4.384137885570526

Epoch:  515/1000  Loss: 4.37219309705369

Epoch:  515/1000  Loss: 4.385020530223846

Epoch:  516/1000  Loss: 4.367804396406133

Epoch:  516/1000  Loss: 4.394892129898071

Epoch:  517/1000  Loss: 4.3692675144114395

Epoch:  517/1000  Loss: 4.390337390899658

Epoch:  518/1000  Loss: 4.3738968128853655

Epoch:  518/1000  Loss: 4.385758613348007

Epoch:  519/1000  Loss: 4.370017182573359

Epoch:  519/1000  Loss: 4.378211244344711

Epoch:  520/1000  Loss: 4.3727713645772734

Epoch:  520/1000  Loss: 4.389226901531219

Epoch:  521/1000  Loss: 4.371488421014015

Epoch:  521/1000  Loss: 4.385660527944565

Epoch:  522/1000  Loss: 4.371169166362032

Epoch:  522/1000  Loss: 4.383669049739837

Epoch:  523/1000  Loss: 4.365665606234936

Epoch:  523/1000  Loss: 4.384692497253418

Epoch:  524/1000  Loss: 4.3648042790433195

Epoch:  524/1000  Loss: 4.3867121231555934

Epoch:  525/1000  Loss: 4.363742229786325

Epoch:  525/1000  Loss: 4.381601824760437

Epoch:  526/1000  Loss: 4.366492043150232

Epoch:  526/1000  Loss: 4.385822649002075

Epoch:  527/1000  Loss: 4.368258404224477

Epoch:  527/1000  Loss: 4.377812786102295

Epoch:  528/1000  Loss: 4.365984765519487

Epoch:  528/1000  Loss: 4.378325545787812

Epoch:  529/1000  Loss: 4.360181411783746

Epoch:  529/1000  Loss: 4.377106237411499

Epoch:  530/1000  Loss: 4.368018649486785

Epoch:  530/1000  Loss: 4.3833245241642

Epoch:  531/1000  Loss: 4.36100215303137

Epoch:  531/1000  Loss: 4.381921871900558

Epoch:  532/1000  Loss: 4.357092716338786

Epoch:  532/1000  Loss: 4.375839776992798

Epoch:  533/1000  Loss: 4.360886847719233

Epoch:  533/1000  Loss: 4.3763702356815335

Epoch:  534/1000  Loss: 4.357499941359175

Epoch:  534/1000  Loss: 4.377320067882538

Epoch:  535/1000  Loss: 4.355588778029097

Epoch:  535/1000  Loss: 4.376261029243469

Epoch:  536/1000  Loss: 4.36272853993355

Epoch:  536/1000  Loss: 4.378386077880859

Epoch:  537/1000  Loss: 4.357098195907917

Epoch:  537/1000  Loss: 4.379648607969284

Epoch:  538/1000  Loss: 4.355665447356853

Epoch:  538/1000  Loss: 4.3753793025016785

Epoch:  539/1000  Loss: 4.34903083659233

Epoch:  539/1000  Loss: 4.372934694290161

Epoch:  540/1000  Loss: 4.351734092387748

Epoch:  540/1000  Loss: 4.377703434228897

Epoch:  541/1000  Loss: 4.353041099994741

Epoch:  541/1000  Loss: 4.36759975194931

Epoch:  542/1000  Loss: 4.356920545659166

Epoch:  542/1000  Loss: 4.374184604883194

Epoch:  543/1000  Loss: 4.354291916908102

Epoch:  543/1000  Loss: 4.374713827371597

Epoch:  544/1000  Loss: 4.355950096820263

Epoch:  544/1000  Loss: 4.368005791902542

Epoch:  545/1000  Loss: 4.352471752369658

Epoch:  545/1000  Loss: 4.368590524196625

Epoch:  546/1000  Loss: 4.34934777503318

Epoch:  546/1000  Loss: 4.367665328979492

Epoch:  547/1000  Loss: 4.349694463039967

Epoch:  547/1000  Loss: 4.369125225543976

Epoch:  548/1000  Loss: 4.352816287507403

Epoch:  548/1000  Loss: 4.3708762156963346

Epoch:  549/1000  Loss: 4.353062710863479

Epoch:  549/1000  Loss: 4.3641853165626525

Epoch:  550/1000  Loss: 4.348785430827039

Epoch:  550/1000  Loss: 4.364522091150284

Epoch:  551/1000  Loss: 4.3461917268468975

Epoch:  551/1000  Loss: 4.3667629683017735

Epoch:  552/1000  Loss: 4.349483270848051

Epoch:  552/1000  Loss: 4.36271012544632

Epoch:  553/1000  Loss: 4.346797206554007

Epoch:  553/1000  Loss: 4.362983456850052

Epoch:  554/1000  Loss: 4.348939489811025

Epoch:  554/1000  Loss: 4.366170400381089

Epoch:  555/1000  Loss: 4.3436688686939

Epoch:  555/1000  Loss: 4.364961106777191

Epoch:  556/1000  Loss: 4.350733795571834

Epoch:  556/1000  Loss: 4.357740718126297

Epoch:  557/1000  Loss: 4.341979700453738

Epoch:  557/1000  Loss: 4.362500982284546

Epoch:  558/1000  Loss: 4.3403892719999275

Epoch:  558/1000  Loss: 4.363763165473938

Epoch:  559/1000  Loss: 4.346565995317825

Epoch:  559/1000  Loss: 4.357930804491043

Epoch:  560/1000  Loss: 4.3479904631350905

Epoch:  560/1000  Loss: 4.358907544612885

Epoch:  561/1000  Loss: 4.347023610865816

Epoch:  561/1000  Loss: 4.355830043554306

Epoch:  562/1000  Loss: 4.337805792625915

Epoch:  562/1000  Loss: 4.360950478315353

Epoch:  563/1000  Loss: 4.340572119773703

Epoch:  563/1000  Loss: 4.355662215948104

Epoch:  564/1000  Loss: 4.338578076058246

Epoch:  564/1000  Loss: 4.357211664915085

Epoch:  565/1000  Loss: 4.341784569557677

Epoch:  565/1000  Loss: 4.360345506668091

Epoch:  566/1000  Loss: 4.337396390387353

Epoch:  566/1000  Loss: 4.365078935623169

Epoch:  567/1000  Loss: 4.3370456452065325

Epoch:  567/1000  Loss: 4.3592591691017155

Epoch:  568/1000  Loss: 4.3412074535451035

Epoch:  568/1000  Loss: 4.356044445037842

Epoch:  569/1000  Loss: 4.338802405621143

Epoch:  569/1000  Loss: 4.356343613862991

Epoch:  570/1000  Loss: 4.336052000776251

Epoch:  570/1000  Loss: 4.350045973062516

Epoch:  571/1000  Loss: 4.334839108649721

Epoch:  571/1000  Loss: 4.356709418296814

Epoch:  572/1000  Loss: 4.340417017835252

Epoch:  572/1000  Loss: 4.354943209886551

Epoch:  573/1000  Loss: 4.33417781464597

Epoch:  573/1000  Loss: 4.354273951053619

Epoch:  574/1000  Loss: 4.335605915556562

Epoch:  574/1000  Loss: 4.351063535213471

Epoch:  575/1000  Loss: 4.33615899288908

Epoch:  575/1000  Loss: 4.351077581644058

Epoch:  576/1000  Loss: 4.337125956758539

Epoch:  576/1000  Loss: 4.355869269371032

Epoch:  577/1000  Loss: 4.334921278852097

Epoch:  577/1000  Loss: 4.347891703844071

Epoch:  578/1000  Loss: 4.333561982499792

Epoch:  578/1000  Loss: 4.349157971143723

Epoch:  579/1000  Loss: 4.3282496685677385

Epoch:  579/1000  Loss: 4.346790586709976

Epoch:  580/1000  Loss: 4.336128584882046

Epoch:  580/1000  Loss: 4.347709386348725

Epoch:  581/1000  Loss: 4.334274266628509

Epoch:  581/1000  Loss: 4.347970839738846

Epoch:  582/1000  Loss: 4.330031889042956

Epoch:  582/1000  Loss: 4.348928171396255

Epoch:  583/1000  Loss: 4.32746283855844

Epoch:  583/1000  Loss: 4.350152423381806

Epoch:  584/1000  Loss: 4.322362185539084

Epoch:  584/1000  Loss: 4.347852902412415

Epoch:  585/1000  Loss: 4.332100726188497

Epoch:  585/1000  Loss: 4.342537353038788

Epoch:  586/1000  Loss: 4.328299324563209

Epoch:  586/1000  Loss: 4.346537036895752

Epoch:  587/1000  Loss: 4.330073914629348

Epoch:  587/1000  Loss: 4.344744917154312

Epoch:  588/1000  Loss: 4.328951761570383

Epoch:  588/1000  Loss: 4.34283096909523

Epoch:  589/1000  Loss: 4.323941657898274

Epoch:  589/1000  Loss: 4.338819839954376

Epoch:  590/1000  Loss: 4.330489031811978

Epoch:  590/1000  Loss: 4.346596171855927

Epoch:  591/1000  Loss: 4.320778788911535

Epoch:  591/1000  Loss: 4.341505408287048

Epoch:  592/1000  Loss: 4.32082966946541

Epoch:  592/1000  Loss: 4.34708008646965

Epoch:  593/1000  Loss: 4.325886460568043

Epoch:  593/1000  Loss: 4.334579986333847

Epoch:  594/1000  Loss: 4.323537399413738

Epoch:  594/1000  Loss: 4.345902760028839

Epoch:  595/1000  Loss: 4.326990571935126

Epoch:  595/1000  Loss: 4.340595937967301

Epoch:  596/1000  Loss: 4.323688526356474

Epoch:  596/1000  Loss: 4.3402991938591

Epoch:  597/1000  Loss: 4.325368681359798

Epoch:  597/1000  Loss: 4.338940871953964

Epoch:  598/1000  Loss: 4.3225296243708184

Epoch:  598/1000  Loss: 4.342596056461335

Epoch:  599/1000  Loss: 4.325466763719599

Epoch:  599/1000  Loss: 4.331440087556839

Epoch:  600/1000  Loss: 4.3210266691573125

Epoch:  600/1000  Loss: 4.335574330091476

Epoch:  601/1000  Loss: 4.316340919251138

Epoch:  601/1000  Loss: 4.331905319690704

Epoch:  602/1000  Loss: 4.320451861239494

Epoch:  602/1000  Loss: 4.331677114963531

Epoch:  603/1000  Loss: 4.31724005252757

Epoch:  603/1000  Loss: 4.337367324829102

Epoch:  604/1000  Loss: 4.319475598030902

Epoch:  604/1000  Loss: 4.334522742033005

Epoch:  605/1000  Loss: 4.317218788633955

Epoch:  605/1000  Loss: 4.337215269804001

Epoch:  606/1000  Loss: 4.314164903316092

Epoch:  606/1000  Loss: 4.333291785717011

Epoch:  607/1000  Loss: 4.313453525177976

Epoch:  607/1000  Loss: 4.3334007477760315

Epoch:  608/1000  Loss: 4.312435702060132

Epoch:  608/1000  Loss: 4.329389398097992

Epoch:  609/1000  Loss: 4.30815290390177

Epoch:  609/1000  Loss: 4.3329139268398285

Epoch:  610/1000  Loss: 4.313190282659328

Epoch:  610/1000  Loss: 4.3327997994422915

Epoch:  611/1000  Loss: 4.3187502779859175

Epoch:  611/1000  Loss: 4.326334217786789

Epoch:  612/1000  Loss: 4.316871034338119

Epoch:  612/1000  Loss: 4.326504043340683

Epoch:  613/1000  Loss: 4.314905202135127

Epoch:  613/1000  Loss: 4.322241979837417

Epoch:  614/1000  Loss: 4.310978248271536

Epoch:  614/1000  Loss: 4.3302896165847775

Epoch:  615/1000  Loss: 4.311142586647196

Epoch:  615/1000  Loss: 4.324446415901184

Epoch:  616/1000  Loss: 4.31036080908268

Epoch:  616/1000  Loss: 4.3262344682216645

Epoch:  617/1000  Loss: 4.311711586282608

Epoch:  617/1000  Loss: 4.3284924185276035

Epoch:  618/1000  Loss: 4.3072690527489845

Epoch:  618/1000  Loss: 4.324441959857941

Epoch:  619/1000  Loss: 4.309467920343927

Epoch:  619/1000  Loss: 4.322458298206329

Epoch:  620/1000  Loss: 4.301786331420249

Epoch:  620/1000  Loss: 4.327719538211823

Epoch:  621/1000  Loss: 4.308222819389181

Epoch:  621/1000  Loss: 4.323871295452118

Epoch:  622/1000  Loss: 4.3087142000807095

Epoch:  622/1000  Loss: 4.326597630977631

Epoch:  623/1000  Loss: 4.307733804621595

Epoch:  623/1000  Loss: 4.322677127122879

Epoch:  624/1000  Loss: 4.302535394912071

Epoch:  624/1000  Loss: 4.3181793344020845

Epoch:  625/1000  Loss: 4.30613753541987

Epoch:  625/1000  Loss: 4.328491920232773

Epoch:  626/1000  Loss: 4.3082890814923225

Epoch:  626/1000  Loss: 4.319448850154877

Epoch:  627/1000  Loss: 4.307285297677872

Epoch:  627/1000  Loss: 4.319253985881805

Epoch:  628/1000  Loss: 4.304628983964311

Epoch:  628/1000  Loss: 4.321986638307571

Epoch:  629/1000  Loss: 4.304143808243123

Epoch:  629/1000  Loss: 4.322382595539093

Epoch:  630/1000  Loss: 4.309902586835496

Epoch:  630/1000  Loss: 4.311464601755143

Epoch:  631/1000  Loss: 4.304738138077107

Epoch:  631/1000  Loss: 4.312421550750733

Epoch:  632/1000  Loss: 4.3029162934485905

Epoch:  632/1000  Loss: 4.322046675682068

Epoch:  633/1000  Loss: 4.30435951618438

Epoch:  633/1000  Loss: 4.316936533451081

Epoch:  634/1000  Loss: 4.305152379705551

Epoch:  634/1000  Loss: 4.316680235862732

Epoch:  635/1000  Loss: 4.302076486831016

Epoch:  635/1000  Loss: 4.320981768369674

Epoch:  636/1000  Loss: 4.304787970603781

Epoch:  636/1000  Loss: 4.315806090831757

Epoch:  637/1000  Loss: 4.302642341370278

Epoch:  637/1000  Loss: 4.3196600866317745

Epoch:  638/1000  Loss: 4.3010553775949685

Epoch:  638/1000  Loss: 4.313994765281677

Epoch:  639/1000  Loss: 4.301940064734601

Epoch:  639/1000  Loss: 4.310184409618378

Epoch:  640/1000  Loss: 4.302160451767293

Epoch:  640/1000  Loss: 4.3226370584964755

Epoch:  641/1000  Loss: 4.299968652522311

Epoch:  641/1000  Loss: 4.316416268348694

Epoch:  642/1000  Loss: 4.2990930141286645

Epoch:  642/1000  Loss: 4.310518245697022

Epoch:  643/1000  Loss: 4.29773641139903

Epoch:  643/1000  Loss: 4.311685174703598

Epoch:  644/1000  Loss: 4.29773005018843

Epoch:  644/1000  Loss: 4.314867340326309

Epoch:  645/1000  Loss: 4.302524824345365

Epoch:  645/1000  Loss: 4.312641820907593

Epoch:  646/1000  Loss: 4.296563076465688

Epoch:  646/1000  Loss: 4.308154675960541

Epoch:  647/1000  Loss: 4.3023094055500435

Epoch:  647/1000  Loss: 4.3154141497611995

Epoch:  648/1000  Loss: 4.296094195386197

Epoch:  648/1000  Loss: 4.3114376902580265

Epoch:  649/1000  Loss: 4.2949913856831

Epoch:  649/1000  Loss: 4.311725180149079

Epoch:  650/1000  Loss: 4.292540710530383

Epoch:  650/1000  Loss: 4.312459976673126

Epoch:  651/1000  Loss: 4.293167629647762

Epoch:  651/1000  Loss: 4.3038563621044155

Epoch:  652/1000  Loss: 4.293256707901651

Epoch:  652/1000  Loss: 4.310374375581741

Epoch:  653/1000  Loss: 4.303853199329782

Epoch:  653/1000  Loss: 4.30869643330574

Epoch:  654/1000  Loss: 4.2936391028952094

Epoch:  654/1000  Loss: 4.30845365524292

Epoch:  655/1000  Loss: 4.291705461258584

Epoch:  655/1000  Loss: 4.311144441366196

Epoch:  656/1000  Loss: 4.292675356154747

Epoch:  656/1000  Loss: 4.317254430055618

Epoch:  657/1000  Loss: 4.293446544890708

Epoch:  657/1000  Loss: 4.312174422740936

Epoch:  658/1000  Loss: 4.287401310941006

Epoch:  658/1000  Loss: 4.30267454624176

Epoch:  659/1000  Loss: 4.292832151372382

Epoch:  659/1000  Loss: 4.30676838517189

Epoch:  660/1000  Loss: 4.294628173746961

Epoch:  660/1000  Loss: 4.310573343038559

Epoch:  661/1000  Loss: 4.290482137558308

Epoch:  661/1000  Loss: 4.304382991790772

Epoch:  662/1000  Loss: 4.290075836790369

Epoch:  662/1000  Loss: 4.301368323564529

Epoch:  663/1000  Loss: 4.288867143874473

Epoch:  663/1000  Loss: 4.306048845052719

Epoch:  664/1000  Loss: 4.291259542424628

Epoch:  664/1000  Loss: 4.306515645980835

Epoch:  665/1000  Loss: 4.28480952749861

Epoch:  665/1000  Loss: 4.307645318508148

Epoch:  666/1000  Loss: 4.290053364571104

Epoch:  666/1000  Loss: 4.306805574893952

Epoch:  667/1000  Loss: 4.2874053214458705

Epoch:  667/1000  Loss: 4.305000443458557

Epoch:  668/1000  Loss: 4.283873777186617

Epoch:  668/1000  Loss: 4.3122657513618465

Epoch:  669/1000  Loss: 4.288270670302371

Epoch:  669/1000  Loss: 4.297441620826721

Epoch:  670/1000  Loss: 4.2819348761375915

Epoch:  670/1000  Loss: 4.311282968521118

Epoch:  671/1000  Loss: 4.291985841507607

Epoch:  671/1000  Loss: 4.299888900518417

Epoch:  672/1000  Loss: 4.2843761423800855

Epoch:  672/1000  Loss: 4.303737391233444

Epoch:  673/1000  Loss: 4.288900407831719

Epoch:  673/1000  Loss: 4.3080549919605255

Epoch:  674/1000  Loss: 4.294618340756031

Epoch:  674/1000  Loss: 4.303298454284668

Epoch:  675/1000  Loss: 4.282866334915161

Epoch:  675/1000  Loss: 4.302434825897217

Epoch:  676/1000  Loss: 4.277169502542374

Epoch:  676/1000  Loss: 4.301446244716645

Epoch:  677/1000  Loss: 4.283259111769656

Epoch:  677/1000  Loss: 4.305021790266037

Epoch:  678/1000  Loss: 4.283762416433781

Epoch:  678/1000  Loss: 4.30198407292366

Epoch:  679/1000  Loss: 4.286384624115964

Epoch:  679/1000  Loss: 4.306740173101425

Epoch:  680/1000  Loss: 4.286031885350004

Epoch:  680/1000  Loss: 4.300534996986389

Epoch:  681/1000  Loss: 4.277519359994442

Epoch:  681/1000  Loss: 4.295381430387497

Epoch:  682/1000  Loss: 4.282160437360723

Epoch:  682/1000  Loss: 4.297067719697952

Epoch:  683/1000  Loss: 4.2866478818528195

Epoch:  683/1000  Loss: 4.295136609077454

Epoch:  684/1000  Loss: 4.280754086311827

Epoch:  684/1000  Loss: 4.297492425441742

Epoch:  685/1000  Loss: 4.277627849578858

Epoch:  685/1000  Loss: 4.307665683031082

Epoch:  686/1000  Loss: 4.285539658526157

Epoch:  686/1000  Loss: 4.294311965703964

Epoch:  687/1000  Loss: 4.284102038119702

Epoch:  687/1000  Loss: 4.29488417506218

Epoch:  688/1000  Loss: 4.277120227002083

Epoch:  688/1000  Loss: 4.294609646797181

Epoch:  689/1000  Loss: 4.2820747801598085

Epoch:  689/1000  Loss: 4.291666306257248

Epoch:  690/1000  Loss: 4.285572499417244

Epoch:  690/1000  Loss: 4.291365904808044

Epoch:  691/1000  Loss: 4.281054796056544

Epoch:  691/1000  Loss: 4.297734303474426

Epoch:  692/1000  Loss: 4.281531004195518

Epoch:  692/1000  Loss: 4.299254610538482

Epoch:  693/1000  Loss: 4.278341402906053

Epoch:  693/1000  Loss: 4.295669187307357

Epoch:  694/1000  Loss: 4.27975033699198

Epoch:  694/1000  Loss: 4.296112505197525

Epoch:  695/1000  Loss: 4.275203888467018

Epoch:  695/1000  Loss: 4.289908308982849

Epoch:  696/1000  Loss: 4.274200868606568

Epoch:  696/1000  Loss: 4.296657378673554

Epoch:  697/1000  Loss: 4.279563052603539

Epoch:  697/1000  Loss: 4.2994731533527375

Epoch:  698/1000  Loss: 4.279010368915315

Epoch:  698/1000  Loss: 4.289016482830047

Epoch:  699/1000  Loss: 4.274751041290608

Epoch:  699/1000  Loss: 4.289741381406784

Epoch:  700/1000  Loss: 4.280550728452966

Epoch:  700/1000  Loss: 4.294936447143555

Epoch:  701/1000  Loss: 4.282019167758049

Epoch:  701/1000  Loss: 4.290248643159867

Epoch:  702/1000  Loss: 4.27940653232818

Epoch:  702/1000  Loss: 4.293891775608063

Epoch:  703/1000  Loss: 4.2765862627232325

Epoch:  703/1000  Loss: 4.295071784257889

Epoch:  704/1000  Loss: 4.276685550365042

Epoch:  704/1000  Loss: 4.2918691420555115

Epoch:  705/1000  Loss: 4.272592364980819

Epoch:  705/1000  Loss: 4.293861646652221

Epoch:  706/1000  Loss: 4.272427987037821

Epoch:  706/1000  Loss: 4.288674517869949

Epoch:  707/1000  Loss: 4.277267053279471

Epoch:  707/1000  Loss: 4.287510712146759

Epoch:  708/1000  Loss: 4.272956089263267

Epoch:  708/1000  Loss: 4.28810963511467

Epoch:  709/1000  Loss: 4.270133621134656

Epoch:  709/1000  Loss: 4.2893186223506925

Epoch:  710/1000  Loss: 4.2745721492361515

Epoch:  710/1000  Loss: 4.290080317258835

Epoch:  711/1000  Loss: 4.276725459606089

Epoch:  711/1000  Loss: 4.289593048095703

Epoch:  712/1000  Loss: 4.274912725610936

Epoch:  712/1000  Loss: 4.285122139453888

Epoch:  713/1000  Loss: 4.276071597160177

Epoch:  713/1000  Loss: 4.28462710738182

Epoch:  714/1000  Loss: 4.270322152401539

Epoch:  714/1000  Loss: 4.293071403503418

Epoch:  715/1000  Loss: 4.281742302914883

Epoch:  715/1000  Loss: 4.281509158611297

Epoch:  716/1000  Loss: 4.264082136559994

Epoch:  716/1000  Loss: 4.2899816131591795

Epoch:  717/1000  Loss: 4.273390621834613

Epoch:  717/1000  Loss: 4.282479455471039

Epoch:  718/1000  Loss: 4.270689281504205

Epoch:  718/1000  Loss: 4.290024622678756

Epoch:  719/1000  Loss: 4.267769358005929

Epoch:  719/1000  Loss: 4.2956735956668854

Epoch:  720/1000  Loss: 4.270242463781479

Epoch:  720/1000  Loss: 4.282808904647827

Epoch:  721/1000  Loss: 4.2765294693885965

Epoch:  721/1000  Loss: 4.289738136529922

Epoch:  722/1000  Loss: 4.270027425441336

Epoch:  722/1000  Loss: 4.281741834878922

Epoch:  723/1000  Loss: 4.269532267590787

Epoch:  723/1000  Loss: 4.28068985581398

Epoch:  724/1000  Loss: 4.270686057273378

Epoch:  724/1000  Loss: 4.285125188827514

Epoch:  725/1000  Loss: 4.268786970097968

Epoch:  725/1000  Loss: 4.2863603115081785

Epoch:  726/1000  Loss: 4.2686459115211

Epoch:  726/1000  Loss: 4.282043497562409

Epoch:  727/1000  Loss: 4.266561340778432

Epoch:  727/1000  Loss: 4.2869668960571286

Epoch:  728/1000  Loss: 4.2699852912984

Epoch:  728/1000  Loss: 4.284008663892746

Epoch:  729/1000  Loss: 4.2696653102306605

Epoch:  729/1000  Loss: 4.28158077955246

Epoch:  730/1000  Loss: 4.265588076571201

Epoch:  730/1000  Loss: 4.284603326320648

Epoch:  731/1000  Loss: 4.271774441130618

Epoch:  731/1000  Loss: 4.281996554136276

Epoch:  732/1000  Loss: 4.265650072503597

Epoch:  732/1000  Loss: 4.290487995147705

Epoch:  733/1000  Loss: 4.260495522681703

Epoch:  733/1000  Loss: 4.2797050023078915

Epoch:  734/1000  Loss: 4.266449451446533

Epoch:  734/1000  Loss: 4.278748071193695

Epoch:  735/1000  Loss: 4.269476503006955

Epoch:  735/1000  Loss: 4.282331873178482

Epoch:  736/1000  Loss: 4.268407112486819

Epoch:  736/1000  Loss: 4.2851195299625395

Epoch:  737/1000  Loss: 4.263818365462283

Epoch:  737/1000  Loss: 4.280702384710312

Epoch:  738/1000  Loss: 4.256895430544589

Epoch:  738/1000  Loss: 4.276391503810882

Epoch:  739/1000  Loss: 4.26256382110271

Epoch:  739/1000  Loss: 4.283273681402206

Epoch:  740/1000  Loss: 4.263980264866606

Epoch:  740/1000  Loss: 4.279883615970611

Epoch:  741/1000  Loss: 4.26223859888442

Epoch:  741/1000  Loss: 4.27609060883522

Epoch:  742/1000  Loss: 4.269623956274479

Epoch:  742/1000  Loss: 4.269142796993256

Epoch:  743/1000  Loss: 4.266871775972082

Epoch:  743/1000  Loss: 4.277728984355926

Epoch:  744/1000  Loss: 4.267422652751842

Epoch:  744/1000  Loss: 4.280020200014114

Epoch:  745/1000  Loss: 4.2670950321441

Epoch:  745/1000  Loss: 4.285800520181656

Epoch:  746/1000  Loss: 4.2646556225228816

Epoch:  746/1000  Loss: 4.27392829656601

Epoch:  747/1000  Loss: 4.26516633541026

Epoch:  747/1000  Loss: 4.2777729105949405

Epoch:  748/1000  Loss: 4.259822063243136

Epoch:  748/1000  Loss: 4.275236918926239

Epoch:  749/1000  Loss: 4.258942212449743

Epoch:  749/1000  Loss: 4.278565983772278

Epoch:  750/1000  Loss: 4.263356472583527

Epoch:  750/1000  Loss: 4.2771377742290495

Epoch:  751/1000  Loss: 4.260509877509259

Epoch:  751/1000  Loss: 4.2763181495666505

Epoch:  752/1000  Loss: 4.261297953382451

Epoch:  752/1000  Loss: 4.2793566870689395

Epoch:  753/1000  Loss: 4.25583305257432

Epoch:  753/1000  Loss: 4.278868721723557

Epoch:  754/1000  Loss: 4.2578984138813425

Epoch:  754/1000  Loss: 4.280008928775787

Epoch:  755/1000  Loss: 4.25589483748091

Epoch:  755/1000  Loss: 4.276726770401001

Epoch:  756/1000  Loss: 4.263141723389321

Epoch:  756/1000  Loss: 4.274141170978546

Epoch:  757/1000  Loss: 4.258778616722594

Epoch:  757/1000  Loss: 4.275844521522522

Epoch:  758/1000  Loss: 4.255354841719282

Epoch:  758/1000  Loss: 4.274883778095245

Epoch:  759/1000  Loss: 4.261304228356544

Epoch:  759/1000  Loss: 4.265246392488479

Epoch:  760/1000  Loss: 4.257558657260652

Epoch:  760/1000  Loss: 4.272147139310837

Epoch:  761/1000  Loss: 4.254789831283245

Epoch:  761/1000  Loss: 4.269219896793365

Epoch:  762/1000  Loss: 4.254487082298766

Epoch:  762/1000  Loss: 4.278409556150437

Epoch:  763/1000  Loss: 4.251001997196928

Epoch:  763/1000  Loss: 4.270302724838257

Epoch:  764/1000  Loss: 4.257269400738656

Epoch:  764/1000  Loss: 4.275276415348053

Epoch:  765/1000  Loss: 4.255350609028593

Epoch:  765/1000  Loss: 4.270115431547165

Epoch:  766/1000  Loss: 4.255314935521876

Epoch:  766/1000  Loss: 4.2736507999897

Epoch:  767/1000  Loss: 4.258672630025985

Epoch:  767/1000  Loss: 4.267810255289078

Epoch:  768/1000  Loss: 4.2519241261989515

Epoch:  768/1000  Loss: 4.271369577646255

Epoch:  769/1000  Loss: 4.2620699253488095

Epoch:  769/1000  Loss: 4.2699126327037815

Epoch:  770/1000  Loss: 4.253290997160242

Epoch:  770/1000  Loss: 4.2743379855155945

Epoch:  771/1000  Loss: 4.254784325335888

Epoch:  771/1000  Loss: 4.269423543214798

Epoch:  772/1000  Loss: 4.253190507280066

Epoch:  772/1000  Loss: 4.273460961580277

Epoch:  773/1000  Loss: 4.256890147797605

Epoch:  773/1000  Loss: 4.270639872550964

Epoch:  774/1000  Loss: 4.2481476286624344

Epoch:  774/1000  Loss: 4.263577696084976

Epoch:  775/1000  Loss: 4.251524601591394

Epoch:  775/1000  Loss: 4.26411484003067

Epoch:  776/1000  Loss: 4.251099001093114

Epoch:  776/1000  Loss: 4.269364438056946

Epoch:  777/1000  Loss: 4.251990258440058

Epoch:  777/1000  Loss: 4.27435671210289

Epoch:  778/1000  Loss: 4.258621173209332

Epoch:  778/1000  Loss: 4.268057206869125

Epoch:  779/1000  Loss: 4.248606156288309

Epoch:  779/1000  Loss: 4.26646954536438

Epoch:  780/1000  Loss: 4.2537102445643

Epoch:  780/1000  Loss: 4.266927261352539

Epoch:  781/1000  Loss: 4.25048560183099

Epoch:  781/1000  Loss: 4.276363226175309

Epoch:  782/1000  Loss: 4.2483832907169425

Epoch:  782/1000  Loss: 4.270498888492584

Epoch:  783/1000  Loss: 4.249089487562788

Epoch:  783/1000  Loss: 4.261977409124374

Epoch:  784/1000  Loss: 4.256878076715672

Epoch:  784/1000  Loss: 4.264523202180863

Epoch:  785/1000  Loss: 4.250816525804235

Epoch:  785/1000  Loss: 4.264696779251099

Epoch:  786/1000  Loss: 4.253292405351679

Epoch:  786/1000  Loss: 4.270165517330169

Epoch:  787/1000  Loss: 4.249308547567814

Epoch:  787/1000  Loss: 4.264834386110306

Epoch:  788/1000  Loss: 4.250222390763303

Epoch:  788/1000  Loss: 4.2615227663517

Epoch:  789/1000  Loss: 4.255713223396463

Epoch:  789/1000  Loss: 4.263658322095871

Epoch:  790/1000  Loss: 4.255579467529946

Epoch:  790/1000  Loss: 4.263446643352508

Epoch:  791/1000  Loss: 4.255400366478778

Epoch:  791/1000  Loss: 4.263916985988617

Epoch:  792/1000  Loss: 4.256498334762898

Epoch:  792/1000  Loss: 4.262290308475494

Epoch:  793/1000  Loss: 4.248669817092571

Epoch:  793/1000  Loss: 4.256122843027115

Epoch:  794/1000  Loss: 4.2489006833827245

Epoch:  794/1000  Loss: 4.267964762449265

Epoch:  795/1000  Loss: 4.24506257036899

Epoch:  795/1000  Loss: 4.269125432968139

Epoch:  796/1000  Loss: 4.250347675161159

Epoch:  796/1000  Loss: 4.2668286800384525

Epoch:  797/1000  Loss: 4.247172221731632

Epoch:  797/1000  Loss: 4.265708507299423

Epoch:  798/1000  Loss: 4.247471967656562

Epoch:  798/1000  Loss: 4.259885014295578

Epoch:  799/1000  Loss: 4.253206435670244

Epoch:  799/1000  Loss: 4.263140176534653

Epoch:  800/1000  Loss: 4.249502952048119

Epoch:  800/1000  Loss: 4.26291597366333

Epoch:  801/1000  Loss: 4.249389788445006

Epoch:  801/1000  Loss: 4.253237012624741

Epoch:  802/1000  Loss: 4.247695059471942

Epoch:  802/1000  Loss: 4.260675866603851

Epoch:  803/1000  Loss: 4.245716388174828

Epoch:  803/1000  Loss: 4.265858991146088

Epoch:  804/1000  Loss: 4.243706470854739

Epoch:  804/1000  Loss: 4.262018880844116

Epoch:  805/1000  Loss: 4.246185465061918

Epoch:  805/1000  Loss: 4.266673396825791

Epoch:  806/1000  Loss: 4.2457797131639845

Epoch:  806/1000  Loss: 4.263430707454681

Epoch:  807/1000  Loss: 4.247077166780512

Epoch:  807/1000  Loss: 4.259955284595489

Epoch:  808/1000  Loss: 4.242259279210517

Epoch:  808/1000  Loss: 4.255958721637726

Epoch:  809/1000  Loss: 4.243518309897564

Epoch:  809/1000  Loss: 4.264563436508179

Epoch:  810/1000  Loss: 4.239884867566698

Epoch:  810/1000  Loss: 4.2567954289913175

Epoch:  811/1000  Loss: 4.242630750574964

Epoch:  811/1000  Loss: 4.249381687641144

Epoch:  812/1000  Loss: 4.244466395073749

Epoch:  812/1000  Loss: 4.259400537014008

Epoch:  813/1000  Loss: 4.240217620768446

Epoch:  813/1000  Loss: 4.256868735551834

Epoch:  814/1000  Loss: 4.244999447274715

Epoch:  814/1000  Loss: 4.261390408277512

Epoch:  815/1000  Loss: 4.246073345427818

Epoch:  815/1000  Loss: 4.260400644540787

Epoch:  816/1000  Loss: 4.247308512951466

Epoch:  816/1000  Loss: 4.257009161710739

Epoch:  817/1000  Loss: 4.241257897843706

Epoch:  817/1000  Loss: 4.2538852798938755

Epoch:  818/1000  Loss: 4.242998659864385

Epoch:  818/1000  Loss: 4.262868338823319

Epoch:  819/1000  Loss: 4.246032258297535

Epoch:  819/1000  Loss: 4.258018808364868

Epoch:  820/1000  Loss: 4.243427220811236

Epoch:  820/1000  Loss: 4.2551637041568755

Epoch:  821/1000  Loss: 4.240153978226033

Epoch:  821/1000  Loss: 4.256166477203369

Epoch:  822/1000  Loss: 4.2372069602317

Epoch:  822/1000  Loss: 4.256007132530212

Epoch:  823/1000  Loss: 4.238206455555368

Epoch:  823/1000  Loss: 4.261645756959915

Epoch:  824/1000  Loss: 4.239785504848399

Epoch:  824/1000  Loss: 4.260566548109055

Epoch:  825/1000  Loss: 4.240086098934742

Epoch:  825/1000  Loss: 4.254371292591095

Epoch:  826/1000  Loss: 4.244038068487289

Epoch:  826/1000  Loss: 4.251478639841079

Epoch:  827/1000  Loss: 4.2429487441448455

Epoch:  827/1000  Loss: 4.252269887924195

Epoch:  828/1000  Loss: 4.237501610086319

Epoch:  828/1000  Loss: 4.259074119329452

Epoch:  829/1000  Loss: 4.24129032074137

Epoch:  829/1000  Loss: 4.254073919057846

Epoch:  830/1000  Loss: 4.240049666546761

Epoch:  830/1000  Loss: 4.248851969242096

Epoch:  831/1000  Loss: 4.246520955511865

Epoch:  831/1000  Loss: 4.25157901763916

Epoch:  832/1000  Loss: 4.233287568802529

Epoch:  832/1000  Loss: 4.2495733010768895

Epoch:  833/1000  Loss: 4.241185553530429

Epoch:  833/1000  Loss: 4.2556919717788695

Epoch:  834/1000  Loss: 4.239413260399027

Epoch:  834/1000  Loss: 4.2527087271213535

Epoch:  835/1000  Loss: 4.240062724783066

Epoch:  835/1000  Loss: 4.2484055781364445

Epoch:  836/1000  Loss: 4.2398858770411065

Epoch:  836/1000  Loss: 4.2504093706607815

Epoch:  837/1000  Loss: 4.243651159773481

Epoch:  837/1000  Loss: 4.248403099775314

Epoch:  838/1000  Loss: 4.240473565649479

Epoch:  838/1000  Loss: 4.252084329128265

Epoch:  839/1000  Loss: 4.2384605509169555

Epoch:  839/1000  Loss: 4.248347197771072

Epoch:  840/1000  Loss: 4.243311933760947

Epoch:  840/1000  Loss: 4.247792762517929

Epoch:  841/1000  Loss: 4.2366018143106015

Epoch:  841/1000  Loss: 4.250341285467147

Epoch:  842/1000  Loss: 4.235540973379257

Epoch:  842/1000  Loss: 4.251919873952866

Epoch:  843/1000  Loss: 4.231689040204312

Epoch:  843/1000  Loss: 4.2500102639198305

Epoch:  844/1000  Loss: 4.231115973249395

Epoch:  844/1000  Loss: 4.2551860332489015

Epoch:  845/1000  Loss: 4.237116821776045

Epoch:  845/1000  Loss: 4.251480119228363

Epoch:  846/1000  Loss: 4.237157021177576

Epoch:  846/1000  Loss: 4.2549300968647

Epoch:  847/1000  Loss: 4.231160537232744

Epoch:  847/1000  Loss: 4.246925208568573

Epoch:  848/1000  Loss: 4.233956784390388

Epoch:  848/1000  Loss: 4.242662763595581

Epoch:  849/1000  Loss: 4.23451879785416

Epoch:  849/1000  Loss: 4.249823170900345

Epoch:  850/1000  Loss: 4.232832493680589

Epoch:  850/1000  Loss: 4.248713899850845

Epoch:  851/1000  Loss: 4.23613763159894

Epoch:  851/1000  Loss: 4.25112890958786

Epoch:  852/1000  Loss: 4.232840411206509

Epoch:  852/1000  Loss: 4.250160974264145

Epoch:  853/1000  Loss: 4.23669871066479

Epoch:  853/1000  Loss: 4.247191693782806

Epoch:  854/1000  Loss: 4.239432102568606

Epoch:  854/1000  Loss: 4.250334705114365

Epoch:  855/1000  Loss: 4.235749564272292

Epoch:  855/1000  Loss: 4.246568148136139

Epoch:  856/1000  Loss: 4.2327106009138395

Epoch:  856/1000  Loss: 4.252983988523483

Epoch:  857/1000  Loss: 4.234374636792122

Epoch:  857/1000  Loss: 4.250980494022369

Epoch:  858/1000  Loss: 4.229666528295963

Epoch:  858/1000  Loss: 4.244695531129837

Epoch:  859/1000  Loss: 4.231598692751946

Epoch:  859/1000  Loss: 4.248168864250183

Epoch:  860/1000  Loss: 4.224963950096293

Epoch:  860/1000  Loss: 4.241870261430741

Epoch:  861/1000  Loss: 4.230077830781328

Epoch:  861/1000  Loss: 4.2481224310398105

Epoch:  862/1000  Loss: 4.227749957429602

Epoch:  862/1000  Loss: 4.2508560502529145

Epoch:  863/1000  Loss: 4.22686569639977

Epoch:  863/1000  Loss: 4.2389839029312135

Epoch:  864/1000  Loss: 4.235322358760428

Epoch:  864/1000  Loss: 4.2427764880657195

Epoch:  865/1000  Loss: 4.234678884262734

Epoch:  865/1000  Loss: 4.244241752624512

Epoch:  866/1000  Loss: 4.231195806949697

Epoch:  866/1000  Loss: 4.246909085512161

Epoch:  867/1000  Loss: 4.23670682704195

Epoch:  867/1000  Loss: 4.242840002775193

Epoch:  868/1000  Loss: 4.2300709359189295

Epoch:  868/1000  Loss: 4.238955169916153

Epoch:  869/1000  Loss: 4.228929347180306

Epoch:  869/1000  Loss: 4.24471519947052

Epoch:  870/1000  Loss: 4.223680608830554

Epoch:  870/1000  Loss: 4.240538280010224

Epoch:  871/1000  Loss: 4.233981690508254

Epoch:  871/1000  Loss: 4.240076681375504

Epoch:  872/1000  Loss: 4.228452581040402

Epoch:  872/1000  Loss: 4.243031740188599

Epoch:  873/1000  Loss: 4.22978260466393

Epoch:  873/1000  Loss: 4.243968377113342

Epoch:  874/1000  Loss: 4.224180058215527

Epoch:  874/1000  Loss: 4.2477171516418455

Epoch:  875/1000  Loss: 4.228738469265877

Epoch:  875/1000  Loss: 4.243226581811905

Epoch:  876/1000  Loss: 4.229845996613198

Epoch:  876/1000  Loss: 4.251406635046005

Epoch:  877/1000  Loss: 4.232549142837525

Epoch:  877/1000  Loss: 4.242992359399795

Epoch:  878/1000  Loss: 4.231218885868154

Epoch:  878/1000  Loss: 4.240310497283936

Epoch:  879/1000  Loss: 4.224451499289654

Epoch:  879/1000  Loss: 4.237640894651413

Epoch:  880/1000  Loss: 4.234633918518716

Epoch:  880/1000  Loss: 4.242412922382354

Epoch:  881/1000  Loss: 4.234890136312931

Epoch:  881/1000  Loss: 4.244538196325302

Epoch:  882/1000  Loss: 4.232509024599765

Epoch:  882/1000  Loss: 4.2367826688289645

Epoch:  883/1000  Loss: 4.222957885011714

Epoch:  883/1000  Loss: 4.2369311702251435

Epoch:  884/1000  Loss: 4.22319025688983

Epoch:  884/1000  Loss: 4.245561497211456

Epoch:  885/1000  Loss: 4.22362657202051

Epoch:  885/1000  Loss: 4.2405714094638824

Epoch:  886/1000  Loss: 4.219949543729742

Epoch:  886/1000  Loss: 4.240482949018478

Epoch:  887/1000  Loss: 4.228876884947431

Epoch:  887/1000  Loss: 4.2417962670326235

Epoch:  888/1000  Loss: 4.22604118002222

Epoch:  888/1000  Loss: 4.240865380764007

Epoch:  889/1000  Loss: 4.22716122383767

Epoch:  889/1000  Loss: 4.233406865596772

Epoch:  890/1000  Loss: 4.235066062846082

Epoch:  890/1000  Loss: 4.24393804192543

Epoch:  891/1000  Loss: 4.229945887910559

Epoch:  891/1000  Loss: 4.240436928272247

Epoch:  892/1000  Loss: 4.226218393001151

Epoch:  892/1000  Loss: 4.239425156116486

Epoch:  893/1000  Loss: 4.222919402223952

Epoch:  893/1000  Loss: 4.235343981981277

Epoch:  894/1000  Loss: 4.223112735342472

Epoch:  894/1000  Loss: 4.245170106887818

Epoch:  895/1000  Loss: 4.218735545746823

Epoch:  895/1000  Loss: 4.238830440044403

Epoch:  896/1000  Loss: 4.222605489162689

Epoch:  896/1000  Loss: 4.241245720386505

Epoch:  897/1000  Loss: 4.218360666518516

Epoch:  897/1000  Loss: 4.238555327653885

Epoch:  898/1000  Loss: 4.223728579663216

Epoch:  898/1000  Loss: 4.232848818302155

Epoch:  899/1000  Loss: 4.223349559053462

Epoch:  899/1000  Loss: 4.241564111709595

Epoch:  900/1000  Loss: 4.215146678559323

Epoch:  900/1000  Loss: 4.232075031995773

Epoch:  901/1000  Loss: 4.224967036348708

Epoch:  901/1000  Loss: 4.235049647092819

Epoch:  902/1000  Loss: 4.216396108586737

Epoch:  902/1000  Loss: 4.234232689142227

Epoch:  903/1000  Loss: 4.221732639759145

Epoch:  903/1000  Loss: 4.237084522247314

Epoch:  904/1000  Loss: 4.2189337111533955

</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_text output_error">
<pre>
<span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">KeyboardInterrupt</span>                         Traceback (most recent call last)
<span class="ansi-green-fg">&lt;ipython-input-32-08f0cb61c5e7&gt;</span> in <span class="ansi-cyan-fg">&lt;module&gt;</span><span class="ansi-blue-fg">()</span>
<span class="ansi-green-intense-fg ansi-bold">     18</span> 
<span class="ansi-green-intense-fg ansi-bold">     19</span>     <span class="ansi-red-fg"># training the model</span>
<span class="ansi-green-fg">---&gt; 20</span><span class="ansi-red-fg">     </span>trained_rnn <span class="ansi-blue-fg">=</span> train_rnn<span class="ansi-blue-fg">(</span>rnn<span class="ansi-blue-fg">,</span> batch_size<span class="ansi-blue-fg">,</span> optimizer<span class="ansi-blue-fg">,</span> criterion<span class="ansi-blue-fg">,</span> num_epochs<span class="ansi-blue-fg">,</span> show_every_n_batches<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">     21</span> 
<span class="ansi-green-intense-fg ansi-bold">     22</span>     <span class="ansi-red-fg"># saving the trained model</span>

<span class="ansi-green-fg">&lt;ipython-input-16-54bd9dc2f859&gt;</span> in <span class="ansi-cyan-fg">train_rnn</span><span class="ansi-blue-fg">(rnn, batch_size, optimizer, criterion, n_epochs, show_every_n_batches)</span>
<span class="ansi-green-intense-fg ansi-bold">     22</span> 
<span class="ansi-green-intense-fg ansi-bold">     23</span>             <span class="ansi-red-fg"># forward, back prop</span>
<span class="ansi-green-fg">---&gt; 24</span><span class="ansi-red-fg">             </span>loss<span class="ansi-blue-fg">,</span> hidden <span class="ansi-blue-fg">=</span> forward_back_prop<span class="ansi-blue-fg">(</span>rnn<span class="ansi-blue-fg">,</span> optimizer<span class="ansi-blue-fg">,</span> criterion<span class="ansi-blue-fg">,</span> inputs<span class="ansi-blue-fg">,</span> labels<span class="ansi-blue-fg">,</span> hidden<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">     25</span>             <span class="ansi-red-fg"># record loss</span>
<span class="ansi-green-intense-fg ansi-bold">     26</span>             batch_losses<span class="ansi-blue-fg">.</span>append<span class="ansi-blue-fg">(</span>loss<span class="ansi-blue-fg">)</span>

<span class="ansi-green-fg">&lt;ipython-input-15-c302a43168d3&gt;</span> in <span class="ansi-cyan-fg">forward_back_prop</span><span class="ansi-blue-fg">(rnn, optimizer, criterion, inp, target, hidden)</span>
<span class="ansi-green-intense-fg ansi-bold">     26</span>     <span class="ansi-red-fg"># calculate the loss and perform backprop</span>
<span class="ansi-green-intense-fg ansi-bold">     27</span>     loss <span class="ansi-blue-fg">=</span> criterion<span class="ansi-blue-fg">(</span>output<span class="ansi-blue-fg">.</span>squeeze<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">,</span> target<span class="ansi-blue-fg">.</span>long<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">)</span>
<span class="ansi-green-fg">---&gt; 28</span><span class="ansi-red-fg">     </span>loss<span class="ansi-blue-fg">.</span>backward<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">     29</span>     loss<span class="ansi-blue-fg">=</span> loss<span class="ansi-blue-fg">.</span>item<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">     30</span>     optimizer<span class="ansi-blue-fg">.</span>step<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span>

<span class="ansi-green-fg">/opt/conda/lib/python3.6/site-packages/torch/tensor.py</span> in <span class="ansi-cyan-fg">backward</span><span class="ansi-blue-fg">(self, gradient, retain_graph, create_graph)</span>
<span class="ansi-green-intense-fg ansi-bold">     91</span>                 products<span class="ansi-blue-fg">.</span> Defaults to<span class="ansi-red-fg"> </span><span class="ansi-red-fg">`</span><span class="ansi-red-fg">`</span><span class="ansi-green-fg">False</span><span class="ansi-red-fg">`</span><span class="ansi-red-fg">`</span><span class="ansi-blue-fg">.</span>
<span class="ansi-green-intense-fg ansi-bold">     92</span>         &#34;&#34;&#34;
<span class="ansi-green-fg">---&gt; 93</span><span class="ansi-red-fg">         </span>torch<span class="ansi-blue-fg">.</span>autograd<span class="ansi-blue-fg">.</span>backward<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">,</span> gradient<span class="ansi-blue-fg">,</span> retain_graph<span class="ansi-blue-fg">,</span> create_graph<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">     94</span> 
<span class="ansi-green-intense-fg ansi-bold">     95</span>     <span class="ansi-green-fg">def</span> register_hook<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">,</span> hook<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">:</span>

<span class="ansi-green-fg">/opt/conda/lib/python3.6/site-packages/torch/autograd/__init__.py</span> in <span class="ansi-cyan-fg">backward</span><span class="ansi-blue-fg">(tensors, grad_tensors, retain_graph, create_graph, grad_variables)</span>
<span class="ansi-green-intense-fg ansi-bold">     87</span>     Variable._execution_engine.run_backward(
<span class="ansi-green-intense-fg ansi-bold">     88</span>         tensors<span class="ansi-blue-fg">,</span> grad_tensors<span class="ansi-blue-fg">,</span> retain_graph<span class="ansi-blue-fg">,</span> create_graph<span class="ansi-blue-fg">,</span>
<span class="ansi-green-fg">---&gt; 89</span><span class="ansi-red-fg">         allow_unreachable=True)  # allow_unreachable flag
</span><span class="ansi-green-intense-fg ansi-bold">     90</span> 
<span class="ansi-green-intense-fg ansi-bold">     91</span> 

<span class="ansi-red-fg">KeyboardInterrupt</span>: </pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Question:-How-did-you-decide-on-your-model-hyperparameters?">Question: How did you decide on your model hyperparameters?<a class="anchor-link" href="#Question:-How-did-you-decide-on-your-model-hyperparameters?">&#182;</a></h3><p>For example, did you try different sequence_lengths and find that one size made the model converge faster? What about your hidden_dim and n_layers; how did you decide on those?</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Answer:</strong> (Write answer, here)</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<h1 id="Checkpoint">Checkpoint<a class="anchor-link" href="#Checkpoint">&#182;</a></h1><p>After running the above training cell, your model will be saved by name, <code>trained_rnn</code>, and if you save your notebook progress, <strong>you can pause here and come back to this code at another time</strong>. You can resume your progress by running the next cell, which will load in our word:id dictionaries <em>and</em> load in your saved model by name!</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[17]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">DON&#39;T MODIFY ANYTHING IN THIS CELL</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">helper</span>
<span class="kn">import</span> <span class="nn">problem_unittests</span> <span class="k">as</span> <span class="nn">tests</span>

<span class="n">_</span><span class="p">,</span> <span class="n">vocab_to_int</span><span class="p">,</span> <span class="n">int_to_vocab</span><span class="p">,</span> <span class="n">token_dict</span> <span class="o">=</span> <span class="n">helper</span><span class="o">.</span><span class="n">load_preprocess</span><span class="p">()</span>
<span class="n">trained_rnn</span> <span class="o">=</span> <span class="n">helper</span><span class="o">.</span><span class="n">load_model</span><span class="p">(</span><span class="s1">&#39;./save/trained_rnn&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Generate-TV-Script">Generate TV Script<a class="anchor-link" href="#Generate-TV-Script">&#182;</a></h2><p>With the network trained and saved, you'll use it to generate a new, "fake" Seinfeld TV script in this section.</p>
<h3 id="Generate-Text">Generate Text<a class="anchor-link" href="#Generate-Text">&#182;</a></h3><p>To generate the text, the network needs to start with a single word and repeat its predictions until it reaches a set length. You'll be using the <code>generate</code> function to do this. It takes a word id to start with, <code>prime_id</code>, and generates a set length of text, <code>predict_len</code>. Also note that it uses topk sampling to introduce some randomness in choosing the most likely next word, given an output set of word scores!</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[18]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">DON&#39;T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>

<span class="k">def</span> <span class="nf">generate</span><span class="p">(</span><span class="n">rnn</span><span class="p">,</span> <span class="n">prime_id</span><span class="p">,</span> <span class="n">int_to_vocab</span><span class="p">,</span> <span class="n">token_dict</span><span class="p">,</span> <span class="n">pad_value</span><span class="p">,</span> <span class="n">predict_len</span><span class="o">=</span><span class="mi">100</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Generate text using the neural network</span>
<span class="sd">    :param decoder: The PyTorch Module that holds the trained neural network</span>
<span class="sd">    :param prime_id: The word id to start the first prediction</span>
<span class="sd">    :param int_to_vocab: Dict of word id keys to word values</span>
<span class="sd">    :param token_dict: Dict of puncuation tokens keys to puncuation values</span>
<span class="sd">    :param pad_value: The value used to pad a sequence</span>
<span class="sd">    :param predict_len: The length of text to generate</span>
<span class="sd">    :return: The generated text</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">rnn</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
    
    <span class="c1"># create a sequence (batch_size=1) with the prime_id</span>
    <span class="n">current_seq</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">),</span> <span class="n">pad_value</span><span class="p">)</span>
    <span class="n">current_seq</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">prime_id</span>
    <span class="n">predicted</span> <span class="o">=</span> <span class="p">[</span><span class="n">int_to_vocab</span><span class="p">[</span><span class="n">prime_id</span><span class="p">]]</span>
    
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">predict_len</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">train_on_gpu</span><span class="p">:</span>
            <span class="n">current_seq</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">(</span><span class="n">current_seq</span><span class="p">)</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">current_seq</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">(</span><span class="n">current_seq</span><span class="p">)</span>
        
        <span class="c1"># initialize the hidden state</span>
        <span class="n">hidden</span> <span class="o">=</span> <span class="n">rnn</span><span class="o">.</span><span class="n">init_hidden</span><span class="p">(</span><span class="n">current_seq</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
        
        <span class="c1"># get the output of the rnn</span>
        <span class="n">output</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">rnn</span><span class="p">(</span><span class="n">current_seq</span><span class="p">,</span> <span class="n">hidden</span><span class="p">)</span>
        
        <span class="c1"># get the next word probabilities</span>
        <span class="n">p</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">data</span>
        <span class="k">if</span><span class="p">(</span><span class="n">train_on_gpu</span><span class="p">):</span>
            <span class="n">p</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span> <span class="c1"># move to cpu</span>
         
        <span class="c1"># use top_k sampling to get the index of the next word</span>
        <span class="n">top_k</span> <span class="o">=</span> <span class="mi">5</span>
        <span class="n">p</span><span class="p">,</span> <span class="n">top_i</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">topk</span><span class="p">(</span><span class="n">top_k</span><span class="p">)</span>
        <span class="n">top_i</span> <span class="o">=</span> <span class="n">top_i</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span>
        
        <span class="c1"># select the likely next word index with some element of randomness</span>
        <span class="n">p</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span>
        <span class="n">word_i</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">top_i</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">p</span><span class="o">/</span><span class="n">p</span><span class="o">.</span><span class="n">sum</span><span class="p">())</span>
        
        <span class="c1"># retrieve that word from the dictionary</span>
        <span class="n">word</span> <span class="o">=</span> <span class="n">int_to_vocab</span><span class="p">[</span><span class="n">word_i</span><span class="p">]</span>
        <span class="n">predicted</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">word</span><span class="p">)</span>     
        
        <span class="c1"># the generated word becomes the next &quot;current sequence&quot; and the cycle can continue</span>
        <span class="n">current_seq</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">roll</span><span class="p">(</span><span class="n">current_seq</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">current_seq</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">word_i</span>
    
    <span class="n">gen_sentences</span> <span class="o">=</span> <span class="s1">&#39; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">predicted</span><span class="p">)</span>
    
    <span class="c1"># Replace punctuation tokens</span>
    <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">token_dict</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="n">ending</span> <span class="o">=</span> <span class="s1">&#39; &#39;</span> <span class="k">if</span> <span class="n">key</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span> <span class="s1">&#39;(&#39;</span><span class="p">,</span> <span class="s1">&#39;&quot;&#39;</span><span class="p">]</span> <span class="k">else</span> <span class="s1">&#39;&#39;</span>
        <span class="n">gen_sentences</span> <span class="o">=</span> <span class="n">gen_sentences</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s1">&#39; &#39;</span> <span class="o">+</span> <span class="n">token</span><span class="o">.</span><span class="n">lower</span><span class="p">(),</span> <span class="n">key</span><span class="p">)</span>
    <span class="n">gen_sentences</span> <span class="o">=</span> <span class="n">gen_sentences</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1"> &#39;</span><span class="p">,</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="n">gen_sentences</span> <span class="o">=</span> <span class="n">gen_sentences</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s1">&#39;( &#39;</span><span class="p">,</span> <span class="s1">&#39;(&#39;</span><span class="p">)</span>
    
    <span class="c1"># return all the sentences</span>
    <span class="k">return</span> <span class="n">gen_sentences</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Generate-a-New-Script">Generate a New Script<a class="anchor-link" href="#Generate-a-New-Script">&#182;</a></h3><p>It's time to generate the text. Set <code>gen_length</code> to the length of TV script you want to generate and set <code>prime_word</code> to one of the following to start the prediction:</p>
<ul>
<li>"jerry"</li>
<li>"elaine"</li>
<li>"george"</li>
<li>"kramer"</li>
</ul>
<p>You can set the prime word to <em>any word</em> in our dictionary, but it's best to start with a name for generating a TV script. (You can also start with any other names you find in the original text file!)</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[27]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># run the cell multiple times to get different results!</span>
<span class="n">gen_length</span> <span class="o">=</span> <span class="mi">400</span> <span class="c1"># modify the length to your preference</span>
<span class="n">prime_word</span> <span class="o">=</span> <span class="s1">&#39;jerry&#39;</span> <span class="c1"># name for starting the script</span>

<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">DON&#39;T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE</span>

<span class="sd">&quot;&quot;&quot;</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;sequence_length: </span><span class="si">{}</span><span class="se">\n</span><span class="s1">batch_size: </span><span class="si">{}</span><span class="se">\n</span><span class="s1">learning_rate: </span><span class="si">{}</span><span class="se">\n</span><span class="s1">embedding_dim: </span><span class="si">{}</span><span class="se">\n</span><span class="s1">n_layers: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">sequence_length</span><span class="p">,</span><span class="n">batch_size</span><span class="p">,</span><span class="n">learning_rate</span><span class="p">,</span><span class="n">embedding_dim</span><span class="p">,</span><span class="n">n_layers</span><span class="p">))</span>

<span class="n">pad_word</span> <span class="o">=</span> <span class="n">helper</span><span class="o">.</span><span class="n">SPECIAL_WORDS</span><span class="p">[</span><span class="s1">&#39;PADDING&#39;</span><span class="p">]</span>
<span class="n">generated_script</span> <span class="o">=</span> <span class="n">generate</span><span class="p">(</span><span class="n">trained_rnn</span><span class="p">,</span> <span class="n">vocab_to_int</span><span class="p">[</span><span class="n">prime_word</span> <span class="o">+</span> <span class="s1">&#39;:&#39;</span><span class="p">],</span> <span class="n">int_to_vocab</span><span class="p">,</span> <span class="n">token_dict</span><span class="p">,</span> <span class="n">vocab_to_int</span><span class="p">[</span><span class="n">pad_word</span><span class="p">],</span> <span class="n">gen_length</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">generated_script</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>sequence_length: 16
batch_size: 64
learning_rate: 0.001
embedding_dim: 300
n_layers: 2
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stderr output_text">
<pre>/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:44: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>jerry: george&#39;s pretty thought very if do don&#39;t with look one your at have of george: very come can&#39;t see that me no) in me of a a in of!? i,?,, i?? jerry: thought one your your- what that me of!,?,?? you i,,,?? jerry: turns her go do he he i&#39;m) george: water think) george: alright too if do don&#39;t that of a in what of george: ready talk these her we),,,? jerry: thought back was-!,,?? you, you you, i,? you? you,?? i,,, i,?,,, i,, i you i, i,,?,, jerry: when hey)! you jerry:!,?,? you jerry:!? i jerry: jerry he well well i&#39;m he my) george: alright one your do like all he)!, you i you, you? i,,, i?, jerry: alright very come one do- kramer: thought could hey)!, i, i,?, jerry: alright back are that me of a in of a a! you you jerry: of george: thought very he&#39;s he&#39;s see yeah- of a in no do don&#39;t that no know don&#39;t that of! jerry: kramer: what with are kramer: kramer: kramer: me of!,,,,??,?? i, jerry: very that&#39;s do don&#39;t with was well) in no do- no hey do don&#39;t that kramer: me no not- kramer: alright if do don&#39;t for have that of a! i, i?,? i, i you,,,, i?,,?, i,,? jerry: turns are that kramer: what with kramer if hey do don&#39;t with was-!,,,? i i?,? you, you, jerry: alright see kramer: with look back are kramer: kramer: of george: ready talk are that kramer: kramer: kramer: of!, jerry: very come come one at have that of!,?,? i
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[25]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">sequence_length: 16</span>
<span class="sd">batch_size: 256</span>
<span class="sd">learning_rate: 0.001</span>
<span class="sd">embedding_dim: 300</span>
<span class="sd">n_layers: 2</span>
<span class="sd">Training for 300 epoch(s)</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;sequence_length: </span><span class="si">{}</span><span class="se">\n</span><span class="s1">batch_size: </span><span class="si">{}</span><span class="se">\n</span><span class="s1">learning_rate: </span><span class="si">{}</span><span class="se">\n</span><span class="s1">embedding_dim: </span><span class="si">{}</span><span class="se">\n</span><span class="s1">n_layers: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">sequence_length</span><span class="p">,</span><span class="n">batch_size</span><span class="p">,</span><span class="n">learning_rate</span><span class="p">,</span><span class="n">embedding_dim</span><span class="p">,</span><span class="n">n_layers</span><span class="p">))</span>

<span class="c1"># run the cell multiple times to get different results!</span>
<span class="n">gen_length</span> <span class="o">=</span> <span class="mi">400</span> <span class="c1"># modify the length to your preference</span>
<span class="n">prime_word</span> <span class="o">=</span> <span class="s1">&#39;jerry&#39;</span> <span class="c1"># name for starting the script</span>

<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">DON&#39;T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="n">pad_word</span> <span class="o">=</span> <span class="n">helper</span><span class="o">.</span><span class="n">SPECIAL_WORDS</span><span class="p">[</span><span class="s1">&#39;PADDING&#39;</span><span class="p">]</span>
<span class="n">generated_script</span> <span class="o">=</span> <span class="n">generate</span><span class="p">(</span><span class="n">trained_rnn</span><span class="p">,</span> <span class="n">vocab_to_int</span><span class="p">[</span><span class="n">prime_word</span> <span class="o">+</span> <span class="s1">&#39;:&#39;</span><span class="p">],</span> <span class="n">int_to_vocab</span><span class="p">,</span> <span class="n">token_dict</span><span class="p">,</span> <span class="n">vocab_to_int</span><span class="p">[</span><span class="n">pad_word</span><span class="p">],</span> <span class="n">gen_length</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">generated_script</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>sequence_length: 16
batch_size: 64
learning_rate: 0.001
embedding_dim: 300
n_layers: 2
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stderr output_text">
<pre>/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:44: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>jerry: project glasses george&#39;s george&#39;s pretty thought very come he&#39;s one do don&#39;t for have yeah he well) a a a!??, you,,, you, jerry: alright one-! i you,, you i i jerry: me of george: ready these one&#34; no hey know on no hey) a! jerry: kramer: of george: ready talk these her have of george: few could do don&#39;t for have kramer: no know don&#39;t for have kramer: kramer: kramer: me what me of a!,,?? i jerry: for have that what that of!,,??, i,? you jerry: is he well)!?? i?,?, you?,,?,?,?,,? you, you,,, jerry: very that&#39;s) george: alright very see yeah he he well i&#39;m) a is like go&#34; no) in me no do- what that kramer: kramer: what me what with are of!?,?,,?,,,,? jerry: turns could know like george don&#39;t kramer: that kramer: me no) george: thought when know- of george: glasses george: alone was) in what kramer: me what that kramer: kramer: me me of!??,,, i,? i,?, you you you you??? i you,, you,?? jerry: water her have with are kramer: kramer: me no not on kramer: kramer: what kramer: what kramer: kramer: of a in kramer: a a a a a a!?, you i,,,,? jerry: alright very come back was he on no do don&#39;t that kramer: me what with kramer if not don&#39;t for all don&#39;t with kramer if do) a a a!,, i i,? jerry: turns are me kramer: kramer: what me kramer:! jerry: of a in no do- kramer:) george: would know on no) a a in what me kramer: of in!,,, jerry: thought too could) a george: thought very see that of george: thought her have for george he on kramer: kramer: what me what kramer: me no do
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[20]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># run the cell multiple times to get different results!</span>
<span class="n">gen_length</span> <span class="o">=</span> <span class="mi">400</span> <span class="c1"># modify the length to your preference</span>
<span class="n">prime_word</span> <span class="o">=</span> <span class="s1">&#39;jerry&#39;</span> <span class="c1"># name for starting the script</span>

<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">DON&#39;T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE</span>

<span class="sd">&quot;&quot;&quot;</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;sequence_length: </span><span class="si">{}</span><span class="se">\n</span><span class="s1">batch_size: </span><span class="si">{}</span><span class="se">\n</span><span class="s1">learning_rate: </span><span class="si">{}</span><span class="se">\n</span><span class="s1">embedding_dim: </span><span class="si">{}</span><span class="se">\n</span><span class="s1">n_layers: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">sequence_length</span><span class="p">,</span><span class="n">batch_size</span><span class="p">,</span><span class="n">learning_rate</span><span class="p">,</span><span class="n">embedding_dim</span><span class="p">,</span><span class="n">n_layers</span><span class="p">))</span>

<span class="n">pad_word</span> <span class="o">=</span> <span class="n">helper</span><span class="o">.</span><span class="n">SPECIAL_WORDS</span><span class="p">[</span><span class="s1">&#39;PADDING&#39;</span><span class="p">]</span>
<span class="n">generated_script</span> <span class="o">=</span> <span class="n">generate</span><span class="p">(</span><span class="n">trained_rnn</span><span class="p">,</span> <span class="n">vocab_to_int</span><span class="p">[</span><span class="n">prime_word</span> <span class="o">+</span> <span class="s1">&#39;:&#39;</span><span class="p">],</span> <span class="n">int_to_vocab</span><span class="p">,</span> <span class="n">token_dict</span><span class="p">,</span> <span class="n">vocab_to_int</span><span class="p">[</span><span class="n">pad_word</span><span class="p">],</span> <span class="n">gen_length</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">generated_script</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>sequence_length: 16
batch_size: 64
learning_rate: 0.0001
embedding_dim: 300
n_layers: 2
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stderr output_text">
<pre>/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:44: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>jerry: project glasses architect architect pretty alright could know he he don&#39;t for have that me kramer: no know don&#39;t kramer: kramer: no know he) in of!,,, you,? jerry: water her go&#34; kramer: of! you?, jerry: would she your- kramer: george&#39;s was don&#39;t with kramer back are of! you? jerry: very he&#39;s come can&#39;t one at go they know- kramer:-!? you i jerry: what yeah- kramer: he my hey) a in of!? i?, i,,??,, jerry: when know he) george: water think do like go at all he) a a george: alright very gonna do don&#39;t that what that me what that no) a george: very see with was he) a george: thought very come one do don&#39;t with look one at have me kramer: kramer: me kramer: of! you i,, jerry: when know don&#39;t with look one do he don&#39;t yeah on me of a in of a a a a a!?,, i, jerry: would) a george: very he&#39;s gonna they know like george) in of!? i i?? jerry: thought one do- kramer: glasses talk thing if hey not-!,, you,, i,, you??, i,,, i,,?,?? i jerry: that kramer: what me! you you,,, you,,,,,?, you,,,? i i,?,?, you?,??, you,??? i, you,,,,, you,,? you, i,, jerry: would hey hey know- of george: few alright could not like have that what of!,, jerry: alright back kramer if do he) george: water alright back look one&#34; no hey know he don&#39;t for go at have that kramer: that of george: only one at out are me no) george: very see with kramer back are that of a a a a!?, you? jerry: would know he he well) a a a a a a
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[19]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># run the cell multiple times to get different results!</span>
<span class="n">gen_length</span> <span class="o">=</span> <span class="mi">400</span> <span class="c1"># modify the length to your preference</span>
<span class="n">prime_word</span> <span class="o">=</span> <span class="s1">&#39;jerry&#39;</span> <span class="c1"># name for starting the script</span>

<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">DON&#39;T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE</span>

<span class="sd">&quot;&quot;&quot;</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;sequence_length: </span><span class="si">{}</span><span class="se">\n</span><span class="s1">batch_size: </span><span class="si">{}</span><span class="se">\n</span><span class="s1">learning_rate: </span><span class="si">{}</span><span class="se">\n</span><span class="s1">embedding_dim: </span><span class="si">{}</span><span class="se">\n</span><span class="s1">n_layers: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">sequence_length</span><span class="p">,</span><span class="n">batch_size</span><span class="p">,</span><span class="n">learning_rate</span><span class="p">,</span><span class="n">embedding_dim</span><span class="p">,</span><span class="n">n_layers</span><span class="p">))</span>

<span class="n">pad_word</span> <span class="o">=</span> <span class="n">helper</span><span class="o">.</span><span class="n">SPECIAL_WORDS</span><span class="p">[</span><span class="s1">&#39;PADDING&#39;</span><span class="p">]</span>
<span class="n">generated_script</span> <span class="o">=</span> <span class="n">generate</span><span class="p">(</span><span class="n">trained_rnn</span><span class="p">,</span> <span class="n">vocab_to_int</span><span class="p">[</span><span class="n">prime_word</span> <span class="o">+</span> <span class="s1">&#39;:&#39;</span><span class="p">],</span> <span class="n">int_to_vocab</span><span class="p">,</span> <span class="n">token_dict</span><span class="p">,</span> <span class="n">vocab_to_int</span><span class="p">[</span><span class="n">pad_word</span><span class="p">],</span> <span class="n">gen_length</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">generated_script</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>sequence_length: 8
batch_size: 256
learning_rate: 0.0001
embedding_dim: 1000
n_layers: 3
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stderr output_text">
<pre>/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:44: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>jerry: george&#39;s pretty her do he well don&#39;t yeah on me no) george: water could not-!,? jerry: water could do) a a is we know don&#39;t with kramer her out kramer if know- no) a a a a a is like have kramer: what that of in kramer:!?, jerry: would do he) a a! you,? you, i you you i you, i?,,,,,,, jerry: would)!,, i,? jerry: us thing uh) george: water think) george: would one they)!? you,,??,? jerry: turns was well he)!?,?, you i,, i you you i,? jerry: thought one do- kramer: thought could) a george: thought her have for! i,,? i,,? jerry: turns was)!, i you,???,,, jerry: would know- kramer:) in of george: alright one at all)! i, i jerry: so yeah like have that what of!?,,, jerry: went sorry could know he)!, i jerry: so for go they hey) george: don&#39;t for all)! jerry: of in kramer: what me of!, jerry: thought too think&#34; no hey do he on kramer: of is don&#39;t me no do he he) george: thought too are that of! i i, jerry: alright see yeah- no not- of! i,,, jerry: very see with was)! i jerry: for have of george: few she&#39;s back was he he)!,???, jerry: went sorry her we do don&#39;t kramer: kramer: what me what of a in kramer:!,, you,?? i? i jerry: for out are for!?? jerry: alright one- kramer: thought could do- kramer: thought could know about- kramer: water could do-! i you i?? jerry: nose talk are of! jerry: that kramer: of a!? i i i,?,, jerry: when hey do- of!?,
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[54]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># run the cell multiple times to get different results!</span>
<span class="n">gen_length</span> <span class="o">=</span> <span class="mi">400</span> <span class="c1"># modify the length to your preference</span>
<span class="n">prime_word</span> <span class="o">=</span> <span class="s1">&#39;jerry&#39;</span> <span class="c1"># name for starting the script</span>

<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">DON&#39;T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE</span>

<span class="sd">&quot;&quot;&quot;</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;sequence_length: </span><span class="si">{}</span><span class="se">\n</span><span class="s1">batch_size: </span><span class="si">{}</span><span class="se">\n</span><span class="s1">learning_rate: </span><span class="si">{}</span><span class="se">\n</span><span class="s1">embedding_dim: </span><span class="si">{}</span><span class="se">\n</span><span class="s1">n_layers: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">sequence_length</span><span class="p">,</span><span class="n">batch_size</span><span class="p">,</span><span class="n">learning_rate</span><span class="p">,</span><span class="n">embedding_dim</span><span class="p">,</span><span class="n">n_layers</span><span class="p">))</span>

<span class="n">pad_word</span> <span class="o">=</span> <span class="n">helper</span><span class="o">.</span><span class="n">SPECIAL_WORDS</span><span class="p">[</span><span class="s1">&#39;PADDING&#39;</span><span class="p">]</span>
<span class="n">generated_script</span> <span class="o">=</span> <span class="n">generate</span><span class="p">(</span><span class="n">trained_rnn</span><span class="p">,</span> <span class="n">vocab_to_int</span><span class="p">[</span><span class="n">prime_word</span> <span class="o">+</span> <span class="s1">&#39;:&#39;</span><span class="p">],</span> <span class="n">int_to_vocab</span><span class="p">,</span> <span class="n">token_dict</span><span class="p">,</span> <span class="n">vocab_to_int</span><span class="p">[</span><span class="n">pad_word</span><span class="p">],</span> <span class="n">gen_length</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">generated_script</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>sequence_length: 8
batch_size: 256
learning_rate: 0.0001
embedding_dim: 1000
n_layers: 2
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stderr output_text">
<pre>/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:44: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>jerry: project alright alright would one at george so that kramer: me me kramer: what kramer: me no do-!? you,, i? you you,, you,,,,,,, jerry: alright back where think- what that kramer: of! you i, i,, jerry: alright he&#39;s gonna do) george: so yeah-!? you??,, i you, i jerry: well) a! you?? i i you,, you jerry:!?, i, i?,? jerry: thought back was don&#39;t kramer: of!? you you?,, jerry: would that&#39;s hey know- no do- what that me of! you, you, you?,,,? you,, i?,? i?, i? you you you? i jerry: yeah he he) george: thought when that&#39;s) a! you,,,,? jerry: thought could) a george: thought could know on what that of a! jerry: kramer: kramer: no know like out look her all) a!? i?? jerry: water think know don&#39;t for go they hey hey)! you?, jerry: went very gonna good her all he) a!,,?, you??, you jerry: a a!,?? you you i?,,?, i, i,???,, i,, jerry: would) george: water her go&#34; no hey not about- no do- kramer: have with kramer one&#34; me me me no do he don&#39;t yeah he he i&#39;m well i&#39;m on of!?, you,,??, you, jerry: alright he&#39;s he&#39;s gonna good back was) george: very see with was)! you,?,, you jerry:! i,, you, you?, i i?,?,? i?,,? i i jerry: that kramer: of!,?,?,,??, i i you i i you i you, i you?, i jerry:)! jerry: what kramer: kramer: no do on what
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[51]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># run the cell multiple times to get different results!</span>
<span class="n">gen_length</span> <span class="o">=</span> <span class="mi">400</span> <span class="c1"># modify the length to your preference</span>
<span class="n">prime_word</span> <span class="o">=</span> <span class="s1">&#39;jerry&#39;</span> <span class="c1"># name for starting the script</span>

<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">DON&#39;T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE</span>

<span class="sd">&quot;&quot;&quot;</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;sequence_length: </span><span class="si">{}</span><span class="se">\n</span><span class="s1">batch_size: </span><span class="si">{}</span><span class="se">\n</span><span class="s1">learning_rate: </span><span class="si">{}</span><span class="se">\n</span><span class="s1">embedding_dim: </span><span class="si">{}</span><span class="se">\n</span><span class="s1">n_layers: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">sequence_length</span><span class="p">,</span><span class="n">batch_size</span><span class="p">,</span><span class="n">learning_rate</span><span class="p">,</span><span class="n">embedding_dim</span><span class="p">,</span><span class="n">n_layers</span><span class="p">))</span>

<span class="n">pad_word</span> <span class="o">=</span> <span class="n">helper</span><span class="o">.</span><span class="n">SPECIAL_WORDS</span><span class="p">[</span><span class="s1">&#39;PADDING&#39;</span><span class="p">]</span>
<span class="n">generated_script</span> <span class="o">=</span> <span class="n">generate</span><span class="p">(</span><span class="n">trained_rnn</span><span class="p">,</span> <span class="n">vocab_to_int</span><span class="p">[</span><span class="n">prime_word</span> <span class="o">+</span> <span class="s1">&#39;:&#39;</span><span class="p">],</span> <span class="n">int_to_vocab</span><span class="p">,</span> <span class="n">token_dict</span><span class="p">,</span> <span class="n">vocab_to_int</span><span class="p">[</span><span class="n">pad_word</span><span class="p">],</span> <span class="n">gen_length</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">generated_script</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>sequence_length: 8
batch_size: 256
learning_rate: 0.0001
embedding_dim: 300
n_layers: 2
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stderr output_text">
<pre>/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:44: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>jerry: george&#39;s alone alright thought very back kramer her out are kramer: of a a a a a a! i i,? i i,,,?, you,,, jerry: when do on of a!? i??? i you,, you, you, i, you,, i??? i? i,,,,,,?? jerry: alright her- no know like george) george: he) in of a is on of in kramer: kramer: what that kramer: what me kramer: kramer: what kramer: me me what me! jerry: kramer: me kramer: of a in of! i? jerry: turns are that no know about- what of a george: very see yeah like all don&#39;t with was don&#39;t that kramer: of a!??,,? you jerry:!?, jerry: very he&#39;s gonna your&#34; me of in kramer: what with are of!? you i,, you i jerry: what kramer: no know- no not- no not like out look one at go&#34; me what of a george: thought one do like go they know on what with was he he) george: water her all well well don&#39;t kramer: me me no) george: alright too could know like have that no hey know like out where was he well) in no not he well) in kramer: of in kramer:! jerry: kramer: kramer: of! you,? you? jerry: went her- kramer: alright back was- a! i?? jerry: water was), i jerry: well well my know he well well well well) a!,,, you i i jerry: jerry- no) a a a is- what of george: few thought could not like all don&#39;t that what kramer: that of in what that me kramer: no do-! jerry: kramer: me what me what that no) a is-!? i,?,,, i, i, i,,,,? i jerry: me kramer: what kramer: kramer: kramer: of a a in!,? jerry: turns are of a a a in of a a!,, jerry:
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[46]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># run the cell multiple times to get different results!</span>
<span class="n">gen_length</span> <span class="o">=</span> <span class="mi">400</span> <span class="c1"># modify the length to your preference</span>
<span class="n">prime_word</span> <span class="o">=</span> <span class="s1">&#39;jerry&#39;</span> <span class="c1"># name for starting the script</span>

<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">DON&#39;T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE</span>

<span class="sd">&quot;&quot;&quot;</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;sequence_length: </span><span class="si">{}</span><span class="se">\n</span><span class="s1">batch_size: </span><span class="si">{}</span><span class="se">\n</span><span class="s1">learning_rate: </span><span class="si">{}</span><span class="se">\n</span><span class="s1">embedding_dim: </span><span class="si">{}</span><span class="se">\n</span><span class="s1">n_layers: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">sequence_length</span><span class="p">,</span><span class="n">batch_size</span><span class="p">,</span><span class="n">learning_rate</span><span class="p">,</span><span class="n">embedding_dim</span><span class="p">,</span><span class="n">n_layers</span><span class="p">))</span>

<span class="n">pad_word</span> <span class="o">=</span> <span class="n">helper</span><span class="o">.</span><span class="n">SPECIAL_WORDS</span><span class="p">[</span><span class="s1">&#39;PADDING&#39;</span><span class="p">]</span>
<span class="n">generated_script</span> <span class="o">=</span> <span class="n">generate</span><span class="p">(</span><span class="n">trained_rnn</span><span class="p">,</span> <span class="n">vocab_to_int</span><span class="p">[</span><span class="n">prime_word</span> <span class="o">+</span> <span class="s1">&#39;:&#39;</span><span class="p">],</span> <span class="n">int_to_vocab</span><span class="p">,</span> <span class="n">token_dict</span><span class="p">,</span> <span class="n">vocab_to_int</span><span class="p">[</span><span class="n">pad_word</span><span class="p">],</span> <span class="n">gen_length</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">generated_script</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>sequence_length: 8
batch_size: 256
learning_rate: 0.0006
embedding_dim: 300
n_layers: 2
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stderr output_text">
<pre>/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:44: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>jerry: pretty thought alright very back are me kramer: of a george: alright could) a george: alright her- of a! jerry: of george: alright very come he&#39;s gonna your at have with was- of in what of a!, jerry: went back was) a is like all) in what of a a! you you, you,, you?? jerry: alright very one- of george: water are that kramer: me no know he)! you?????,?,, jerry: went back kramer if) george: thought her go they do- what kramer: me of!? you, jerry: thought when see for have kramer: what me kramer: of is we do on kramer: me what kramer: of!,?,, jerry: would she do don&#39;t for all) george:)! you jerry: a is don&#39;t kramer: me kramer: what that kramer: me me no not he on kramer:! i you, you you, you?,, jerry: very gonna your your at all well) george: alright one your do he on me kramer: of!??? i?, you i?,?,, you?, you?? you you you you?,, i,?,, jerry: would do- of!, you,,,, you, i?, you you, jerry: would know he so that me no know-! jerry: of in kramer:!? you, you you i,, jerry: very gonna your&#34; me of!, you you, i,???,?,, you? i jerry: for have of a!?, you jerry: a! jerry: what me no do don&#39;t for all don&#39;t yeah he don&#39;t for have that kramer: me kramer: no do- what that kramer: of a a george: alright one- kramer: alright back look think know on no know- of a in no know he well don&#39;t that kramer: of george: few she&#39;s one do he well)! you? i, jerry: thought very one- of george: water are that no hey know-!?,? jerry: turns are that of
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[41]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># run the cell multiple times to get different results!</span>
<span class="n">gen_length</span> <span class="o">=</span> <span class="mi">400</span> <span class="c1"># modify the length to your preference</span>
<span class="n">prime_word</span> <span class="o">=</span> <span class="s1">&#39;jerry&#39;</span> <span class="c1"># name for starting the script</span>

<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">DON&#39;T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE</span>

<span class="sd">&quot;&quot;&quot;</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;sequence_length: </span><span class="si">{}</span><span class="se">\n</span><span class="s1">batch_size: </span><span class="si">{}</span><span class="se">\n</span><span class="s1">learning_rate: </span><span class="si">{}</span><span class="se">\n</span><span class="s1">embedding_dim: </span><span class="si">{}</span><span class="se">\n</span><span class="s1">n_layers: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">sequence_length</span><span class="p">,</span><span class="n">batch_size</span><span class="p">,</span><span class="n">learning_rate</span><span class="p">,</span><span class="n">embedding_dim</span><span class="p">,</span><span class="n">n_layers</span><span class="p">))</span>

<span class="n">pad_word</span> <span class="o">=</span> <span class="n">helper</span><span class="o">.</span><span class="n">SPECIAL_WORDS</span><span class="p">[</span><span class="s1">&#39;PADDING&#39;</span><span class="p">]</span>
<span class="n">generated_script</span> <span class="o">=</span> <span class="n">generate</span><span class="p">(</span><span class="n">trained_rnn</span><span class="p">,</span> <span class="n">vocab_to_int</span><span class="p">[</span><span class="n">prime_word</span> <span class="o">+</span> <span class="s1">&#39;:&#39;</span><span class="p">],</span> <span class="n">int_to_vocab</span><span class="p">,</span> <span class="n">token_dict</span><span class="p">,</span> <span class="n">vocab_to_int</span><span class="p">[</span><span class="n">pad_word</span><span class="p">],</span> <span class="n">gen_length</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">generated_script</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>sequence_length: 8
batch_size: 256
learning_rate: 0.001
embedding_dim: 1000
n_layers: 2
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stderr output_text">
<pre>/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:44: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>jerry: george&#39;s alright alright sorry could not he don&#39;t for go do don&#39;t that of in what of george: water are with was- of a!,,, jerry: would do- what kramer: kramer: me of!, i,?, i, you, you,? i? i, you????, i?? i jerry: that kramer: me what that of in no do- what me of george: glasses george: alone talk thing uh do-! i,, jerry: very gonna&#34; of a a a!, you? i,??,,?,,?,? you? you you,, you? i, jerry: would that&#39;s)! jerry: kramer: kramer: no not about-!,?, i i, you, jerry: would know he) george: thought too her have with kramer her have that what of! you jerry: george: alright very come see yeah he so with kramer one at have me of george: ready talk these one at out where if know like all) george: so that kramer: that kramer: what kramer: that of george: alright back are of george: ready was he well i&#39;m don&#39;t that kramer: what with are that kramer: that kramer: of is- kramer: water her all well) george: water could not don&#39;t kramer: kramer: what of a a!,,, i i,,,?? i,, jerry: alright one your&#34; what of!?, jerry: would that&#39;s know he) a george: ready was it&#39;s well) a! i?,, jerry: when know- kramer: well don&#39;t kramer: kramer: no do don&#39;t that kramer: that me kramer: what with are with was he he)! you you??,,?,,,,?,, you?, jerry: alright one at all he) in no do don&#39;t kramer: kramer: what of! you??,, i jerry:)! jerry: what that me what me kramer: what that what kramer: kramer: kramer: of george: thought her- kramer: thought her go&#34; no hey do- of! you i, jerry: went very see for have for all) george: so
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[38]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># run the cell multiple times to get different results!</span>
<span class="n">gen_length</span> <span class="o">=</span> <span class="mi">400</span> <span class="c1"># modify the length to your preference</span>
<span class="n">prime_word</span> <span class="o">=</span> <span class="s1">&#39;jerry&#39;</span> <span class="c1"># name for starting the script</span>

<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">DON&#39;T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE</span>

<span class="sd">&quot;&quot;&quot;</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;sequence_length: </span><span class="si">{}</span><span class="se">\n</span><span class="s1">batch_size: </span><span class="si">{}</span><span class="se">\n</span><span class="s1">learning_rate: </span><span class="si">{}</span><span class="se">\n</span><span class="s1">embedding_dim: </span><span class="si">{}</span><span class="se">\n</span><span class="s1">n_layers: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">sequence_length</span><span class="p">,</span><span class="n">batch_size</span><span class="p">,</span><span class="n">learning_rate</span><span class="p">,</span><span class="n">embedding_dim</span><span class="p">,</span><span class="n">n_layers</span><span class="p">))</span>

<span class="n">pad_word</span> <span class="o">=</span> <span class="n">helper</span><span class="o">.</span><span class="n">SPECIAL_WORDS</span><span class="p">[</span><span class="s1">&#39;PADDING&#39;</span><span class="p">]</span>
<span class="n">generated_script</span> <span class="o">=</span> <span class="n">generate</span><span class="p">(</span><span class="n">trained_rnn</span><span class="p">,</span> <span class="n">vocab_to_int</span><span class="p">[</span><span class="n">prime_word</span> <span class="o">+</span> <span class="s1">&#39;:&#39;</span><span class="p">],</span> <span class="n">int_to_vocab</span><span class="p">,</span> <span class="n">token_dict</span><span class="p">,</span> <span class="n">vocab_to_int</span><span class="p">[</span><span class="n">pad_word</span><span class="p">],</span> <span class="n">gen_length</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">generated_script</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>sequence_length: 8
batch_size: 256
learning_rate: 0.001
embedding_dim: 300
n_layers: 2
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stderr output_text">
<pre>/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:44: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>jerry: pretty glasses pretty her do don&#39;t with kramer if know like all don&#39;t yeah- what that what that me kramer: no) george: alright when)!, i,?,, jerry: would that&#39;s do on kramer: kramer: kramer: no) a a in kramer: a!,,, i? i i you,,, you you,, jerry: alright when do- of george: very if hey) in me no not he he well my) george: very one&#34; no not about&#34; kramer: that of george: thought very see kramer: me of!,? you i you? jerry: alright when) a in!,,? you you i, you? you you jerry: what me!,, jerry: thought he&#39;s he&#39;s come back look one- kramer: glasses are kramer: kramer: me no)!?,, jerry: went sorry wait back was) george: thought very come one your&#34; what with are kramer: what of a a a a is- kramer: water her all)!,?, jerry: would that&#39;s not- kramer: water are kramer: kramer: kramer: of a george: alright back where uh) in kramer: of george: water was he) a george: few thought back was well) george: alright too if) a!, i jerry: so kramer: that me of in kramer:!,? you??? i,,,,??,, jerry: very come one&#34; kramer: kramer: of!, i? you?,?, jerry: thought when know don&#39;t with are for go&#34; no know- kramer: well well my hey not he well don&#39;t kramer: what me no not- kramer: glasses alone was) a a a! i?, you jerry:!?, i,?, jerry: thought one at have with was he he)!?,?? i,,,? jerry: water could know he) george: very he&#39;s one your-! jerry: kramer: no hey know like all) george: he don&#39;t that of george: would one your do- of in of a!, jerry: alright back was) a a george: few thought back was) a! i? you
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[35]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># run the cell multiple times to get different results!</span>
<span class="n">gen_length</span> <span class="o">=</span> <span class="mi">400</span> <span class="c1"># modify the length to your preference</span>
<span class="n">prime_word</span> <span class="o">=</span> <span class="s1">&#39;jerry&#39;</span> <span class="c1"># name for starting the script</span>

<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">DON&#39;T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE</span>

<span class="sd">&quot;&quot;&quot;</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;sequence_length: </span><span class="si">{}</span><span class="se">\n</span><span class="s1">batch_size: </span><span class="si">{}</span><span class="se">\n</span><span class="s1">learning_rate: </span><span class="si">{}</span><span class="se">\n</span><span class="s1">embedding_dim: </span><span class="si">{}</span><span class="se">\n</span><span class="s1">n_layers: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">sequence_length</span><span class="p">,</span><span class="n">batch_size</span><span class="p">,</span><span class="n">learning_rate</span><span class="p">,</span><span class="n">embedding_dim</span><span class="p">,</span><span class="n">n_layers</span><span class="p">))</span>

<span class="n">pad_word</span> <span class="o">=</span> <span class="n">helper</span><span class="o">.</span><span class="n">SPECIAL_WORDS</span><span class="p">[</span><span class="s1">&#39;PADDING&#39;</span><span class="p">]</span>
<span class="n">generated_script</span> <span class="o">=</span> <span class="n">generate</span><span class="p">(</span><span class="n">trained_rnn</span><span class="p">,</span> <span class="n">vocab_to_int</span><span class="p">[</span><span class="n">prime_word</span> <span class="o">+</span> <span class="s1">&#39;:&#39;</span><span class="p">],</span> <span class="n">int_to_vocab</span><span class="p">,</span> <span class="n">token_dict</span><span class="p">,</span> <span class="n">vocab_to_int</span><span class="p">[</span><span class="n">pad_word</span><span class="p">],</span> <span class="n">gen_length</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">generated_script</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>sequence_length: 8
batch_size: 256
learning_rate: 0.0001
embedding_dim: 300
n_layers: 2
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stderr output_text">
<pre>/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:44: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>jerry: project george&#39;s pretty alright very one at go your&#34; me kramer: no) a in kramer:! jerry: what that no know he don&#39;t yeah he so with look one- no know on kramer: no hey do- kramer: alright one do- kramer: thought her go do like have with are that no do don&#39;t that me me what kramer: no do-!?, jerry: thought when do on what of a! you??,,?? i,,? jerry: us will one- of a george: very he&#39;s see with are with are for have kramer: of george: thought very see yeah like go&#34; me no hey not he he well so kramer: of a in of george: alright her go do don&#39;t with was he he my do he he don&#39;t that kramer: that of george: ready was he don&#39;t kramer: that kramer: kramer: me no hey)!,, you,? jerry: thought back kramer think know like george don&#39;t that kramer: of!? you you you i,?,? you,, jerry: alright one do he he)!, you? jerry: thought back was) a is don&#39;t with are for george well on kramer: what that what with was) a a a!, i?? you you? i? jerry: turns are me of in kramer: what that of! you you,, you?,,,???????,, jerry: alright too if do don&#39;t kramer: of a george: sorry she&#39;s her we do don&#39;t with look if do on kramer: kramer: what with are that no do he don&#39;t for have with look if hey hey not he well my hey know don&#39;t for george he he i&#39;m on what me kramer: of george: water think at have of! you? you i,? you you,, jerry: very gonna&#34; what kramer: me kramer: what kramer: what that kramer: kramer: of in kramer: kramer: kramer: what of in! you,, i i i, i you jerry: a!? jerry: old talk these come see for have for go at have me kramer: me what of george: ready these see with was he well)
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[19]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">sequence_length: 8</span>
<span class="sd">batch_size: 128</span>
<span class="sd">learning_rate: 0.0001</span>
<span class="sd">embedding_dim: 1000</span>
<span class="sd">n_layers: 3</span>
<span class="sd">Training for 400 epoch(s)</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="c1"># run the cell multiple times to get different results!</span>
<span class="n">gen_length</span> <span class="o">=</span> <span class="mi">400</span> <span class="c1"># modify the length to your preference</span>
<span class="n">prime_word</span> <span class="o">=</span> <span class="s1">&#39;jerry&#39;</span> <span class="c1"># name for starting the script</span>

<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">DON&#39;T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE</span>

<span class="sd">&quot;&quot;&quot;</span>
<span class="n">pad_word</span> <span class="o">=</span> <span class="n">helper</span><span class="o">.</span><span class="n">SPECIAL_WORDS</span><span class="p">[</span><span class="s1">&#39;PADDING&#39;</span><span class="p">]</span>
<span class="n">generated_script</span> <span class="o">=</span> <span class="n">generate</span><span class="p">(</span><span class="n">trained_rnn</span><span class="p">,</span> <span class="n">vocab_to_int</span><span class="p">[</span><span class="n">prime_word</span> <span class="o">+</span> <span class="s1">&#39;:&#39;</span><span class="p">],</span> <span class="n">int_to_vocab</span><span class="p">,</span> <span class="n">token_dict</span><span class="p">,</span> <span class="n">vocab_to_int</span><span class="p">[</span><span class="n">pad_word</span><span class="p">],</span> <span class="n">gen_length</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">generated_script</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stderr output_text">
<pre>/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:44: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>jerry: project glasses architect architect alone joke talk these her go your- kramer: water think&#34; kramer: no do-! jerry: what kramer: no not- kramer: glasses alone talk thing if) a in kramer: what me of a! you, jerry: went sorry could hey) a a george: ready talk talk thing think do don&#39;t yeah like out look think do don&#39;t that kramer: of in what kramer: kramer: of!? jerry: by her have of a is he),,?,,, i,,,?,?? jerry: alright her- of george: ready talk talk thing uh) a a a a george: few she&#39;s her we not on no know on kramer: kramer: what of a a a george: ready are me no hey do he on me what me of a!? jerry: us talk thing think hey do),, you?? jerry: us us talk thing if do he), i jerry: well my do- kramer: water was)!,,,?? i?? jerry: turns her- of george: ready are of george: alright could know he he)!,?, i,? jerry: water could do) in of!,, you,,,, you jerry: george: very come he&#39;s come one your- what that no) george: would see for a a a!,, jerry: would do don&#39;t for have yeah don&#39;t for!?,,,,,? i? i you, i?, jerry: would know like-! i, jerry: would) a!? i? you?,,,,, i,, you,,,, you,,, i you? you,, i, you, jerry: very gonna do he don&#39;t that of a!? jerry: alright back think all well on no) a a in!,,?, jerry: went back was- what that of a! you jerry: a a!, i?,? jerry: turns are of!, you?,,,, you? you jerry: george: wait sorry wait sorry her we hey not on kramer: me kramer:
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Save-your-favorite-scripts">Save your favorite scripts<a class="anchor-link" href="#Save-your-favorite-scripts">&#182;</a></h4><p>Once you have a script that you like (or find interesting), save it to a text file!</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[28]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># save script to a text file</span>
<span class="n">f</span> <span class="o">=</span>  <span class="nb">open</span><span class="p">(</span><span class="s2">&quot;generated_script_1.txt&quot;</span><span class="p">,</span><span class="s2">&quot;w&quot;</span><span class="p">)</span>
<span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">generated_script</span><span class="p">)</span>
<span class="n">f</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="The-TV-Script-is-Not-Perfect">The TV Script is Not Perfect<a class="anchor-link" href="#The-TV-Script-is-Not-Perfect">&#182;</a></h1><p>It's ok if the TV script doesn't make perfect sense. It should look like alternating lines of dialogue, here is one such example of a few generated lines.</p>
<h3 id="Example-generated-script">Example generated script<a class="anchor-link" href="#Example-generated-script">&#182;</a></h3><blockquote><p>jerry: what about me?</p>
<p>jerry: i don't have to wait.</p>
<p>kramer:(to the sales table)</p>
<p>elaine:(to jerry) hey, look at this, i'm a good doctor.</p>
<p>newman:(to elaine) you think i have no idea of this...</p>
<p>elaine: oh, you better take the phone, and he was a little nervous.</p>
<p>kramer:(to the phone) hey, hey, jerry, i don't want to be a little bit.(to kramer and jerry) you can't.</p>
<p>jerry: oh, yeah. i don't even know, i know.</p>
<p>jerry:(to the phone) oh, i know.</p>
<p>kramer:(laughing) you know...(to jerry) you don't know.</p>
</blockquote>
<p>You can see that there are multiple characters that say (somewhat) complete sentences, but it doesn't have to be perfect! It takes quite a while to get good results, and often, you'll have to use a smaller vocabulary (and discard uncommon words), or get more data.  The Seinfeld dataset is about 3.4 MB, which is big enough for our purposes; for script generation you'll want more than 1 MB of text, generally.</p>
<h1 id="Submitting-This-Project">Submitting This Project<a class="anchor-link" href="#Submitting-This-Project">&#182;</a></h1><p>When submitting this project, make sure to run all the cells before saving the notebook. Save the notebook file as "dlnd_tv_script_generation.ipynb" and save another copy as an HTML file by clicking "File" -&gt; "Download as.."-&gt;"html". Include the "helper.py" and "problem_unittests.py" files in your submission. Once you download these files, compress them into one zip file for submission.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span> 
</pre></div>

    </div>
</div>
</div>

</div>
    </div>
  </div>
</body>

 


</html>
